{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOuBxW2zOUvg3Pq27npUPMS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WalterPHD/Ai-Data/blob/main/Ensemble.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uyQyluZ-W-Su",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dcb4583-1178-465d-fb51-b2e9acb31955"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set: (1168, 2), Validation set: (292, 2)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Loading the CSV\n",
        "df = pd.read_csv('train.csv')\n",
        "\n",
        "#Selecting columns\n",
        "features = ['GrLivArea', 'YearBuilt']\n",
        "target = 'SalePrice'\n",
        "\n",
        "#Dropped rows with missing values in the selected columns\n",
        "df = df[features + [target]].dropna()\n",
        "\n",
        "#Separating features and target\n",
        "X = df[features].values\n",
        "y = df[target].values\n",
        "\n",
        "#80/20 split\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "#Veryfying shape to confirm\n",
        "print(f\"Training set: {X_train.shape}, Validation set: {X_valid.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 1 - Blending scratch mounting"
      ],
      "metadata": {
        "id": "kCKna88QazaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression, Ridge\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "#Loading and clean data\n",
        "df = pd.read_csv('train.csv')\n",
        "features = ['GrLivArea', 'YearBuilt']\n",
        "target = 'SalePrice'\n",
        "df = df[features + [target]].dropna()\n",
        "\n",
        "#Feature + target split\n",
        "X = df[features].values\n",
        "y = df[target].values\n",
        "\n",
        "#Log-transform target for model 1\n",
        "y_log = np.log1p(y)\n",
        "\n",
        "# 80/20 split\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "_, _, y_log_train, y_log_valid = train_test_split(X, y_log, test_size=0.2, random_state=42)\n",
        "\n",
        "#Linear Regression (with log transform + standardization)\n",
        "scaler1 = StandardScaler()\n",
        "X_train_std1 = scaler1.fit_transform(X_train)\n",
        "X_valid_std1 = scaler1.transform(X_valid)\n",
        "\n",
        "model1 = LinearRegression()\n",
        "model1.fit(X_train_std1, y_log_train)\n",
        "pred1 = np.expm1(model1.predict(X_valid_std1))\n",
        "\n",
        "#Model 2: Support Vector Regressor (RBF kernel)\n",
        "scaler2 = StandardScaler()\n",
        "X_train_std2 = scaler2.fit_transform(X_train)\n",
        "X_valid_std2 = scaler2.transform(X_valid)\n",
        "\n",
        "model2 = SVR(kernel='rbf', C=100, epsilon=0.1)\n",
        "model2.fit(X_train_std2, y_train)\n",
        "pred2 = model2.predict(X_valid_std2)\n",
        "\n",
        "#Model 3: Decision Tree (no scaling + raw data)\n",
        "model3 = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
        "model3.fit(X_train, y_train)\n",
        "pred3 = model3.predict(X_valid)\n",
        "\n",
        "#Meta-Model: Ridge Regression on stacked predictions\n",
        "meta_X_train = np.column_stack([pred1, pred2, pred3])\n",
        "meta_model = Ridge(alpha=1.0)\n",
        "meta_model.fit(meta_X_train, y_valid)\n",
        "\n",
        "#Final Predictions\n",
        "final_pred = meta_model.predict(meta_X_train)\n",
        "mse_blending = mean_squared_error(y_valid, final_pred)\n",
        "\n",
        "#Base Model for Comparison\n",
        "base_model = LinearRegression()\n",
        "base_model.fit(X_train_std1, y_train)\n",
        "mse_base = mean_squared_error(y_valid, base_model.predict(X_valid_std1))\n",
        "\n",
        "#Results\n",
        "print(f\"Base Linear Regression MSE: {mse_base:.2f}\")\n",
        "print(f\"Blending Ensemble (Diverse Models) MSE: {mse_blending:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wi33Nn6Oa2bM",
        "outputId": "11232f61-8b09-45b6-a828-fb7af5db627a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base Linear Regression MSE: 2495554898.67\n",
            "Blending Ensemble (Diverse Models) MSE: 1616660017.89\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 2 - Bagging scratch mounting"
      ],
      "metadata": {
        "id": "wMDQ_PyfbPDl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scratch bagging and Linear model"
      ],
      "metadata": {
        "id": "Dk8vC8uebU7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class ScratchLinearRegression:\n",
        "    def __init__(self):\n",
        "        self.coef_ = None\n",
        "        self.intercept_ = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
        "        theta = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y\n",
        "        self.intercept_ = theta[0]\n",
        "        self.coef_ = theta[1:]\n",
        "\n",
        "    def predict(self, X):\n",
        "        return X @ self.coef_ + self.intercept_\n",
        "\n",
        "\n",
        "class BaggingRegressor:\n",
        "    def __init__(self, base_model_class, n_estimators=20, sample_size=None, random_state=None):\n",
        "        self.base_model_class = base_model_class\n",
        "        self.n_estimators = n_estimators\n",
        "        self.sample_size = sample_size\n",
        "        self.models = []\n",
        "        self.random_state = random_state\n",
        "        if self.random_state is not None:\n",
        "            np.random.seed(self.random_state)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples = X.shape[0]\n",
        "        self.sample_size = self.sample_size or int(n_samples * 0.63)\n",
        "        self.models = []\n",
        "\n",
        "        for _ in range(self.n_estimators):\n",
        "            indices = np.random.choice(n_samples, self.sample_size, replace=True)\n",
        "            X_sample = X[indices]\n",
        "            y_sample = y[indices]\n",
        "\n",
        "            model = self.base_model_class()\n",
        "            model.fit(X_sample, y_sample)\n",
        "            self.models.append(model)\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = np.array([model.predict(X) for model in self.models])\n",
        "        return predictions.mean(axis=0)\n"
      ],
      "metadata": {
        "id": "R8vWBR04bX4S"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Loading"
      ],
      "metadata": {
        "id": "J0SdcXOmgsRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = pd.read_csv('train.csv')\n",
        "features = ['GrLivArea', 'YearBuilt']\n",
        "target = 'SalePrice'\n",
        "df = df[features + [target]].dropna()\n",
        "\n",
        "X = df[features].values\n",
        "y = df[target].values\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n"
      ],
      "metadata": {
        "id": "Vjo-BhzMgu5o"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparison"
      ],
      "metadata": {
        "id": "NFlRVe54bo54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "#Single base model for comparison\n",
        "single_model = ScratchLinearRegression()\n",
        "single_model.fit(X_train, y_train)\n",
        "y_pred_single = single_model.predict(X_valid)\n",
        "mse_single = mean_squared_error(y_valid, y_pred_single)\n",
        "\n",
        "#Bagging ensemble model\n",
        "bagging_model = BaggingRegressor(\n",
        "    ScratchLinearRegression,\n",
        "    n_estimators=10,\n",
        "    # random_state=42 # Removed the random_state argument\n",
        ")\n",
        "bagging_model.fit(X_train, y_train)\n",
        "y_pred_bagging = bagging_model.predict(X_valid)\n",
        "mse_bagging = mean_squared_error(y_valid, y_pred_bagging)\n",
        "\n",
        "print(f\"Single Model MSE: {mse_single:.2f}\")\n",
        "print(f\"Bagging Model MSE: {mse_bagging:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "474p3uQnbqJj",
        "outputId": "4d780293-436f-40e2-96f5-3f59c842947c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single Model MSE: 2495554898.67\n",
            "Bagging Model MSE: 2479886145.71\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Problem 3 - Stacking scratch mounting"
      ],
      "metadata": {
        "id": "fnMHI5VycXm3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stacking scrattch code & LinerRegression code"
      ],
      "metadata": {
        "id": "o1l6epClceWs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "class ScratchLinearRegression:\n",
        "    def __init__(self):\n",
        "        self.coef_ = None\n",
        "        self.intercept_ = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
        "        theta = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y\n",
        "        self.intercept_ = theta[0]\n",
        "        self.coef_ = theta[1:]\n",
        "\n",
        "    def predict(self, X):\n",
        "        return X @ self.coef_ + self.intercept_\n",
        "\n",
        "\n",
        "class StackingRegressor:\n",
        "    def __init__(self, base_models, meta_model, n_folds=5):\n",
        "        self.base_models = base_models\n",
        "        self.meta_model = meta_model\n",
        "        self.n_folds = n_folds\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.base_models_fitted = [[] for _ in self.base_models]\n",
        "        kf = KFold(n_splits=self.n_folds, shuffle=True, random_state=42)\n",
        "\n",
        "        # Out-of-fold predictions for training meta-model\n",
        "        oof_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
        "\n",
        "        for i, base_model_class in enumerate(self.base_models):\n",
        "            for train_idx, val_idx in kf.split(X, y):\n",
        "                X_train_k, X_val_k = X[train_idx], X[val_idx]\n",
        "                y_train_k = y[train_idx]\n",
        "\n",
        "                model = base_model_class()\n",
        "                model.fit(X_train_k, y_train_k)\n",
        "                y_pred = model.predict(X_val_k)\n",
        "\n",
        "                oof_predictions[val_idx, i] = y_pred\n",
        "                self.base_models_fitted[i].append(model)\n",
        "\n",
        "        # Train meta-model on out-of-fold predictions\n",
        "        self.meta_model.fit(oof_predictions, y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        meta_features = np.zeros((X.shape[0], len(self.base_models)))\n",
        "\n",
        "        for i, models in enumerate(self.base_models_fitted):\n",
        "            preds = [model.predict(X) for model in models]\n",
        "            meta_features[:, i] = np.mean(preds, axis=0)\n",
        "\n",
        "        return self.meta_model.predict(meta_features)\n"
      ],
      "metadata": {
        "id": "09MqtxrKckny"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparison of Blending, Bagging, Stacking and LinearRegression"
      ],
      "metadata": {
        "id": "u2sWEjHCeRbo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "#Scratch Linear Regression\n",
        "class ScratchLinearRegression:\n",
        "    def __init__(self):\n",
        "        self.coef_ = None\n",
        "        self.intercept_ = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
        "        theta = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y\n",
        "        self.intercept_ = theta[0]\n",
        "        self.coef_ = theta[1:]\n",
        "\n",
        "    def predict(self, X):\n",
        "        return X @ self.coef_ + self.intercept_\n",
        "\n",
        "#Bagging Regressor\n",
        "class BaggingRegressor:\n",
        "    def __init__(self, base_model_class, n_estimators=20, sample_size=None, random_state=None):\n",
        "        self.base_model_class = base_model_class\n",
        "        self.n_estimators = n_estimators\n",
        "        self.sample_size = sample_size\n",
        "        self.models = []\n",
        "        self.random_state = random_state\n",
        "        if self.random_state is not None:\n",
        "            np.random.seed(self.random_state)\n",
        "\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples = X.shape[0]\n",
        "        self.sample_size = self.sample_size or int(n_samples * 0.63)\n",
        "        self.models = []\n",
        "\n",
        "        for _ in range(self.n_estimators):\n",
        "            indices = np.random.choice(n_samples, self.sample_size, replace=True)\n",
        "            X_sample = X[indices]\n",
        "            y_sample = y[indices]\n",
        "\n",
        "            model = self.base_model_class()\n",
        "            model.fit(X_sample, y_sample)\n",
        "            self.models.append(model)\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = np.array([model.predict(X) for model in self.models])\n",
        "        return predictions.mean(axis=0)\n",
        "\n",
        "#Stacking Regressor\n",
        "class StackingRegressor:\n",
        "    def __init__(self, base_models, meta_model, n_folds=5):\n",
        "        self.base_models = base_models\n",
        "        self.meta_model = meta_model\n",
        "        self.n_folds = n_folds\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.base_models_fitted = [[] for _ in self.base_models]\n",
        "        kf = KFold(n_splits=self.n_folds, shuffle=True, random_state=42)\n",
        "\n",
        "        oof_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
        "\n",
        "        for i, base_model_class in enumerate(self.base_models):\n",
        "            for train_idx, val_idx in kf.split(X, y):\n",
        "                X_train_k, X_val_k = X[train_idx], X[val_idx]\n",
        "                y_train_k = y[train_idx]\n",
        "\n",
        "                model = base_model_class()\n",
        "                model.fit(X_train_k, y_train_k)\n",
        "                y_pred = model.predict(X_val_k)\n",
        "\n",
        "                oof_predictions[val_idx, i] = y_pred\n",
        "                self.base_models_fitted[i].append(model)\n",
        "\n",
        "        self.meta_model.fit(oof_predictions, y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        meta_features = np.zeros((X.shape[0], len(self.base_models)))\n",
        "        for i, models in enumerate(self.base_models_fitted):\n",
        "            preds = [model.predict(X) for model in models]\n",
        "            meta_features[:, i] = np.mean(preds, axis=0)\n",
        "        return self.meta_model.predict(meta_features)\n",
        "\n",
        "#Blending Regressor\n",
        "class BlendingRegressor:\n",
        "    def __init__(self, base_models, meta_model):\n",
        "        self.base_models = base_models\n",
        "        self.meta_model = meta_model\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.fitted_base_models = []\n",
        "        base_preds = []\n",
        "\n",
        "        for base_model in self.base_models:\n",
        "            if callable(base_model):\n",
        "                model = base_model()\n",
        "            else:\n",
        "                model = base_model\n",
        "            model.fit(X, y)\n",
        "            self.fitted_base_models.append(model)\n",
        "            base_preds.append(model.predict(X))\n",
        "\n",
        "        stacked_features = np.column_stack(base_preds)\n",
        "        self.meta_model.fit(stacked_features, y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        base_preds = [model.predict(X) for model in self.fitted_base_models]\n",
        "        stacked_features = np.column_stack(base_preds)\n",
        "        return self.meta_model.predict(stacked_features)\n",
        "\n",
        "\n",
        "\n",
        "#Loading dataset and split once again 80/20\n",
        "df = pd.read_csv('train.csv')\n",
        "features = ['GrLivArea', 'YearBuilt']\n",
        "target = 'SalePrice'\n",
        "df = df[features + [target]].dropna()\n",
        "\n",
        "X = df[features].values\n",
        "y = df[target].values\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "#Single Model (LinearRegression)\n",
        "single_model = ScratchLinearRegression()\n",
        "single_model.fit(X_train, y_train)\n",
        "y_pred_single = single_model.predict(X_valid)\n",
        "mse_single = mean_squared_error(y_valid, y_pred_single)\n",
        "\n",
        "#Bagging\n",
        "bagging_model = BaggingRegressor(\n",
        "    ScratchLinearRegression,\n",
        "    n_estimators=20,\n",
        "    # random_state=42 # Removed the random_state argument\n",
        ")\n",
        "bagging_model.fit(X_train, y_train)\n",
        "y_pred_bagging = bagging_model.predict(X_valid)\n",
        "mse_bagging = mean_squared_error(y_valid, y_pred_bagging)\n",
        "\n",
        "#Stacking (with diverse base models)\n",
        "stacking_model = StackingRegressor(\n",
        "    base_models=[ScratchLinearRegression, DecisionTreeRegressor],\n",
        "    meta_model=Ridge(alpha=1.0),\n",
        "    n_folds=5\n",
        ")\n",
        "stacking_model.fit(X_train, y_train)\n",
        "y_pred_stack = stacking_model.predict(X_valid)\n",
        "mse_stack = mean_squared_error(y_valid, y_pred_stack)\n",
        "\n",
        "#Blending (with diverse base models)\n",
        "blending_model = BlendingRegressor(\n",
        "    base_models=[ScratchLinearRegression, DecisionTreeRegressor, SVR(kernel='rbf', C=1.0, epsilon=0.1)],\n",
        "    meta_model=Ridge(alpha=1.0)\n",
        ")\n",
        "blending_model.fit(X_train, y_train)\n",
        "y_pred_blending = blending_model.predict(X_valid)\n",
        "mse_blending = mean_squared_error(y_valid, y_pred_blending)\n",
        "\n",
        "# --- Print comparison ---\n",
        "print(f\"{'Model':<15} | {'Validation MSE'}\")\n",
        "print(\"-\" * 30)\n",
        "print(f\"{'Single':<15} | {mse_single:.2f}\")\n",
        "print(f\"{'Bagging':<15} | {mse_bagging:.2f}\")\n",
        "print(f\"{'Stacking':<15} | {mse_stack:.2f}\")\n",
        "print(f\"{'Blending':<15} | {mse_blending:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5z1WaKheYNr",
        "outputId": "4a1217cc-2827-44cc-9c13-0cabd48c1704"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model           | Validation MSE\n",
            "------------------------------\n",
            "Single          | 2495554898.67\n",
            "Bagging         | 2481482732.04\n",
            "Stacking        | 2157381456.90\n",
            "Blending        | 2090022577.55\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#What I understood"
      ],
      "metadata": {
        "id": "R214EvHGjeL2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Single model is the baseline MSE.\n",
        "\n",
        "Bagging gives a slight improvement by reducing variance.\n",
        "\n",
        "Stacking and Blending leverage diversity and meta-models to significantly reduce error.\n",
        "\n",
        "Blending’s lowest MSE shows that using diverse models and a good meta-model can be very powerful."
      ],
      "metadata": {
        "id": "R5TIkkmRjhRs"
      }
    }
  ]
}