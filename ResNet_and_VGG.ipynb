{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WalterPHD/Ai-Data/blob/main/ResNet_and_VGG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Problem 1\n",
        "\n",
        "My first version of the code was a standard U-Net, where I built both the encoder and decoder from scratch. All convolutional layers started with random weights, so the model had to learn everything—image features and segmentation boundaries—directly from the salt dataset. While this worked, it required more training data and time, and it was more likely to overfit.\n",
        "\n",
        "The new version uses transfer learning. Instead of training the encoder from zero, it uses a pre-trained backbone like ResNet50 or VGG16, originally trained on ImageNet. These networks already know how to detect general features such as edges, textures, and shapes. The U-Net decoder is then added on top, using upsampling and skip connections to generate the segmentation mask.\n",
        "\n",
        "I understood the it works more like...\n",
        "Load a pre-trained encoder (e.g., ResNet or VGG) with weights='imagenet' and include_top=False—this keeps the convolutional layers for feature extraction but removes the classification layers.\n",
        "\n",
        "Use intermediate feature maps from the encoder as skip connections in the U-Net.\n",
        "\n",
        "Add a decoder with upsampling and convolution layers to rebuild the segmentation mask.\n",
        "\n",
        "Train only the decoder at first while keeping the encoder frozen, and later fine-tune some encoder layers if needed.\n",
        "\n",
        "The key difference is that my first model had to learn all features from scratch, while the new one reuses knowledge from ImageNet. This makes training faster, reduces overfitting, and gives better accuracy, even with a smaller dataset."
      ],
      "metadata": {
        "id": "dModyCkjd8SG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Problem 2"
      ],
      "metadata": {
        "id": "YBtfJg9zvb_W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.layers import Conv2D, UpSampling2D, MaxPooling2D, concatenate\n",
        "from tensorflow.keras.layers import BatchNormalization, Activation, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "from tensorflow.keras.applications import ResNet50, VGG16\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "# Make GPU memory growth safer\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for g in gpus:\n",
        "            tf.config.experimental.set_memory_growth(g, True)\n",
        "    except Exception as e:\n",
        "        print(\"GPU config warning:\", e)\n",
        "\n",
        "def dice_coef(y_true, y_pred, smooth=1e-6):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(K.cast(y_pred > 0.5, 'float32'))\n",
        "    inter = K.sum(y_true_f * y_pred_f)\n",
        "    return (2. * inter + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
        "\n",
        "def iou_coef(y_true, y_pred, smooth=1e-6):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(K.cast(y_pred > 0.5, 'float32'))\n",
        "    inter = K.sum(y_true_f * y_pred_f)\n",
        "    union = K.sum(y_true_f) + K.sum(y_pred_f) - inter\n",
        "    return (inter + smooth) / (union + smooth)\n",
        "\n",
        "def conv_block(x, n_filters):\n",
        "    x = Conv2D(n_filters, 3, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(n_filters, 3, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "def build_unet_backbone(backbone='resnet50', input_shape=(256, 256, 3), freeze_encoder=True):\n",
        "    \"\"\"\n",
        "    backbone: 'resnet50' or 'vgg16'\n",
        "    input_shape must be 3-channel (RGB) because we load ImageNet weights.\n",
        "    \"\"\"\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    if backbone.lower() == 'resnet50':\n",
        "        base = ResNet50(weights='imagenet', include_top=False, input_tensor=inputs)\n",
        "        # Skip features (from shallow to deep)\n",
        "        skip1 = base.get_layer('conv1_relu').output         # ~1/2\n",
        "        skip2 = base.get_layer('conv2_block3_out').output   # ~1/4\n",
        "        skip3 = base.get_layer('conv3_block4_out').output   # ~1/8\n",
        "        skip4 = base.get_layer('conv4_block6_out').output   # ~1/16\n",
        "        bottleneck = base.get_layer('conv5_block3_out').output  # ~1/32\n",
        "    elif backbone.lower() == 'vgg16':\n",
        "        base = VGG16(weights='imagenet', include_top=False, input_tensor=inputs)\n",
        "        # VGG blocks\n",
        "        skip1 = base.get_layer('block1_conv2').output  # ~1/2\n",
        "        skip2 = base.get_layer('block2_conv2').output  # ~1/4\n",
        "        skip3 = base.get_layer('block3_conv3').output  # ~1/8\n",
        "        skip4 = base.get_layer('block4_conv3').output  # ~1/16\n",
        "        bottleneck = base.get_layer('block5_conv3').output  # ~1/32\n",
        "    else:\n",
        "        raise ValueError(\"backbone must be 'resnet50' or 'vgg16'\")\n",
        "\n",
        "    if freeze_encoder:\n",
        "        for l in base.layers:\n",
        "            l.trainable = False\n",
        "\n",
        "\n",
        "    x = UpSampling2D((2, 2))(bottleneck)        # 1/16\n",
        "    x = concatenate([x, skip4])\n",
        "    x = conv_block(x, 512)\n",
        "\n",
        "    x = UpSampling2D((2, 2))(x)                  # 1/8\n",
        "    x = concatenate([x, skip3])\n",
        "    x = conv_block(x, 256)\n",
        "\n",
        "    x = UpSampling2D((2, 2))(x)                  # 1/4\n",
        "    x = concatenate([x, skip2])\n",
        "    x = conv_block(x, 128)\n",
        "\n",
        "    x = UpSampling2D((2, 2))(x)                  # 1/2\n",
        "    x = concatenate([x, skip1])\n",
        "    x = conv_block(x, 64)\n",
        "\n",
        "    x = UpSampling2D((2, 2))(x)                  # 1/1\n",
        "    x = conv_block(x, 32)\n",
        "\n",
        "    outputs = Conv2D(1, 1, activation='sigmoid')(x)\n",
        "    model = Model(inputs, outputs, name=f'unet_{backbone}')\n",
        "    return model\n",
        "\n",
        "# ========= Data pipeline (pairs RGB image + 1ch mask) =========\n",
        "def make_generators(train_dir,\n",
        "                    img_sub='image',\n",
        "                    mask_sub='label',\n",
        "                    target_size=(256, 256),\n",
        "                    batch_size=4,\n",
        "                    val_split=0.1,\n",
        "                    seed=42,\n",
        "                    augment=True):\n",
        "\n",
        "    if augment:\n",
        "        common_args = dict(\n",
        "            rescale=1./255,\n",
        "            rotation_range=10,\n",
        "            width_shift_range=0.05,\n",
        "            height_shift_range=0.05,\n",
        "            shear_range=0.05,\n",
        "            zoom_range=0.05,\n",
        "            horizontal_flip=True,\n",
        "            fill_mode='nearest',\n",
        "            validation_split=val_split\n",
        "        )\n",
        "    else:\n",
        "        common_args = dict(rescale=1./255, validation_split=val_split)\n",
        "\n",
        "    img_gen = ImageDataGenerator(**common_args)\n",
        "    msk_gen = ImageDataGenerator(**common_args)\n",
        "\n",
        "    img_train = img_gen.flow_from_directory(\n",
        "        train_dir, classes=[img_sub], class_mode=None,\n",
        "        target_size=target_size, color_mode='rgb',\n",
        "        batch_size=batch_size, subset='training', seed=seed, shuffle=True)\n",
        "\n",
        "    msk_train = msk_gen.flow_from_directory(\n",
        "        train_dir, classes=[mask_sub], class_mode=None,\n",
        "        target_size=target_size, color_mode='grayscale',\n",
        "        batch_size=batch_size, subset='training', seed=seed, shuffle=True)\n",
        "\n",
        "    img_val = img_gen.flow_from_directory(\n",
        "        train_dir, classes=[img_sub], class_mode=None,\n",
        "        target_size=target_size, color_mode='rgb',\n",
        "        batch_size=batch_size, subset='validation', seed=seed, shuffle=False)\n",
        "\n",
        "    msk_val = msk_gen.flow_from_directory(\n",
        "        train_dir, classes=[mask_sub], class_mode=None,\n",
        "        target_size=target_size, color_mode='grayscale',\n",
        "        batch_size=batch_size, subset='validation', seed=seed, shuffle=False)\n",
        "\n",
        "    def pair_gen(a, b):\n",
        "        while True:\n",
        "            X = next(a)\n",
        "            y = next(b)\n",
        "            y = (y > 0.5).astype('float32')\n",
        "            yield X, y\n",
        "\n",
        "    train_pairs = pair_gen(img_train, msk_train)\n",
        "    val_pairs = pair_gen(img_val, msk_val)\n",
        "\n",
        "    steps_train = len(img_train)\n",
        "    steps_val = len(img_val)\n",
        "    return train_pairs, val_pairs, steps_train, steps_val\n",
        "\n",
        "\n",
        "def train_and_eval(backbone, train_dir='data/membrane/train', input_size=(256, 256), batch_size=4,  epochs=10, freeze_encoder=True, out_path=None):\n",
        "    train_gen, val_gen, steps_tr, steps_va = make_generators( train_dir=train_dir, target_size=input_size, batch_size=batch_size, val_split=0.1, augment=True)\n",
        "    model = build_unet_backbone(backbone=backbone,input_shape=(input_size[0], input_size[1], 3), freeze_encoder=freeze_encoder)\n",
        "\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-4), loss='binary_crossentropy', metrics=['accuracy', dice_coef, iou_coef])\n",
        "\n",
        "    if out_path is None:\n",
        "        out_path = f'unet_{backbone}.keras'\n",
        "\n",
        "    callbacks = [\n",
        "        ModelCheckpoint(out_path, monitor='val_iou_coef', mode='max',\n",
        "                        save_best_only=True, verbose=1),\n",
        "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1),\n",
        "        EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True, verbose=1)\n",
        "    ]\n",
        "\n",
        "    history = model.fit(train_gen, steps_per_epoch=steps_tr, validation_data=val_gen, validation_steps=steps_va, epochs=epochs, callbacks=callbacks, verbose=1)\n",
        "\n",
        "    # Final validation evaluation\n",
        "    val_scores = model.evaluate(val_gen, steps=steps_va, verbose=0)\n",
        "    scores = dict(zip(model.metrics_names, val_scores))\n",
        "    return model, history, scores\n",
        "\n",
        "\n",
        "def save_sample_preds(model, val_gen, out_dir='pred_samples', n=4):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    X, y = next(val_gen)\n",
        "    preds = model.predict(X, verbose=0)\n",
        "    for i in range(min(n, X.shape[0])):\n",
        "        tf.keras.preprocessing.image.save_img(os.path.join(out_dir, f'img_{i}.png'), X[i])\n",
        "        tf.keras.preprocessing.image.save_img(os.path.join(out_dir, f'mask_{i}.png'), y[i])\n",
        "        tf.keras.preprocessing.image.save_img(os.path.join(out_dir, f'pred_{i}.png'),\n",
        "                                              (preds[i] > 0.5).astype('float32'))\n"
      ],
      "metadata": {
        "id": "VmRCI7C-eDRj"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Problem 3\n"
      ],
      "metadata": {
        "id": "g9UCSqZVvlAE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_DIR = 'data/membrane/train'\n",
        "\n",
        "INPUT_SIZE = (128, 128)\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 1\n",
        "FREEZE_ENCODER = True\n",
        "\n",
        "resnet_model, resnet_hist, resnet_scores = train_and_eval(\n",
        "    backbone='resnet50',\n",
        "    train_dir=TRAIN_DIR,\n",
        "    input_size=INPUT_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    freeze_encoder=FREEZE_ENCODER,\n",
        "    out_path='unet_resnet50.keras'\n",
        ")\n",
        "\n",
        "vgg_model, vgg_hist, vgg_scores = train_and_eval(\n",
        "    backbone='vgg16',\n",
        "    train_dir=TRAIN_DIR,\n",
        "    input_size=INPUT_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    freeze_encoder=FREEZE_ENCODER,\n",
        "    out_path='unet_vgg16.keras'\n",
        ")\n",
        "\n",
        "def short_scores(name, s):\n",
        "    return f\"{name}: val_loss={s.get('loss',None):.4f} | val_acc={s.get('accuracy',None):.4f} | val_dice={s.get('dice_coef',None):.4f} | val_iou={s.get('iou_coef',None):.4f}\"\n",
        "\n",
        "print(short_scores(\"ResNet50\", resnet_scores))\n",
        "print(short_scores(\"VGG16   \", vgg_scores))\n",
        "\n",
        "_, val_gen_r, _, _ = make_generators(TRAIN_DIR, target_size=INPUT_SIZE, batch_size=BATCH_SIZE, val_split=0.1, augment=False)\n",
        "save_sample_preds(resnet_model, val_gen_r, out_dir='pred_samples_resnet', n=4)\n",
        "\n",
        "_, val_gen_v, _, _ = make_generators(TRAIN_DIR, target_size=INPUT_SIZE, batch_size=BATCH_SIZE, val_split=0.1, augment=False)\n",
        "save_sample_preds(vgg_model, val_gen_v, out_dir='pred_samples_vgg', n=4)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcFqpb31uqpC",
        "outputId": "223fc0e0-5173-406e-93ed-7063df043f4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 0 images belonging to 1 classes.\n",
            "Found 0 images belonging to 1 classes.\n",
            "Found 0 images belonging to 1 classes.\n",
            "Found 0 images belonging to 1 classes.\n",
            "   1127/Unknown \u001b[1m311s\u001b[0m 261ms/step - accuracy: 0.0000e+00 - dice_coef: 1.0000 - iou_coef: 1.0000 - loss: 0.0000e+00"
          ]
        }
      ]
    }
  ]
}