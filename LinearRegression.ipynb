{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO4CR1nDDneyyMrCs6PFLAj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WalterPHD/Ai-Data/blob/main/LinearRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Learning Field\n"
      ],
      "metadata": {
        "id": "t8uM0RE6j-Up"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPs_41Bf8hGq"
      },
      "outputs": [],
      "source": [
        "class ScratchLinearRegression():\n",
        "  def __init__(self, num_iter, lr, no_bias, verbose):\n",
        "    self.num_iter = num_iter\n",
        "    self.lr = lr\n",
        "    self.bias = bias\n",
        "    self.verbose = verbose\n",
        "    self.theta = np.array([])\n",
        "    self.loss = np.array([])\n",
        "    self.val_loss = np.array([])\n",
        "\n",
        "# problem6（ Learning and estimation）\n",
        "def fit(self):\n",
        "  \"\"\n",
        "\"\n",
        "Learning linear regression\n",
        "  \"\"\n",
        "\"\n",
        "pass\n",
        "\n",
        "\n",
        "# problem1\n",
        "def _linear_hypothesis(self):\n",
        "  # estimates are calculated here\n",
        "  \"\"\n",
        "\"\n",
        "Hypothetical\n",
        "function \"\"\n",
        "\"\n",
        "pass\n",
        "\n",
        "# problem2\n",
        "def _gradient_descent(self):\n",
        "  \"\"\n",
        "\"\n",
        "Calculation of updated parameter values using the steepest descent method.\n",
        "\"\"\n",
        "\"\n",
        "pass\n",
        "\n",
        "# problem3\n",
        "def predict(self):\n",
        "  \"\"\n",
        "\"\n",
        "Estimation by linear regression.\n",
        "\"\"\n",
        "\"\n",
        "pass\n",
        "\n",
        "# problem4\n",
        "def _mse(self):\n",
        "  \"\"\n",
        "\"\n",
        "Calculation of mean square error \"\"\n",
        "\"\n",
        "pass\n",
        "\n",
        "# problem5\n",
        "def _loss_func(self):\n",
        "  \"\"\n",
        "\"\n",
        "loss\n",
        "function \"\"\n",
        "\"\n",
        "pass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# y=ax1+b\n",
        "a = 1\n",
        "b = 2\n",
        "x1 = 3\n",
        "y = a * x1 + b\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipee3IY--2qk",
        "outputId": "4d521ff8-e688-4d60-b0e2-753c5edd5bbc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "theta = np.array([[b], [a]])\n",
        "X = np.array([[1, x1]])\n",
        "y = X @ theta\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e03B_r3W-32O",
        "outputId": "95143521-a1aa-45f5-e62a-32ca6a7e2666"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[5]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _gradient_descent(self, X, y):\n",
        "    m = X.shape[0]\n",
        "    n = X.shape[1]\n",
        "    pred = self.linear_hypothesis(X)\n",
        "    for j in range(n):\n",
        "        gradient = 0\n",
        "        for i in range(m):\n",
        "            gradient += (pred[i] - y[i]) * X[i, j]\n",
        "        self.theta[j] = self.theta[j] - self.lr * (gradient / m)"
      ],
      "metadata": {
        "id": "GyAMbw-PBF1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(self, X):\n",
        "    if self.bias == True:\n",
        "        bias = np.ones(X.shape[0]).reshape(X.shape[0], 1)\n",
        "        X = np.hstack([bias, X])\n",
        "    pred_y = self._linear_hypothesis(X)\n",
        "    return pred_y"
      ],
      "metadata": {
        "id": "-Xcx2t44Cs-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _loss_func(self, y_pred, y):\n",
        "    loss = self.MSE(pred, y) / 2\n",
        "    return loss"
      ],
      "metadata": {
        "id": "4XQerNKnbs7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def MSE(self, y_pred, y):\n",
        "    mse = ((y_pred - y) ** 2).sum() / X.shape[0]\n",
        "    return mse"
      ],
      "metadata": {
        "id": "eJUAwNdEcBWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.plot(reg.loss)\n",
        "plt.plot(reg.val_loss)"
      ],
      "metadata": {
        "id": "rQ4qLfEohx7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(self, X, y, X_val, y_val):\n",
        "  if self.bias==True:\n",
        "    bias=np.ones((X.shape[0], 1)) X=np.hstack((bias, X))\n",
        "    bias=np.ones((X_val.shape[0], 1)) X_val=np.hstack((bias, X_val))\n",
        "    self.theta=np.zeros(X.shape[1])\n",
        "    self.theta=self.theta.reshape(X.shape[1], 1)\n",
        "    for i in range(self.num_iter):\n",
        "      pred=self._linear_hypothesis(X)\n",
        "      pred_val=self._linear_hypothesis(X_val) self._gradient_descent(X, y)\n",
        "      loss=self._loss_func(pred, y) self.loss=np.append(self.loss, loss)\n",
        "      loss_val=self._loss_func(pred_val, y_val) self.val_loss=np.append(self.val_loss, loss_val)\n",
        "      if verbose==True:\n",
        "        print('{}The loss of the second study was{}' .format(i, loss))\n"
      ],
      "metadata": {
        "id": "ohqgm2P4igT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "dataset = pd.read_csv(\"train.csv\")\n",
        "X = dataset.loc[:, [\"GrLivArea\", \"YearBuilt\"]]\n",
        "y = dataset.loc[:, [\"SalePrice\"]]\n",
        "X = X.values\n",
        "y = y.values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)"
      ],
      "metadata": {
        "id": "RQcLiGSWjfho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "slr = ScratchLinearRegression(num_iter=10, lr=0.01, no_bias=True, verbose=True)\n",
        "slr.fit(X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "id": "BtBvwuUSjkdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "slr.predict(X_test)"
      ],
      "metadata": {
        "id": "A4Lpii6wjnKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression Assingment"
      ],
      "metadata": {
        "id": "8r5Z9e8-j2Hf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScratchLinearRegression:\n",
        "    \"\"\"\n",
        "    Scratch implementation of linear regression\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    num_iter : int\n",
        "      Number of iterations\n",
        "    lr : float\n",
        "      Learning rate\n",
        "    no_bias : bool\n",
        "      True if no bias term is included\n",
        "    verbose : bool\n",
        "      True to output the learning process\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    self.coef_ : of the following form. ndarray, shape (n_features,)\n",
        "      Parameters\n",
        "    self.loss : of the following form. ndarray, shape (self.iter,)\n",
        "      Record losses on training data\n",
        "    self.val_loss : of the following form. ndarray, shape (self.iter,)\n",
        "      Record loss on validation data\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_iter, lr, no_bias, verbose):\n",
        "        # Record hyperparameters as attributes\n",
        "        self.iter = num_iter\n",
        "        self.lr = lr\n",
        "        self.no_bias = no_bias\n",
        "        self.verbose = verbose\n",
        "        # Prepare an array to record the loss\n",
        "        self.loss = np.zeros(self.iter)\n",
        "        self.val_loss = np.zeros(self.iter)\n",
        "\n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        \"\"\"\n",
        "        Learn linear regression. If validation data is entered, the loss and accuracy for it are also calculated for each iteration.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : of the following form. ndarray, shape (n_samples, n_features)\n",
        "            Features of training data\n",
        "        y : of the following form. ndarray, shape (n_samples, )\n",
        "            Correct answer value of training data\n",
        "        X_val : of the following form. ndarray, shape (n_samples, n_features)\n",
        "            Features of verification data\n",
        "        y_val : of the following form. ndarray, shape (n_samples, )\n",
        "            Correct value of verification data\n",
        "        \"\"\"\n",
        "        if self.verbose:\n",
        "            # Output learning process when verbose is set to True\n",
        "            print()\n",
        "        pass\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Estimate using linear regression.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : of the following form. ndarray, shape (n_samples, n_features)\n",
        "            sample\n",
        "        Returns\n",
        "        -------\n",
        "            of the following form. ndarray, shape (n_samples, 1)\n",
        "            Estimated result by linear regression\n",
        "        \"\"\"\n",
        "        pass\n",
        "        return"
      ],
      "metadata": {
        "id": "2iEi5zU3qfiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 1\n"
      ],
      "metadata": {
        "id": "srOEQMXgpPnu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class ScratchLogisticRegression():\n",
        "    def __init__(self, num_iter, lr, bias, verbose):\n",
        "        self.iter = num_iter\n",
        "        self.lr = lr\n",
        "        self.bias = bias\n",
        "        self.verbose = verbose\n",
        "        self.loss = np.zeros(self.iter)\n",
        "        self.val_loss = np.zeros(self.iter)\n",
        "\n",
        "    def _sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Add bias column if needed\n",
        "        if self.bias:\n",
        "            X = np.c_[np.ones(n_samples), X]\n",
        "            if X_val is not None:\n",
        "                X_val = np.c_[np.ones(X_val.shape[0]), X_val]\n",
        "\n",
        "        # Initialize weights\n",
        "        self.coef_ = np.zeros(X.shape[1])\n",
        "\n",
        "        if self.verbose:\n",
        "            # Output the learning process if verbose is True\n",
        "            print(\"Initialized weights:\", self.coef_)\n",
        "\n",
        "        # Training logic will be added here later (gradient descent)\n",
        "        pass\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        if self.bias:\n",
        "            X = np.c_[np.ones(X.shape[0]), X]\n",
        "        z = np.dot(X, self.coef_)\n",
        "        return self._sigmoid(z).reshape(-1, 1)\n",
        "\n",
        "    def predict(self, X):\n",
        "        proba = self.predict_proba(X)\n",
        "        return (proba >= 0.5).astype(int)\n"
      ],
      "metadata": {
        "id": "IOzJPlvyj9GO"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "problem 2"
      ],
      "metadata": {
        "id": "W6dqxBTvu2r0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _gradient_descent(self, X, y):\n",
        "    m, n = X.shape\n",
        "    pred = self._logistic_hypothesis(X)\n",
        "\n",
        "    for j in range(n):\n",
        "        gradient = 0\n",
        "        for i in range(m):\n",
        "            gradient += (pred[i] - y[i]) * X[i, j]\n",
        "\n",
        "        if j == 0:\n",
        "            self.coef_[j] -= self.lr * (gradient / m)\n",
        "        else:\n",
        "            self.coef_[j] -= self.lr * ((gradient + self.lam * self.coef_[j]) / m)\n"
      ],
      "metadata": {
        "id": "w9j4Hx-2u2Iy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 3"
      ],
      "metadata": {
        "id": "poM1TlosxPHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class ScratchLinearRegression():\n",
        "    def __init__(self, num_iter, lr, bias=True, verbose=False, lam=0.0):\n",
        "        self.iter = num_iter\n",
        "        self.lr = lr\n",
        "        self.bias = bias\n",
        "        self.verbose = verbose\n",
        "        self.lam = lam\n",
        "        self.loss = np.zeros(self.iter)\n",
        "        self.val_loss = np.zeros(self.iter)\n",
        "\n",
        "    def _sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def _gradient_descent(self, X, y):\n",
        "        m, n = X.shape\n",
        "        pred = self.predict_proba(X).flatten()\n",
        "\n",
        "        for j in range(n):\n",
        "            gradient = np.sum((pred - y) * X[:, j])\n",
        "\n",
        "            if j == 0:\n",
        "                self.coef_[j] -= self.lr * (gradient / m)\n",
        "            else:\n",
        "                self.coef_[j] -= self.lr * ((gradient + self.lam * self.coef_[j]) / m)\n",
        "\n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        if self.bias:\n",
        "            X = np.c_[np.ones(n_samples), X]\n",
        "            if X_val is not None:\n",
        "                X_val = np.c_[np.ones(X_val.shape[0]), X_val]\n",
        "\n",
        "        self.coef_ = np.zeros(X.shape[1])\n",
        "\n",
        "        for i in range(self.iter):\n",
        "            self._gradient_descent(X, y)\n",
        "\n",
        "            if X_val is not None and y_val is not None:\n",
        "                val_pred = self.predict_proba(X_val).flatten()\n",
        "                val_loss = -np.mean(y_val * np.log(val_pred + 1e-15) + (1 - y_val) * np.log(1 - val_pred + 1e-15))\n",
        "                self.val_loss[i] = val_loss\n",
        "\n",
        "            if self.verbose:\n",
        "                print(f\"Iteration {i+1}, Loss: {loss:.4f}\")\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        if self.bias:\n",
        "            X = np.c_[np.ones(X.shape[0]), X]\n",
        "        z = np.dot(X, self.coef_)\n",
        "        return self._sigmoid(z).reshape(-1, 1)\n",
        "\n",
        "    def predict(self, X):\n",
        "        proba = self.predict_proba(X)\n",
        "        return (proba >= 0.5).astype(int)\n"
      ],
      "metadata": {
        "id": "EcGVgeP-xOux"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "problem 4"
      ],
      "metadata": {
        "id": "LhFlcW4IvJR2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class ScratchLinearRegression():\n",
        "    def __init__(self, num_iter=1000, lr=0.01, bias=True, verbose=False):\n",
        "        self.iter = num_iter\n",
        "        self.lr = lr\n",
        "        self.bias = bias\n",
        "        self.verbose = verbose\n",
        "        self.loss = np.zeros(self.iter)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        m, n = X.shape\n",
        "\n",
        "        if self.bias:\n",
        "            X = np.c_[np.ones((m, 1)), X]\n",
        "\n",
        "        self.coef_ = np.zeros(X.shape[1])\n",
        "\n",
        "        for i in range(self.iter):\n",
        "            y_pred = X.dot(self.coef_)\n",
        "            error = y_pred - y\n",
        "\n",
        "            # Calcula o gradiente (derivada do erro quadrático médio)\n",
        "            gradient = (1 / m) * X.T.dot(error)\n",
        "            self.coef_ -= self.lr * gradient\n",
        "\n",
        "            # Calcula o MSE da iteração atual e armazena\n",
        "            mse = np.mean(error ** 2)\n",
        "            self.loss[i] = mse\n",
        "\n",
        "            if self.verbose:\n",
        "                print(f\"Iteração {i+1}: MSE = {mse:.6f}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        if self.bias:\n",
        "            X = np.c_[np.ones((X.shape[0], 1)), X]\n",
        "        return X.dot(self.coef_)\n",
        "\n",
        "    def mean_squared_error(self, X, y):\n",
        "        y_pred = self.predict(X)\n",
        "        mse = np.mean((y - y_pred) ** 2)\n",
        "        return mse\n"
      ],
      "metadata": {
        "id": "HDVpX0f6vKTA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Small teste to see if its actually working"
      ],
      "metadata": {
        "id": "uAwSYWqGzM5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.array([[1], [2], [3]])\n",
        "y_train = np.array([2, 4, 6])\n",
        "\n",
        "model = ScratchLinearRegression(num_iter=1000, lr=0.01, verbose=True)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "X_test = np.array([[4], [5]])\n",
        "y_test = np.array([8, 10])\n",
        "\n",
        "print(\"Prediction:\", model.predict(X_test))\n",
        "print(\"MSE :\", model.mean_squared_error(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8lrx8iGNzQ3i",
        "outputId": "39b50614-3e7f-44d9-bfbf-ac212c3b348c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteração 1: MSE = 18.666667\n",
            "Iteração 2: MSE = 16.661630\n",
            "Iteração 3: MSE = 14.872822\n",
            "Iteração 4: MSE = 13.276924\n",
            "Iteração 5: MSE = 11.853128\n",
            "Iteração 6: MSE = 10.582873\n",
            "Iteração 7: MSE = 9.449600\n",
            "Iteração 8: MSE = 8.438534\n",
            "Iteração 9: MSE = 7.536495\n",
            "Iteração 10: MSE = 6.731725\n",
            "Iteração 11: MSE = 6.013732\n",
            "Iteração 12: MSE = 5.373157\n",
            "Iteração 13: MSE = 4.801651\n",
            "Iteração 14: MSE = 4.291765\n",
            "Iteração 15: MSE = 3.836852\n",
            "Iteração 16: MSE = 3.430984\n",
            "Iteração 17: MSE = 3.068871\n",
            "Iteração 18: MSE = 2.745793\n",
            "Iteração 19: MSE = 2.457542\n",
            "Iteração 20: MSE = 2.200360\n",
            "Iteração 21: MSE = 1.970898\n",
            "Iteração 22: MSE = 1.766165\n",
            "Iteração 23: MSE = 1.583494\n",
            "Iteração 24: MSE = 1.420506\n",
            "Iteração 25: MSE = 1.275078\n",
            "Iteração 26: MSE = 1.145316\n",
            "Iteração 27: MSE = 1.029530\n",
            "Iteração 28: MSE = 0.926214\n",
            "Iteração 29: MSE = 0.834023\n",
            "Iteração 30: MSE = 0.751755\n",
            "Iteração 31: MSE = 0.678343\n",
            "Iteração 32: MSE = 0.612829\n",
            "Iteração 33: MSE = 0.554363\n",
            "Iteração 34: MSE = 0.502185\n",
            "Iteração 35: MSE = 0.455616\n",
            "Iteração 36: MSE = 0.414051\n",
            "Iteração 37: MSE = 0.376951\n",
            "Iteração 38: MSE = 0.343834\n",
            "Iteração 39: MSE = 0.314271\n",
            "Iteração 40: MSE = 0.287878\n",
            "Iteração 41: MSE = 0.264313\n",
            "Iteração 42: MSE = 0.243273\n",
            "Iteração 43: MSE = 0.224483\n",
            "Iteração 44: MSE = 0.207702\n",
            "Iteração 45: MSE = 0.192713\n",
            "Iteração 46: MSE = 0.179323\n",
            "Iteração 47: MSE = 0.167360\n",
            "Iteração 48: MSE = 0.156669\n",
            "Iteração 49: MSE = 0.147113\n",
            "Iteração 50: MSE = 0.138570\n",
            "Iteração 51: MSE = 0.130931\n",
            "Iteração 52: MSE = 0.124099\n",
            "Iteração 53: MSE = 0.117986\n",
            "Iteração 54: MSE = 0.112514\n",
            "Iteração 55: MSE = 0.107616\n",
            "Iteração 56: MSE = 0.103228\n",
            "Iteração 57: MSE = 0.099296\n",
            "Iteração 58: MSE = 0.095771\n",
            "Iteração 59: MSE = 0.092609\n",
            "Iteração 60: MSE = 0.089771\n",
            "Iteração 61: MSE = 0.087221\n",
            "Iteração 62: MSE = 0.084930\n",
            "Iteração 63: MSE = 0.082868\n",
            "Iteração 64: MSE = 0.081012\n",
            "Iteração 65: MSE = 0.079339\n",
            "Iteração 66: MSE = 0.077829\n",
            "Iteração 67: MSE = 0.076466\n",
            "Iteração 68: MSE = 0.075232\n",
            "Iteração 69: MSE = 0.074115\n",
            "Iteração 70: MSE = 0.073101\n",
            "Iteração 71: MSE = 0.072180\n",
            "Iteração 72: MSE = 0.071342\n",
            "Iteração 73: MSE = 0.070577\n",
            "Iteração 74: MSE = 0.069878\n",
            "Iteração 75: MSE = 0.069238\n",
            "Iteração 76: MSE = 0.068650\n",
            "Iteração 77: MSE = 0.068110\n",
            "Iteração 78: MSE = 0.067611\n",
            "Iteração 79: MSE = 0.067149\n",
            "Iteração 80: MSE = 0.066721\n",
            "Iteração 81: MSE = 0.066322\n",
            "Iteração 82: MSE = 0.065951\n",
            "Iteração 83: MSE = 0.065603\n",
            "Iteração 84: MSE = 0.065276\n",
            "Iteração 85: MSE = 0.064968\n",
            "Iteração 86: MSE = 0.064678\n",
            "Iteração 87: MSE = 0.064402\n",
            "Iteração 88: MSE = 0.064140\n",
            "Iteração 89: MSE = 0.063891\n",
            "Iteração 90: MSE = 0.063652\n",
            "Iteração 91: MSE = 0.063423\n",
            "Iteração 92: MSE = 0.063203\n",
            "Iteração 93: MSE = 0.062990\n",
            "Iteração 94: MSE = 0.062785\n",
            "Iteração 95: MSE = 0.062586\n",
            "Iteração 96: MSE = 0.062393\n",
            "Iteração 97: MSE = 0.062204\n",
            "Iteração 98: MSE = 0.062021\n",
            "Iteração 99: MSE = 0.061841\n",
            "Iteração 100: MSE = 0.061665\n",
            "Iteração 101: MSE = 0.061493\n",
            "Iteração 102: MSE = 0.061324\n",
            "Iteração 103: MSE = 0.061157\n",
            "Iteração 104: MSE = 0.060993\n",
            "Iteração 105: MSE = 0.060831\n",
            "Iteração 106: MSE = 0.060671\n",
            "Iteração 107: MSE = 0.060513\n",
            "Iteração 108: MSE = 0.060357\n",
            "Iteração 109: MSE = 0.060202\n",
            "Iteração 110: MSE = 0.060049\n",
            "Iteração 111: MSE = 0.059897\n",
            "Iteração 112: MSE = 0.059746\n",
            "Iteração 113: MSE = 0.059596\n",
            "Iteração 114: MSE = 0.059447\n",
            "Iteração 115: MSE = 0.059300\n",
            "Iteração 116: MSE = 0.059153\n",
            "Iteração 117: MSE = 0.059007\n",
            "Iteração 118: MSE = 0.058861\n",
            "Iteração 119: MSE = 0.058717\n",
            "Iteração 120: MSE = 0.058573\n",
            "Iteração 121: MSE = 0.058430\n",
            "Iteração 122: MSE = 0.058287\n",
            "Iteração 123: MSE = 0.058145\n",
            "Iteração 124: MSE = 0.058004\n",
            "Iteração 125: MSE = 0.057863\n",
            "Iteração 126: MSE = 0.057722\n",
            "Iteração 127: MSE = 0.057583\n",
            "Iteração 128: MSE = 0.057443\n",
            "Iteração 129: MSE = 0.057304\n",
            "Iteração 130: MSE = 0.057166\n",
            "Iteração 131: MSE = 0.057027\n",
            "Iteração 132: MSE = 0.056890\n",
            "Iteração 133: MSE = 0.056752\n",
            "Iteração 134: MSE = 0.056615\n",
            "Iteração 135: MSE = 0.056479\n",
            "Iteração 136: MSE = 0.056343\n",
            "Iteração 137: MSE = 0.056207\n",
            "Iteração 138: MSE = 0.056072\n",
            "Iteração 139: MSE = 0.055937\n",
            "Iteração 140: MSE = 0.055802\n",
            "Iteração 141: MSE = 0.055668\n",
            "Iteração 142: MSE = 0.055534\n",
            "Iteração 143: MSE = 0.055400\n",
            "Iteração 144: MSE = 0.055267\n",
            "Iteração 145: MSE = 0.055134\n",
            "Iteração 146: MSE = 0.055001\n",
            "Iteração 147: MSE = 0.054869\n",
            "Iteração 148: MSE = 0.054737\n",
            "Iteração 149: MSE = 0.054605\n",
            "Iteração 150: MSE = 0.054474\n",
            "Iteração 151: MSE = 0.054343\n",
            "Iteração 152: MSE = 0.054213\n",
            "Iteração 153: MSE = 0.054082\n",
            "Iteração 154: MSE = 0.053952\n",
            "Iteração 155: MSE = 0.053823\n",
            "Iteração 156: MSE = 0.053693\n",
            "Iteração 157: MSE = 0.053564\n",
            "Iteração 158: MSE = 0.053435\n",
            "Iteração 159: MSE = 0.053307\n",
            "Iteração 160: MSE = 0.053179\n",
            "Iteração 161: MSE = 0.053051\n",
            "Iteração 162: MSE = 0.052924\n",
            "Iteração 163: MSE = 0.052797\n",
            "Iteração 164: MSE = 0.052670\n",
            "Iteração 165: MSE = 0.052543\n",
            "Iteração 166: MSE = 0.052417\n",
            "Iteração 167: MSE = 0.052291\n",
            "Iteração 168: MSE = 0.052165\n",
            "Iteração 169: MSE = 0.052040\n",
            "Iteração 170: MSE = 0.051915\n",
            "Iteração 171: MSE = 0.051790\n",
            "Iteração 172: MSE = 0.051666\n",
            "Iteração 173: MSE = 0.051542\n",
            "Iteração 174: MSE = 0.051418\n",
            "Iteração 175: MSE = 0.051294\n",
            "Iteração 176: MSE = 0.051171\n",
            "Iteração 177: MSE = 0.051048\n",
            "Iteração 178: MSE = 0.050925\n",
            "Iteração 179: MSE = 0.050803\n",
            "Iteração 180: MSE = 0.050681\n",
            "Iteração 181: MSE = 0.050559\n",
            "Iteração 182: MSE = 0.050438\n",
            "Iteração 183: MSE = 0.050317\n",
            "Iteração 184: MSE = 0.050196\n",
            "Iteração 185: MSE = 0.050075\n",
            "Iteração 186: MSE = 0.049955\n",
            "Iteração 187: MSE = 0.049835\n",
            "Iteração 188: MSE = 0.049715\n",
            "Iteração 189: MSE = 0.049596\n",
            "Iteração 190: MSE = 0.049476\n",
            "Iteração 191: MSE = 0.049358\n",
            "Iteração 192: MSE = 0.049239\n",
            "Iteração 193: MSE = 0.049121\n",
            "Iteração 194: MSE = 0.049003\n",
            "Iteração 195: MSE = 0.048885\n",
            "Iteração 196: MSE = 0.048768\n",
            "Iteração 197: MSE = 0.048650\n",
            "Iteração 198: MSE = 0.048533\n",
            "Iteração 199: MSE = 0.048417\n",
            "Iteração 200: MSE = 0.048301\n",
            "Iteração 201: MSE = 0.048185\n",
            "Iteração 202: MSE = 0.048069\n",
            "Iteração 203: MSE = 0.047953\n",
            "Iteração 204: MSE = 0.047838\n",
            "Iteração 205: MSE = 0.047723\n",
            "Iteração 206: MSE = 0.047608\n",
            "Iteração 207: MSE = 0.047494\n",
            "Iteração 208: MSE = 0.047380\n",
            "Iteração 209: MSE = 0.047266\n",
            "Iteração 210: MSE = 0.047153\n",
            "Iteração 211: MSE = 0.047039\n",
            "Iteração 212: MSE = 0.046926\n",
            "Iteração 213: MSE = 0.046814\n",
            "Iteração 214: MSE = 0.046701\n",
            "Iteração 215: MSE = 0.046589\n",
            "Iteração 216: MSE = 0.046477\n",
            "Iteração 217: MSE = 0.046365\n",
            "Iteração 218: MSE = 0.046254\n",
            "Iteração 219: MSE = 0.046143\n",
            "Iteração 220: MSE = 0.046032\n",
            "Iteração 221: MSE = 0.045921\n",
            "Iteração 222: MSE = 0.045811\n",
            "Iteração 223: MSE = 0.045701\n",
            "Iteração 224: MSE = 0.045591\n",
            "Iteração 225: MSE = 0.045482\n",
            "Iteração 226: MSE = 0.045372\n",
            "Iteração 227: MSE = 0.045263\n",
            "Iteração 228: MSE = 0.045155\n",
            "Iteração 229: MSE = 0.045046\n",
            "Iteração 230: MSE = 0.044938\n",
            "Iteração 231: MSE = 0.044830\n",
            "Iteração 232: MSE = 0.044722\n",
            "Iteração 233: MSE = 0.044615\n",
            "Iteração 234: MSE = 0.044508\n",
            "Iteração 235: MSE = 0.044401\n",
            "Iteração 236: MSE = 0.044294\n",
            "Iteração 237: MSE = 0.044188\n",
            "Iteração 238: MSE = 0.044081\n",
            "Iteração 239: MSE = 0.043976\n",
            "Iteração 240: MSE = 0.043870\n",
            "Iteração 241: MSE = 0.043764\n",
            "Iteração 242: MSE = 0.043659\n",
            "Iteração 243: MSE = 0.043554\n",
            "Iteração 244: MSE = 0.043450\n",
            "Iteração 245: MSE = 0.043345\n",
            "Iteração 246: MSE = 0.043241\n",
            "Iteração 247: MSE = 0.043137\n",
            "Iteração 248: MSE = 0.043034\n",
            "Iteração 249: MSE = 0.042930\n",
            "Iteração 250: MSE = 0.042827\n",
            "Iteração 251: MSE = 0.042724\n",
            "Iteração 252: MSE = 0.042622\n",
            "Iteração 253: MSE = 0.042519\n",
            "Iteração 254: MSE = 0.042417\n",
            "Iteração 255: MSE = 0.042315\n",
            "Iteração 256: MSE = 0.042214\n",
            "Iteração 257: MSE = 0.042112\n",
            "Iteração 258: MSE = 0.042011\n",
            "Iteração 259: MSE = 0.041910\n",
            "Iteração 260: MSE = 0.041809\n",
            "Iteração 261: MSE = 0.041709\n",
            "Iteração 262: MSE = 0.041609\n",
            "Iteração 263: MSE = 0.041509\n",
            "Iteração 264: MSE = 0.041409\n",
            "Iteração 265: MSE = 0.041310\n",
            "Iteração 266: MSE = 0.041210\n",
            "Iteração 267: MSE = 0.041111\n",
            "Iteração 268: MSE = 0.041013\n",
            "Iteração 269: MSE = 0.040914\n",
            "Iteração 270: MSE = 0.040816\n",
            "Iteração 271: MSE = 0.040718\n",
            "Iteração 272: MSE = 0.040620\n",
            "Iteração 273: MSE = 0.040522\n",
            "Iteração 274: MSE = 0.040425\n",
            "Iteração 275: MSE = 0.040328\n",
            "Iteração 276: MSE = 0.040231\n",
            "Iteração 277: MSE = 0.040134\n",
            "Iteração 278: MSE = 0.040038\n",
            "Iteração 279: MSE = 0.039942\n",
            "Iteração 280: MSE = 0.039846\n",
            "Iteração 281: MSE = 0.039750\n",
            "Iteração 282: MSE = 0.039654\n",
            "Iteração 283: MSE = 0.039559\n",
            "Iteração 284: MSE = 0.039464\n",
            "Iteração 285: MSE = 0.039369\n",
            "Iteração 286: MSE = 0.039275\n",
            "Iteração 287: MSE = 0.039180\n",
            "Iteração 288: MSE = 0.039086\n",
            "Iteração 289: MSE = 0.038992\n",
            "Iteração 290: MSE = 0.038899\n",
            "Iteração 291: MSE = 0.038805\n",
            "Iteração 292: MSE = 0.038712\n",
            "Iteração 293: MSE = 0.038619\n",
            "Iteração 294: MSE = 0.038526\n",
            "Iteração 295: MSE = 0.038434\n",
            "Iteração 296: MSE = 0.038341\n",
            "Iteração 297: MSE = 0.038249\n",
            "Iteração 298: MSE = 0.038157\n",
            "Iteração 299: MSE = 0.038066\n",
            "Iteração 300: MSE = 0.037974\n",
            "Iteração 301: MSE = 0.037883\n",
            "Iteração 302: MSE = 0.037792\n",
            "Iteração 303: MSE = 0.037701\n",
            "Iteração 304: MSE = 0.037611\n",
            "Iteração 305: MSE = 0.037520\n",
            "Iteração 306: MSE = 0.037430\n",
            "Iteração 307: MSE = 0.037340\n",
            "Iteração 308: MSE = 0.037250\n",
            "Iteração 309: MSE = 0.037161\n",
            "Iteração 310: MSE = 0.037072\n",
            "Iteração 311: MSE = 0.036983\n",
            "Iteração 312: MSE = 0.036894\n",
            "Iteração 313: MSE = 0.036805\n",
            "Iteração 314: MSE = 0.036717\n",
            "Iteração 315: MSE = 0.036628\n",
            "Iteração 316: MSE = 0.036540\n",
            "Iteração 317: MSE = 0.036453\n",
            "Iteração 318: MSE = 0.036365\n",
            "Iteração 319: MSE = 0.036278\n",
            "Iteração 320: MSE = 0.036191\n",
            "Iteração 321: MSE = 0.036104\n",
            "Iteração 322: MSE = 0.036017\n",
            "Iteração 323: MSE = 0.035930\n",
            "Iteração 324: MSE = 0.035844\n",
            "Iteração 325: MSE = 0.035758\n",
            "Iteração 326: MSE = 0.035672\n",
            "Iteração 327: MSE = 0.035586\n",
            "Iteração 328: MSE = 0.035501\n",
            "Iteração 329: MSE = 0.035416\n",
            "Iteração 330: MSE = 0.035330\n",
            "Iteração 331: MSE = 0.035246\n",
            "Iteração 332: MSE = 0.035161\n",
            "Iteração 333: MSE = 0.035076\n",
            "Iteração 334: MSE = 0.034992\n",
            "Iteração 335: MSE = 0.034908\n",
            "Iteração 336: MSE = 0.034824\n",
            "Iteração 337: MSE = 0.034741\n",
            "Iteração 338: MSE = 0.034657\n",
            "Iteração 339: MSE = 0.034574\n",
            "Iteração 340: MSE = 0.034491\n",
            "Iteração 341: MSE = 0.034408\n",
            "Iteração 342: MSE = 0.034325\n",
            "Iteração 343: MSE = 0.034243\n",
            "Iteração 344: MSE = 0.034160\n",
            "Iteração 345: MSE = 0.034078\n",
            "Iteração 346: MSE = 0.033997\n",
            "Iteração 347: MSE = 0.033915\n",
            "Iteração 348: MSE = 0.033833\n",
            "Iteração 349: MSE = 0.033752\n",
            "Iteração 350: MSE = 0.033671\n",
            "Iteração 351: MSE = 0.033590\n",
            "Iteração 352: MSE = 0.033509\n",
            "Iteração 353: MSE = 0.033429\n",
            "Iteração 354: MSE = 0.033349\n",
            "Iteração 355: MSE = 0.033268\n",
            "Iteração 356: MSE = 0.033189\n",
            "Iteração 357: MSE = 0.033109\n",
            "Iteração 358: MSE = 0.033029\n",
            "Iteração 359: MSE = 0.032950\n",
            "Iteração 360: MSE = 0.032871\n",
            "Iteração 361: MSE = 0.032792\n",
            "Iteração 362: MSE = 0.032713\n",
            "Iteração 363: MSE = 0.032634\n",
            "Iteração 364: MSE = 0.032556\n",
            "Iteração 365: MSE = 0.032478\n",
            "Iteração 366: MSE = 0.032400\n",
            "Iteração 367: MSE = 0.032322\n",
            "Iteração 368: MSE = 0.032244\n",
            "Iteração 369: MSE = 0.032167\n",
            "Iteração 370: MSE = 0.032090\n",
            "Iteração 371: MSE = 0.032012\n",
            "Iteração 372: MSE = 0.031936\n",
            "Iteração 373: MSE = 0.031859\n",
            "Iteração 374: MSE = 0.031782\n",
            "Iteração 375: MSE = 0.031706\n",
            "Iteração 376: MSE = 0.031630\n",
            "Iteração 377: MSE = 0.031554\n",
            "Iteração 378: MSE = 0.031478\n",
            "Iteração 379: MSE = 0.031402\n",
            "Iteração 380: MSE = 0.031327\n",
            "Iteração 381: MSE = 0.031252\n",
            "Iteração 382: MSE = 0.031177\n",
            "Iteração 383: MSE = 0.031102\n",
            "Iteração 384: MSE = 0.031027\n",
            "Iteração 385: MSE = 0.030952\n",
            "Iteração 386: MSE = 0.030878\n",
            "Iteração 387: MSE = 0.030804\n",
            "Iteração 388: MSE = 0.030730\n",
            "Iteração 389: MSE = 0.030656\n",
            "Iteração 390: MSE = 0.030582\n",
            "Iteração 391: MSE = 0.030509\n",
            "Iteração 392: MSE = 0.030436\n",
            "Iteração 393: MSE = 0.030362\n",
            "Iteração 394: MSE = 0.030289\n",
            "Iteração 395: MSE = 0.030217\n",
            "Iteração 396: MSE = 0.030144\n",
            "Iteração 397: MSE = 0.030072\n",
            "Iteração 398: MSE = 0.029999\n",
            "Iteração 399: MSE = 0.029927\n",
            "Iteração 400: MSE = 0.029855\n",
            "Iteração 401: MSE = 0.029784\n",
            "Iteração 402: MSE = 0.029712\n",
            "Iteração 403: MSE = 0.029641\n",
            "Iteração 404: MSE = 0.029570\n",
            "Iteração 405: MSE = 0.029499\n",
            "Iteração 406: MSE = 0.029428\n",
            "Iteração 407: MSE = 0.029357\n",
            "Iteração 408: MSE = 0.029286\n",
            "Iteração 409: MSE = 0.029216\n",
            "Iteração 410: MSE = 0.029146\n",
            "Iteração 411: MSE = 0.029076\n",
            "Iteração 412: MSE = 0.029006\n",
            "Iteração 413: MSE = 0.028936\n",
            "Iteração 414: MSE = 0.028867\n",
            "Iteração 415: MSE = 0.028797\n",
            "Iteração 416: MSE = 0.028728\n",
            "Iteração 417: MSE = 0.028659\n",
            "Iteração 418: MSE = 0.028590\n",
            "Iteração 419: MSE = 0.028522\n",
            "Iteração 420: MSE = 0.028453\n",
            "Iteração 421: MSE = 0.028385\n",
            "Iteração 422: MSE = 0.028317\n",
            "Iteração 423: MSE = 0.028249\n",
            "Iteração 424: MSE = 0.028181\n",
            "Iteração 425: MSE = 0.028113\n",
            "Iteração 426: MSE = 0.028045\n",
            "Iteração 427: MSE = 0.027978\n",
            "Iteração 428: MSE = 0.027911\n",
            "Iteração 429: MSE = 0.027844\n",
            "Iteração 430: MSE = 0.027777\n",
            "Iteração 431: MSE = 0.027710\n",
            "Iteração 432: MSE = 0.027644\n",
            "Iteração 433: MSE = 0.027577\n",
            "Iteração 434: MSE = 0.027511\n",
            "Iteração 435: MSE = 0.027445\n",
            "Iteração 436: MSE = 0.027379\n",
            "Iteração 437: MSE = 0.027313\n",
            "Iteração 438: MSE = 0.027248\n",
            "Iteração 439: MSE = 0.027182\n",
            "Iteração 440: MSE = 0.027117\n",
            "Iteração 441: MSE = 0.027052\n",
            "Iteração 442: MSE = 0.026987\n",
            "Iteração 443: MSE = 0.026922\n",
            "Iteração 444: MSE = 0.026857\n",
            "Iteração 445: MSE = 0.026793\n",
            "Iteração 446: MSE = 0.026728\n",
            "Iteração 447: MSE = 0.026664\n",
            "Iteração 448: MSE = 0.026600\n",
            "Iteração 449: MSE = 0.026536\n",
            "Iteração 450: MSE = 0.026472\n",
            "Iteração 451: MSE = 0.026409\n",
            "Iteração 452: MSE = 0.026345\n",
            "Iteração 453: MSE = 0.026282\n",
            "Iteração 454: MSE = 0.026219\n",
            "Iteração 455: MSE = 0.026156\n",
            "Iteração 456: MSE = 0.026093\n",
            "Iteração 457: MSE = 0.026030\n",
            "Iteração 458: MSE = 0.025968\n",
            "Iteração 459: MSE = 0.025905\n",
            "Iteração 460: MSE = 0.025843\n",
            "Iteração 461: MSE = 0.025781\n",
            "Iteração 462: MSE = 0.025719\n",
            "Iteração 463: MSE = 0.025657\n",
            "Iteração 464: MSE = 0.025596\n",
            "Iteração 465: MSE = 0.025534\n",
            "Iteração 466: MSE = 0.025473\n",
            "Iteração 467: MSE = 0.025412\n",
            "Iteração 468: MSE = 0.025351\n",
            "Iteração 469: MSE = 0.025290\n",
            "Iteração 470: MSE = 0.025229\n",
            "Iteração 471: MSE = 0.025168\n",
            "Iteração 472: MSE = 0.025108\n",
            "Iteração 473: MSE = 0.025048\n",
            "Iteração 474: MSE = 0.024987\n",
            "Iteração 475: MSE = 0.024927\n",
            "Iteração 476: MSE = 0.024867\n",
            "Iteração 477: MSE = 0.024808\n",
            "Iteração 478: MSE = 0.024748\n",
            "Iteração 479: MSE = 0.024689\n",
            "Iteração 480: MSE = 0.024629\n",
            "Iteração 481: MSE = 0.024570\n",
            "Iteração 482: MSE = 0.024511\n",
            "Iteração 483: MSE = 0.024452\n",
            "Iteração 484: MSE = 0.024393\n",
            "Iteração 485: MSE = 0.024335\n",
            "Iteração 486: MSE = 0.024276\n",
            "Iteração 487: MSE = 0.024218\n",
            "Iteração 488: MSE = 0.024160\n",
            "Iteração 489: MSE = 0.024102\n",
            "Iteração 490: MSE = 0.024044\n",
            "Iteração 491: MSE = 0.023986\n",
            "Iteração 492: MSE = 0.023929\n",
            "Iteração 493: MSE = 0.023871\n",
            "Iteração 494: MSE = 0.023814\n",
            "Iteração 495: MSE = 0.023757\n",
            "Iteração 496: MSE = 0.023699\n",
            "Iteração 497: MSE = 0.023643\n",
            "Iteração 498: MSE = 0.023586\n",
            "Iteração 499: MSE = 0.023529\n",
            "Iteração 500: MSE = 0.023473\n",
            "Iteração 501: MSE = 0.023416\n",
            "Iteração 502: MSE = 0.023360\n",
            "Iteração 503: MSE = 0.023304\n",
            "Iteração 504: MSE = 0.023248\n",
            "Iteração 505: MSE = 0.023192\n",
            "Iteração 506: MSE = 0.023136\n",
            "Iteração 507: MSE = 0.023081\n",
            "Iteração 508: MSE = 0.023025\n",
            "Iteração 509: MSE = 0.022970\n",
            "Iteração 510: MSE = 0.022915\n",
            "Iteração 511: MSE = 0.022860\n",
            "Iteração 512: MSE = 0.022805\n",
            "Iteração 513: MSE = 0.022750\n",
            "Iteração 514: MSE = 0.022695\n",
            "Iteração 515: MSE = 0.022641\n",
            "Iteração 516: MSE = 0.022586\n",
            "Iteração 517: MSE = 0.022532\n",
            "Iteração 518: MSE = 0.022478\n",
            "Iteração 519: MSE = 0.022424\n",
            "Iteração 520: MSE = 0.022370\n",
            "Iteração 521: MSE = 0.022316\n",
            "Iteração 522: MSE = 0.022263\n",
            "Iteração 523: MSE = 0.022209\n",
            "Iteração 524: MSE = 0.022156\n",
            "Iteração 525: MSE = 0.022103\n",
            "Iteração 526: MSE = 0.022050\n",
            "Iteração 527: MSE = 0.021997\n",
            "Iteração 528: MSE = 0.021944\n",
            "Iteração 529: MSE = 0.021891\n",
            "Iteração 530: MSE = 0.021838\n",
            "Iteração 531: MSE = 0.021786\n",
            "Iteração 532: MSE = 0.021734\n",
            "Iteração 533: MSE = 0.021681\n",
            "Iteração 534: MSE = 0.021629\n",
            "Iteração 535: MSE = 0.021577\n",
            "Iteração 536: MSE = 0.021525\n",
            "Iteração 537: MSE = 0.021474\n",
            "Iteração 538: MSE = 0.021422\n",
            "Iteração 539: MSE = 0.021371\n",
            "Iteração 540: MSE = 0.021319\n",
            "Iteração 541: MSE = 0.021268\n",
            "Iteração 542: MSE = 0.021217\n",
            "Iteração 543: MSE = 0.021166\n",
            "Iteração 544: MSE = 0.021115\n",
            "Iteração 545: MSE = 0.021064\n",
            "Iteração 546: MSE = 0.021014\n",
            "Iteração 547: MSE = 0.020963\n",
            "Iteração 548: MSE = 0.020913\n",
            "Iteração 549: MSE = 0.020863\n",
            "Iteração 550: MSE = 0.020813\n",
            "Iteração 551: MSE = 0.020763\n",
            "Iteração 552: MSE = 0.020713\n",
            "Iteração 553: MSE = 0.020663\n",
            "Iteração 554: MSE = 0.020613\n",
            "Iteração 555: MSE = 0.020564\n",
            "Iteração 556: MSE = 0.020514\n",
            "Iteração 557: MSE = 0.020465\n",
            "Iteração 558: MSE = 0.020416\n",
            "Iteração 559: MSE = 0.020367\n",
            "Iteração 560: MSE = 0.020318\n",
            "Iteração 561: MSE = 0.020269\n",
            "Iteração 562: MSE = 0.020220\n",
            "Iteração 563: MSE = 0.020172\n",
            "Iteração 564: MSE = 0.020123\n",
            "Iteração 565: MSE = 0.020075\n",
            "Iteração 566: MSE = 0.020027\n",
            "Iteração 567: MSE = 0.019979\n",
            "Iteração 568: MSE = 0.019931\n",
            "Iteração 569: MSE = 0.019883\n",
            "Iteração 570: MSE = 0.019835\n",
            "Iteração 571: MSE = 0.019787\n",
            "Iteração 572: MSE = 0.019740\n",
            "Iteração 573: MSE = 0.019692\n",
            "Iteração 574: MSE = 0.019645\n",
            "Iteração 575: MSE = 0.019598\n",
            "Iteração 576: MSE = 0.019551\n",
            "Iteração 577: MSE = 0.019504\n",
            "Iteração 578: MSE = 0.019457\n",
            "Iteração 579: MSE = 0.019410\n",
            "Iteração 580: MSE = 0.019364\n",
            "Iteração 581: MSE = 0.019317\n",
            "Iteração 582: MSE = 0.019271\n",
            "Iteração 583: MSE = 0.019224\n",
            "Iteração 584: MSE = 0.019178\n",
            "Iteração 585: MSE = 0.019132\n",
            "Iteração 586: MSE = 0.019086\n",
            "Iteração 587: MSE = 0.019040\n",
            "Iteração 588: MSE = 0.018995\n",
            "Iteração 589: MSE = 0.018949\n",
            "Iteração 590: MSE = 0.018903\n",
            "Iteração 591: MSE = 0.018858\n",
            "Iteração 592: MSE = 0.018813\n",
            "Iteração 593: MSE = 0.018768\n",
            "Iteração 594: MSE = 0.018722\n",
            "Iteração 595: MSE = 0.018677\n",
            "Iteração 596: MSE = 0.018633\n",
            "Iteração 597: MSE = 0.018588\n",
            "Iteração 598: MSE = 0.018543\n",
            "Iteração 599: MSE = 0.018499\n",
            "Iteração 600: MSE = 0.018454\n",
            "Iteração 601: MSE = 0.018410\n",
            "Iteração 602: MSE = 0.018366\n",
            "Iteração 603: MSE = 0.018322\n",
            "Iteração 604: MSE = 0.018278\n",
            "Iteração 605: MSE = 0.018234\n",
            "Iteração 606: MSE = 0.018190\n",
            "Iteração 607: MSE = 0.018146\n",
            "Iteração 608: MSE = 0.018102\n",
            "Iteração 609: MSE = 0.018059\n",
            "Iteração 610: MSE = 0.018016\n",
            "Iteração 611: MSE = 0.017972\n",
            "Iteração 612: MSE = 0.017929\n",
            "Iteração 613: MSE = 0.017886\n",
            "Iteração 614: MSE = 0.017843\n",
            "Iteração 615: MSE = 0.017800\n",
            "Iteração 616: MSE = 0.017757\n",
            "Iteração 617: MSE = 0.017715\n",
            "Iteração 618: MSE = 0.017672\n",
            "Iteração 619: MSE = 0.017630\n",
            "Iteração 620: MSE = 0.017587\n",
            "Iteração 621: MSE = 0.017545\n",
            "Iteração 622: MSE = 0.017503\n",
            "Iteração 623: MSE = 0.017461\n",
            "Iteração 624: MSE = 0.017419\n",
            "Iteração 625: MSE = 0.017377\n",
            "Iteração 626: MSE = 0.017335\n",
            "Iteração 627: MSE = 0.017294\n",
            "Iteração 628: MSE = 0.017252\n",
            "Iteração 629: MSE = 0.017211\n",
            "Iteração 630: MSE = 0.017169\n",
            "Iteração 631: MSE = 0.017128\n",
            "Iteração 632: MSE = 0.017087\n",
            "Iteração 633: MSE = 0.017046\n",
            "Iteração 634: MSE = 0.017005\n",
            "Iteração 635: MSE = 0.016964\n",
            "Iteração 636: MSE = 0.016923\n",
            "Iteração 637: MSE = 0.016883\n",
            "Iteração 638: MSE = 0.016842\n",
            "Iteração 639: MSE = 0.016802\n",
            "Iteração 640: MSE = 0.016761\n",
            "Iteração 641: MSE = 0.016721\n",
            "Iteração 642: MSE = 0.016681\n",
            "Iteração 643: MSE = 0.016641\n",
            "Iteração 644: MSE = 0.016601\n",
            "Iteração 645: MSE = 0.016561\n",
            "Iteração 646: MSE = 0.016521\n",
            "Iteração 647: MSE = 0.016482\n",
            "Iteração 648: MSE = 0.016442\n",
            "Iteração 649: MSE = 0.016402\n",
            "Iteração 650: MSE = 0.016363\n",
            "Iteração 651: MSE = 0.016324\n",
            "Iteração 652: MSE = 0.016284\n",
            "Iteração 653: MSE = 0.016245\n",
            "Iteração 654: MSE = 0.016206\n",
            "Iteração 655: MSE = 0.016167\n",
            "Iteração 656: MSE = 0.016129\n",
            "Iteração 657: MSE = 0.016090\n",
            "Iteração 658: MSE = 0.016051\n",
            "Iteração 659: MSE = 0.016013\n",
            "Iteração 660: MSE = 0.015974\n",
            "Iteração 661: MSE = 0.015936\n",
            "Iteração 662: MSE = 0.015897\n",
            "Iteração 663: MSE = 0.015859\n",
            "Iteração 664: MSE = 0.015821\n",
            "Iteração 665: MSE = 0.015783\n",
            "Iteração 666: MSE = 0.015745\n",
            "Iteração 667: MSE = 0.015707\n",
            "Iteração 668: MSE = 0.015670\n",
            "Iteração 669: MSE = 0.015632\n",
            "Iteração 670: MSE = 0.015594\n",
            "Iteração 671: MSE = 0.015557\n",
            "Iteração 672: MSE = 0.015520\n",
            "Iteração 673: MSE = 0.015482\n",
            "Iteração 674: MSE = 0.015445\n",
            "Iteração 675: MSE = 0.015408\n",
            "Iteração 676: MSE = 0.015371\n",
            "Iteração 677: MSE = 0.015334\n",
            "Iteração 678: MSE = 0.015297\n",
            "Iteração 679: MSE = 0.015260\n",
            "Iteração 680: MSE = 0.015224\n",
            "Iteração 681: MSE = 0.015187\n",
            "Iteração 682: MSE = 0.015151\n",
            "Iteração 683: MSE = 0.015114\n",
            "Iteração 684: MSE = 0.015078\n",
            "Iteração 685: MSE = 0.015042\n",
            "Iteração 686: MSE = 0.015006\n",
            "Iteração 687: MSE = 0.014970\n",
            "Iteração 688: MSE = 0.014934\n",
            "Iteração 689: MSE = 0.014898\n",
            "Iteração 690: MSE = 0.014862\n",
            "Iteração 691: MSE = 0.014826\n",
            "Iteração 692: MSE = 0.014791\n",
            "Iteração 693: MSE = 0.014755\n",
            "Iteração 694: MSE = 0.014720\n",
            "Iteração 695: MSE = 0.014684\n",
            "Iteração 696: MSE = 0.014649\n",
            "Iteração 697: MSE = 0.014614\n",
            "Iteração 698: MSE = 0.014579\n",
            "Iteração 699: MSE = 0.014544\n",
            "Iteração 700: MSE = 0.014509\n",
            "Iteração 701: MSE = 0.014474\n",
            "Iteração 702: MSE = 0.014439\n",
            "Iteração 703: MSE = 0.014404\n",
            "Iteração 704: MSE = 0.014370\n",
            "Iteração 705: MSE = 0.014335\n",
            "Iteração 706: MSE = 0.014301\n",
            "Iteração 707: MSE = 0.014267\n",
            "Iteração 708: MSE = 0.014232\n",
            "Iteração 709: MSE = 0.014198\n",
            "Iteração 710: MSE = 0.014164\n",
            "Iteração 711: MSE = 0.014130\n",
            "Iteração 712: MSE = 0.014096\n",
            "Iteração 713: MSE = 0.014062\n",
            "Iteração 714: MSE = 0.014028\n",
            "Iteração 715: MSE = 0.013995\n",
            "Iteração 716: MSE = 0.013961\n",
            "Iteração 717: MSE = 0.013927\n",
            "Iteração 718: MSE = 0.013894\n",
            "Iteração 719: MSE = 0.013861\n",
            "Iteração 720: MSE = 0.013827\n",
            "Iteração 721: MSE = 0.013794\n",
            "Iteração 722: MSE = 0.013761\n",
            "Iteração 723: MSE = 0.013728\n",
            "Iteração 724: MSE = 0.013695\n",
            "Iteração 725: MSE = 0.013662\n",
            "Iteração 726: MSE = 0.013629\n",
            "Iteração 727: MSE = 0.013596\n",
            "Iteração 728: MSE = 0.013564\n",
            "Iteração 729: MSE = 0.013531\n",
            "Iteração 730: MSE = 0.013499\n",
            "Iteração 731: MSE = 0.013466\n",
            "Iteração 732: MSE = 0.013434\n",
            "Iteração 733: MSE = 0.013402\n",
            "Iteração 734: MSE = 0.013369\n",
            "Iteração 735: MSE = 0.013337\n",
            "Iteração 736: MSE = 0.013305\n",
            "Iteração 737: MSE = 0.013273\n",
            "Iteração 738: MSE = 0.013241\n",
            "Iteração 739: MSE = 0.013210\n",
            "Iteração 740: MSE = 0.013178\n",
            "Iteração 741: MSE = 0.013146\n",
            "Iteração 742: MSE = 0.013115\n",
            "Iteração 743: MSE = 0.013083\n",
            "Iteração 744: MSE = 0.013052\n",
            "Iteração 745: MSE = 0.013020\n",
            "Iteração 746: MSE = 0.012989\n",
            "Iteração 747: MSE = 0.012958\n",
            "Iteração 748: MSE = 0.012927\n",
            "Iteração 749: MSE = 0.012896\n",
            "Iteração 750: MSE = 0.012865\n",
            "Iteração 751: MSE = 0.012834\n",
            "Iteração 752: MSE = 0.012803\n",
            "Iteração 753: MSE = 0.012772\n",
            "Iteração 754: MSE = 0.012741\n",
            "Iteração 755: MSE = 0.012711\n",
            "Iteração 756: MSE = 0.012680\n",
            "Iteração 757: MSE = 0.012650\n",
            "Iteração 758: MSE = 0.012619\n",
            "Iteração 759: MSE = 0.012589\n",
            "Iteração 760: MSE = 0.012559\n",
            "Iteração 761: MSE = 0.012529\n",
            "Iteração 762: MSE = 0.012499\n",
            "Iteração 763: MSE = 0.012469\n",
            "Iteração 764: MSE = 0.012439\n",
            "Iteração 765: MSE = 0.012409\n",
            "Iteração 766: MSE = 0.012379\n",
            "Iteração 767: MSE = 0.012349\n",
            "Iteração 768: MSE = 0.012320\n",
            "Iteração 769: MSE = 0.012290\n",
            "Iteração 770: MSE = 0.012260\n",
            "Iteração 771: MSE = 0.012231\n",
            "Iteração 772: MSE = 0.012202\n",
            "Iteração 773: MSE = 0.012172\n",
            "Iteração 774: MSE = 0.012143\n",
            "Iteração 775: MSE = 0.012114\n",
            "Iteração 776: MSE = 0.012085\n",
            "Iteração 777: MSE = 0.012056\n",
            "Iteração 778: MSE = 0.012027\n",
            "Iteração 779: MSE = 0.011998\n",
            "Iteração 780: MSE = 0.011969\n",
            "Iteração 781: MSE = 0.011940\n",
            "Iteração 782: MSE = 0.011912\n",
            "Iteração 783: MSE = 0.011883\n",
            "Iteração 784: MSE = 0.011854\n",
            "Iteração 785: MSE = 0.011826\n",
            "Iteração 786: MSE = 0.011798\n",
            "Iteração 787: MSE = 0.011769\n",
            "Iteração 788: MSE = 0.011741\n",
            "Iteração 789: MSE = 0.011713\n",
            "Iteração 790: MSE = 0.011685\n",
            "Iteração 791: MSE = 0.011657\n",
            "Iteração 792: MSE = 0.011629\n",
            "Iteração 793: MSE = 0.011601\n",
            "Iteração 794: MSE = 0.011573\n",
            "Iteração 795: MSE = 0.011545\n",
            "Iteração 796: MSE = 0.011517\n",
            "Iteração 797: MSE = 0.011489\n",
            "Iteração 798: MSE = 0.011462\n",
            "Iteração 799: MSE = 0.011434\n",
            "Iteração 800: MSE = 0.011407\n",
            "Iteração 801: MSE = 0.011379\n",
            "Iteração 802: MSE = 0.011352\n",
            "Iteração 803: MSE = 0.011325\n",
            "Iteração 804: MSE = 0.011298\n",
            "Iteração 805: MSE = 0.011271\n",
            "Iteração 806: MSE = 0.011243\n",
            "Iteração 807: MSE = 0.011216\n",
            "Iteração 808: MSE = 0.011189\n",
            "Iteração 809: MSE = 0.011163\n",
            "Iteração 810: MSE = 0.011136\n",
            "Iteração 811: MSE = 0.011109\n",
            "Iteração 812: MSE = 0.011082\n",
            "Iteração 813: MSE = 0.011056\n",
            "Iteração 814: MSE = 0.011029\n",
            "Iteração 815: MSE = 0.011003\n",
            "Iteração 816: MSE = 0.010976\n",
            "Iteração 817: MSE = 0.010950\n",
            "Iteração 818: MSE = 0.010924\n",
            "Iteração 819: MSE = 0.010897\n",
            "Iteração 820: MSE = 0.010871\n",
            "Iteração 821: MSE = 0.010845\n",
            "Iteração 822: MSE = 0.010819\n",
            "Iteração 823: MSE = 0.010793\n",
            "Iteração 824: MSE = 0.010767\n",
            "Iteração 825: MSE = 0.010741\n",
            "Iteração 826: MSE = 0.010715\n",
            "Iteração 827: MSE = 0.010690\n",
            "Iteração 828: MSE = 0.010664\n",
            "Iteração 829: MSE = 0.010638\n",
            "Iteração 830: MSE = 0.010613\n",
            "Iteração 831: MSE = 0.010587\n",
            "Iteração 832: MSE = 0.010562\n",
            "Iteração 833: MSE = 0.010536\n",
            "Iteração 834: MSE = 0.010511\n",
            "Iteração 835: MSE = 0.010486\n",
            "Iteração 836: MSE = 0.010461\n",
            "Iteração 837: MSE = 0.010436\n",
            "Iteração 838: MSE = 0.010410\n",
            "Iteração 839: MSE = 0.010385\n",
            "Iteração 840: MSE = 0.010361\n",
            "Iteração 841: MSE = 0.010336\n",
            "Iteração 842: MSE = 0.010311\n",
            "Iteração 843: MSE = 0.010286\n",
            "Iteração 844: MSE = 0.010261\n",
            "Iteração 845: MSE = 0.010237\n",
            "Iteração 846: MSE = 0.010212\n",
            "Iteração 847: MSE = 0.010188\n",
            "Iteração 848: MSE = 0.010163\n",
            "Iteração 849: MSE = 0.010139\n",
            "Iteração 850: MSE = 0.010114\n",
            "Iteração 851: MSE = 0.010090\n",
            "Iteração 852: MSE = 0.010066\n",
            "Iteração 853: MSE = 0.010042\n",
            "Iteração 854: MSE = 0.010017\n",
            "Iteração 855: MSE = 0.009993\n",
            "Iteração 856: MSE = 0.009969\n",
            "Iteração 857: MSE = 0.009945\n",
            "Iteração 858: MSE = 0.009922\n",
            "Iteração 859: MSE = 0.009898\n",
            "Iteração 860: MSE = 0.009874\n",
            "Iteração 861: MSE = 0.009850\n",
            "Iteração 862: MSE = 0.009827\n",
            "Iteração 863: MSE = 0.009803\n",
            "Iteração 864: MSE = 0.009779\n",
            "Iteração 865: MSE = 0.009756\n",
            "Iteração 866: MSE = 0.009732\n",
            "Iteração 867: MSE = 0.009709\n",
            "Iteração 868: MSE = 0.009686\n",
            "Iteração 869: MSE = 0.009662\n",
            "Iteração 870: MSE = 0.009639\n",
            "Iteração 871: MSE = 0.009616\n",
            "Iteração 872: MSE = 0.009593\n",
            "Iteração 873: MSE = 0.009570\n",
            "Iteração 874: MSE = 0.009547\n",
            "Iteração 875: MSE = 0.009524\n",
            "Iteração 876: MSE = 0.009501\n",
            "Iteração 877: MSE = 0.009478\n",
            "Iteração 878: MSE = 0.009456\n",
            "Iteração 879: MSE = 0.009433\n",
            "Iteração 880: MSE = 0.009410\n",
            "Iteração 881: MSE = 0.009388\n",
            "Iteração 882: MSE = 0.009365\n",
            "Iteração 883: MSE = 0.009342\n",
            "Iteração 884: MSE = 0.009320\n",
            "Iteração 885: MSE = 0.009298\n",
            "Iteração 886: MSE = 0.009275\n",
            "Iteração 887: MSE = 0.009253\n",
            "Iteração 888: MSE = 0.009231\n",
            "Iteração 889: MSE = 0.009209\n",
            "Iteração 890: MSE = 0.009186\n",
            "Iteração 891: MSE = 0.009164\n",
            "Iteração 892: MSE = 0.009142\n",
            "Iteração 893: MSE = 0.009120\n",
            "Iteração 894: MSE = 0.009099\n",
            "Iteração 895: MSE = 0.009077\n",
            "Iteração 896: MSE = 0.009055\n",
            "Iteração 897: MSE = 0.009033\n",
            "Iteração 898: MSE = 0.009011\n",
            "Iteração 899: MSE = 0.008990\n",
            "Iteração 900: MSE = 0.008968\n",
            "Iteração 901: MSE = 0.008947\n",
            "Iteração 902: MSE = 0.008925\n",
            "Iteração 903: MSE = 0.008904\n",
            "Iteração 904: MSE = 0.008882\n",
            "Iteração 905: MSE = 0.008861\n",
            "Iteração 906: MSE = 0.008840\n",
            "Iteração 907: MSE = 0.008818\n",
            "Iteração 908: MSE = 0.008797\n",
            "Iteração 909: MSE = 0.008776\n",
            "Iteração 910: MSE = 0.008755\n",
            "Iteração 911: MSE = 0.008734\n",
            "Iteração 912: MSE = 0.008713\n",
            "Iteração 913: MSE = 0.008692\n",
            "Iteração 914: MSE = 0.008671\n",
            "Iteração 915: MSE = 0.008650\n",
            "Iteração 916: MSE = 0.008630\n",
            "Iteração 917: MSE = 0.008609\n",
            "Iteração 918: MSE = 0.008588\n",
            "Iteração 919: MSE = 0.008568\n",
            "Iteração 920: MSE = 0.008547\n",
            "Iteração 921: MSE = 0.008526\n",
            "Iteração 922: MSE = 0.008506\n",
            "Iteração 923: MSE = 0.008485\n",
            "Iteração 924: MSE = 0.008465\n",
            "Iteração 925: MSE = 0.008445\n",
            "Iteração 926: MSE = 0.008424\n",
            "Iteração 927: MSE = 0.008404\n",
            "Iteração 928: MSE = 0.008384\n",
            "Iteração 929: MSE = 0.008364\n",
            "Iteração 930: MSE = 0.008344\n",
            "Iteração 931: MSE = 0.008324\n",
            "Iteração 932: MSE = 0.008304\n",
            "Iteração 933: MSE = 0.008284\n",
            "Iteração 934: MSE = 0.008264\n",
            "Iteração 935: MSE = 0.008244\n",
            "Iteração 936: MSE = 0.008224\n",
            "Iteração 937: MSE = 0.008204\n",
            "Iteração 938: MSE = 0.008185\n",
            "Iteração 939: MSE = 0.008165\n",
            "Iteração 940: MSE = 0.008145\n",
            "Iteração 941: MSE = 0.008126\n",
            "Iteração 942: MSE = 0.008106\n",
            "Iteração 943: MSE = 0.008087\n",
            "Iteração 944: MSE = 0.008067\n",
            "Iteração 945: MSE = 0.008048\n",
            "Iteração 946: MSE = 0.008029\n",
            "Iteração 947: MSE = 0.008009\n",
            "Iteração 948: MSE = 0.007990\n",
            "Iteração 949: MSE = 0.007971\n",
            "Iteração 950: MSE = 0.007952\n",
            "Iteração 951: MSE = 0.007933\n",
            "Iteração 952: MSE = 0.007914\n",
            "Iteração 953: MSE = 0.007895\n",
            "Iteração 954: MSE = 0.007876\n",
            "Iteração 955: MSE = 0.007857\n",
            "Iteração 956: MSE = 0.007838\n",
            "Iteração 957: MSE = 0.007819\n",
            "Iteração 958: MSE = 0.007800\n",
            "Iteração 959: MSE = 0.007782\n",
            "Iteração 960: MSE = 0.007763\n",
            "Iteração 961: MSE = 0.007744\n",
            "Iteração 962: MSE = 0.007726\n",
            "Iteração 963: MSE = 0.007707\n",
            "Iteração 964: MSE = 0.007689\n",
            "Iteração 965: MSE = 0.007670\n",
            "Iteração 966: MSE = 0.007652\n",
            "Iteração 967: MSE = 0.007633\n",
            "Iteração 968: MSE = 0.007615\n",
            "Iteração 969: MSE = 0.007597\n",
            "Iteração 970: MSE = 0.007578\n",
            "Iteração 971: MSE = 0.007560\n",
            "Iteração 972: MSE = 0.007542\n",
            "Iteração 973: MSE = 0.007524\n",
            "Iteração 974: MSE = 0.007506\n",
            "Iteração 975: MSE = 0.007488\n",
            "Iteração 976: MSE = 0.007470\n",
            "Iteração 977: MSE = 0.007452\n",
            "Iteração 978: MSE = 0.007434\n",
            "Iteração 979: MSE = 0.007416\n",
            "Iteração 980: MSE = 0.007398\n",
            "Iteração 981: MSE = 0.007381\n",
            "Iteração 982: MSE = 0.007363\n",
            "Iteração 983: MSE = 0.007345\n",
            "Iteração 984: MSE = 0.007327\n",
            "Iteração 985: MSE = 0.007310\n",
            "Iteração 986: MSE = 0.007292\n",
            "Iteração 987: MSE = 0.007275\n",
            "Iteração 988: MSE = 0.007257\n",
            "Iteração 989: MSE = 0.007240\n",
            "Iteração 990: MSE = 0.007222\n",
            "Iteração 991: MSE = 0.007205\n",
            "Iteração 992: MSE = 0.007188\n",
            "Iteração 993: MSE = 0.007171\n",
            "Iteração 994: MSE = 0.007153\n",
            "Iteração 995: MSE = 0.007136\n",
            "Iteração 996: MSE = 0.007119\n",
            "Iteração 997: MSE = 0.007102\n",
            "Iteração 998: MSE = 0.007085\n",
            "Iteração 999: MSE = 0.007068\n",
            "Iteração 1000: MSE = 0.007051\n",
            "Previsões: [7.83179993 9.73439231]\n",
            "MSE no teste: 0.04941935288552856\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class ScratchLinearRegression():\n",
        "    def __init__(self, num_iter=1000, lr=0.01, bias=True, verbose=False):\n",
        "        self.iter = num_iter\n",
        "        self.lr = lr\n",
        "        self.bias = bias\n",
        "        self.verbose = verbose\n",
        "        self.loss = np.zeros(self.iter)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        def fit(self, X, y, X_val=None, y_val=None):\n",
        "    m, n = X.shape\n",
        "\n",
        "    if self.bias:\n",
        "        X = np.c_[np.ones((m, 1)), X]\n",
        "        if X_val is not None:\n",
        "            X_val = np.c_[np.ones((X_val.shape[0], 1)), X_val]\n",
        "\n",
        "    self.coef_ = np.zeros(X.shape[1])\n",
        "\n",
        "    for i in range(self.iter):\n",
        "        y_pred = X.dot(self.coef_)\n",
        "        error = y_pred - y\n",
        "\n",
        "        gradient = (1 / m) * X.T.dot(error)\n",
        "        self.coef_ -= self.lr * gradient\n",
        "\n",
        "        J = (1 / (2 * m)) * np.sum(error ** 2)\n",
        "        self.loss[i] = J\n",
        "\n",
        "        if X_val is not None and y_val is not None:\n",
        "            y_val_pred = X_val.dot(self.coef_)\n",
        "            val_error = y_val_pred - y_val\n",
        "            J_val = (1 / (2 * X_val.shape[0])) * np.sum(val_error ** 2)\n",
        "            self.val_loss[i] = J_val\n",
        "\n",
        "        if self.verbose:\n",
        "            if X_val is not None and y_val is not None:\n",
        "                print(f\"Iteração {i+1}: Loss treino = {J:.6f}, Loss validação = {J_val:.6f}\")\n",
        "            else:\n",
        "                print(f\"Iteração {i+1}: Loss treino = {J:.6f}\")\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        if self.bias:\n",
        "            X = np.c_[np.ones((X.shape[0], 1)), X]\n",
        "        return X.dot(self.coef_)\n",
        "\n",
        "    def mean_squared_error(self, X, y):\n",
        "        y_pred = self.predict(X)\n",
        "        mse = np.mean((y - y_pred) ** 2)\n",
        "        return mse\n"
      ],
      "metadata": {
        "id": "Si10b0-4zqCD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}