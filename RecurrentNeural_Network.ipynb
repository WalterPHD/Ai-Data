{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNiB2xcMWQ1sfn0tWWprZUl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WalterPHD/Ai-Data/blob/main/RecurrentNeural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 1"
      ],
      "metadata": {
        "id": "RrKUYewaJpxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class SimpleRNN:\n",
        "    def __init__(self, n_features, n_nodes, activation=np.tanh):\n",
        "        self.n_features = n_features\n",
        "        self.n_nodes = n_nodes\n",
        "        self.activation = activation\n",
        "\n",
        "        #Initializing weights\n",
        "        self.Wx = np.random.randn(n_features, n_nodes) * 0.01\n",
        "        self.Wh = np.random.randn(n_nodes, n_nodes) * 0.01\n",
        "        self.b = np.zeros(n_nodes)\n",
        "\n",
        "    def forward(self, x, h0=None):\n",
        "        batch_size, n_sequences, _ = x.shape\n",
        "\n",
        "        #Initializing hidden state\n",
        "        if h0 is None:\n",
        "            h_t = np.zeros((batch_size, self.n_nodes))\n",
        "        else:\n",
        "            h_t = h0\n",
        "\n",
        "        #Stored states for all timesteps\n",
        "        self.h_list = []\n",
        "\n",
        "        for t in range(n_sequences):\n",
        "            x_t = x[:, t, :]  # (batch_size, n_features)\n",
        "\n",
        "            a_t = x_t @ self.Wx + h_t @ self.Wh + self.b\n",
        "            h_t = self.activation(a_t)\n",
        "\n",
        "            self.h_list.append(h_t)\n",
        "\n",
        "        #Final hidden state\n",
        "        return h_t, self.h_list\n"
      ],
      "metadata": {
        "id": "BOzaKCS2JqJF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 2"
      ],
      "metadata": {
        "id": "nAmoPpjOJxB7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50BsQMExJgVj",
        "outputId": "8e84dbc4-f3b0-463f-9eb0-d43625635f8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final hidden state:\n",
            " [[0.79494228 0.81839002 0.83939649 0.85584174]]\n"
          ]
        }
      ],
      "source": [
        "#Small sequence test\n",
        "x = np.array([[[1, 2], [2, 3], [3, 4]]]) / 100  # (1, 3, 2)\n",
        "w_x = np.array([[1, 3, 5, 7], [3, 5, 7, 8]]) / 100  # (2, 4)\n",
        "w_h = np.array([[1, 3, 5, 7],\n",
        "                [2, 4, 6, 8],\n",
        "                [3, 5, 7, 8],\n",
        "                [4, 6, 8, 10]]) / 100  # (4, 4)\n",
        "\n",
        "batch_size = x.shape[0]  # 1\n",
        "n_sequences = x.shape[1]  # 3\n",
        "n_features = x.shape[2]  # 2\n",
        "n_nodes = w_x.shape[1]  # 4\n",
        "\n",
        "h0 = np.zeros((batch_size, n_nodes))\n",
        "b = np.array([1, 1, 1, 1])  # bias\n",
        "\n",
        "rnn = SimpleRNN(n_features, n_nodes)\n",
        "rnn.Wx = w_x\n",
        "rnn.Wh = w_h\n",
        "rnn.b = b\n",
        "\n",
        "h_final, h_list = rnn.forward(x, h0)\n",
        "print(\"Final hidden state:\\n\", h_final)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 3"
      ],
      "metadata": {
        "id": "IcJ0AyFkKofA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleRNN:\n",
        "    def __init__(self, n_features, n_nodes, activation=np.tanh):\n",
        "        self.n_features = n_features\n",
        "        self.n_nodes = n_nodes\n",
        "        self.activation = activation\n",
        "\n",
        "        #Parameters\n",
        "        self.Wx = np.random.randn(n_features, n_nodes) * 0.01\n",
        "        self.Wh = np.random.randn(n_nodes, n_nodes) * 0.01\n",
        "        self.b = np.zeros(n_nodes)\n",
        "\n",
        "    def forward(self, x, h0=None):\n",
        "        batch_size, n_sequences, _ = x.shape\n",
        "\n",
        "        if h0 is None:\n",
        "            h_t = np.zeros((batch_size, self.n_nodes))\n",
        "        else:\n",
        "            h_t = h0\n",
        "\n",
        "        self.h_list = [h_t]\n",
        "        self.a_list = []\n",
        "        self.x = x\n",
        "\n",
        "        for t in range(n_sequences):\n",
        "            x_t = x[:, t, :]  # (batch_size, n_features)\n",
        "\n",
        "            a_t = x_t @ self.Wx + h_t @ self.Wh + self.b\n",
        "            h_t = self.activation(a_t)\n",
        "\n",
        "            self.a_list.append(a_t)\n",
        "            self.h_list.append(h_t)\n",
        "\n",
        "        return h_t, self.h_list[1:]  # last hidden + all h_t\n",
        "\n",
        "    def backward(self, dh_last, learning_rate=0.01):\n",
        "        batch_size, n_sequences, _ = self.x.shape\n",
        "\n",
        "        #Initializing grads\n",
        "        dWx = np.zeros_like(self.Wx)\n",
        "        dWh = np.zeros_like(self.Wh)\n",
        "        db = np.zeros_like(self.b)\n",
        "\n",
        "        dx = np.zeros_like(self.x)\n",
        "        dh_next = dh_last  # starts from loss gradient at last timestep\n",
        "\n",
        "        #Looping backwards through time\n",
        "        for t in reversed(range(n_sequences)):\n",
        "            a_t = self.a_list[t]\n",
        "            h_prev = self.h_list[t]\n",
        "            x_t = self.x[:, t, :]\n",
        "\n",
        "            #Gradient wrt pre-activation\n",
        "            dtanh = (1 - np.tanh(a_t) ** 2) * dh_next\n",
        "\n",
        "            #Accumulate grads\n",
        "            dWx += x_t.T @ dtanh\n",
        "            dWh += h_prev.T @ dtanh\n",
        "            db += dtanh.sum(axis=0)\n",
        "\n",
        "            # Gradients to pass backward\n",
        "            dx[:, t, :] = dtanh @ self.Wx.T\n",
        "            dh_next = dtanh @ self.Wh.T\n",
        "\n",
        "        # Normalize by batch size\n",
        "        dWx /= batch_size\n",
        "        dWh /= batch_size\n",
        "        db /= batch_size\n",
        "\n",
        "        # Update params\n",
        "        self.Wx -= learning_rate * dWx\n",
        "        self.Wh -= learning_rate * dWh\n",
        "        self.b -= learning_rate * db\n",
        "\n",
        "        return dx, dh_next, dWx, dWh, db\n"
      ],
      "metadata": {
        "id": "9tWhsmyPKhgM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forward(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anVlc1i4J1f6",
        "outputId": "b84b1eca-496d-4fae-fae7-db23919af596"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.79494228, 0.81839002, 0.83939649, 0.85584174]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    }
  ]
}