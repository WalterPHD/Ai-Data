{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMWGMVWT5SVLHuBtC+GHxOr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WalterPHD/Ai-Data/blob/main/Tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "problem 1"
      ],
      "metadata": {
        "id": "h6nhDxtOzHla"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HoHhEICPynqj"
      },
      "outputs": [],
      "source": [
        "# Look back at the previous scratches so far, To implement deep learning we:\n",
        "\n",
        "# Had to initialize the weights\n",
        "# Needed an epoch loop\n",
        "# Coded the Activation Functions\n",
        "# Decided the learning rate, sizes, number of nodes and so on"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "problem 2"
      ],
      "metadata": {
        "id": "6o8soUkQzKGF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "dataset_path =\"Iris.csv\"\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "df = df[(df[\"Species\"] == \"Iris-versicolor\")|(df[\"Species\"] == \"Iris-virginica\")]\n",
        "y = df[\"Species\"]\n",
        "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
        "y = np.array(y)\n",
        "X = np.array(X)\n",
        "\n",
        "y[y=='Iris-versicolor'] = 0\n",
        "y[y=='Iris-virginica'] = 1\n",
        "y = y.astype(int)[:, np.newaxis]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "\n",
        "class GetMiniBatch:\n",
        "\n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "\n",
        "learning_rate = 0.01\n",
        "batch_size = 10\n",
        "num_epochs = 10\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 1\n",
        "\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "def example_net(x):\n",
        "\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3']\n",
        "    return layer_output\n",
        "\n",
        "logits = example_net(X)\n",
        "\n",
        "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
        "\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(int)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "            total_acc += acc\n",
        "        total_loss /= n_samples\n",
        "        total_acc /= n_samples\n",
        "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G29rM9KWzLAf",
        "outputId": "1e8d1628-f6b6-42bd-a1b3-444f17c3e9fe"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss : 27.1081, val_loss : 61.6922, acc : 0.750, val_acc : 0.375\n",
            "Epoch 1, loss : 5.1116, val_loss : 11.4570, acc : 0.750, val_acc : 0.375\n",
            "Epoch 2, loss : 22.5828, val_loss : 5.8402, acc : 0.500, val_acc : 0.688\n",
            "Epoch 3, loss : 1.5144, val_loss : 3.4949, acc : 0.750, val_acc : 0.625\n",
            "Epoch 4, loss : 3.8978, val_loss : 0.3837, acc : 0.750, val_acc : 0.875\n",
            "Epoch 5, loss : 0.0071, val_loss : 1.1565, acc : 1.000, val_acc : 0.812\n",
            "Epoch 6, loss : 0.4322, val_loss : 0.0570, acc : 0.750, val_acc : 0.938\n",
            "Epoch 7, loss : 0.0000, val_loss : 0.3331, acc : 1.000, val_acc : 0.875\n",
            "Epoch 8, loss : 0.0000, val_loss : 0.0001, acc : 1.000, val_acc : 1.000\n",
            "Epoch 9, loss : 0.0000, val_loss : 0.0009, acc : 1.000, val_acc : 1.000\n",
            "test_acc : 0.850\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "problem 3"
      ],
      "metadata": {
        "id": "kRBQlO5XzPYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "# Load the Iris dataset from scikit-learn\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Reshape y to be a column vector\n",
        "y = y[:, np.newaxis]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "enc = OneHotEncoder(handle_unknown='ignore')\n",
        "y_train_one_hot = enc.fit_transform(y_train)\n",
        "y_val_one_hot = enc.transform(y_val)\n",
        "y_test_one_hot = enc.transform(y_test)\n",
        "\n",
        "class GetMiniBatch:\n",
        "\n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        # Convert sparse matrix to dense array\n",
        "        return self.X[p0:p1], self.y[p0:p1].toarray()\n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        # Convert sparse matrix to dense array\n",
        "        return self.X[p0:p1], self.y[p0:p1].toarray()\n",
        "\n",
        "learning_rate = 0.01\n",
        "batch_size = 10\n",
        "num_epochs = 10\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 3\n",
        "\n",
        "X_placeholder = tf.placeholder(\"float\", [None, n_input]) # Renamed to avoid conflict\n",
        "Y_placeholder = tf.placeholder(\"float\", [None, n_classes]) # Renamed to avoid conflict\n",
        "\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train_one_hot, batch_size=batch_size)\n",
        "\n",
        "def example_net(x):\n",
        "\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3']\n",
        "    return layer_output\n",
        "\n",
        "logits = example_net(X_placeholder) # Using the renamed placeholder\n",
        "\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y_placeholder, logits=logits)) # Using the renamed placeholder\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "correct_pred = tf.equal(tf.argmax(Y_placeholder, 1), tf.argmax(logits, 1)) # Using the renamed placeholder\n",
        "\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(int)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            sess.run(train_op, feed_dict={X_placeholder: mini_batch_x, Y_placeholder: mini_batch_y}) # Using the renamed placeholders\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X_placeholder: mini_batch_x, Y_placeholder: mini_batch_y}) # Using the renamed placeholders\n",
        "            total_loss += loss\n",
        "            total_acc += acc\n",
        "        total_loss /= n_samples # Note: This average might be incorrect as it divides by total samples, not number of batches.\n",
        "        total_acc /= n_samples # Note: This average might be incorrect as it divides by total samples, not number of batches.\n",
        "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X_placeholder: X_val, Y_placeholder: y_val_one_hot.toarray()}) # Using the renamed placeholders and converting to dense array\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X_placeholder: X_test, Y_placeholder: y_test_one_hot.toarray()}) # Using the renamed placeholders and converting to dense array\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xp7_YGxwzQRd",
        "outputId": "687f28ec-2cde-48ff-fd99-22b374125e96"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss : 103.8163, val_loss : 56.9547, acc : 0.333, val_acc : 0.417\n",
            "Epoch 1, loss : 11.9050, val_loss : 6.6492, acc : 0.333, val_acc : 0.708\n",
            "Epoch 2, loss : 0.0234, val_loss : 3.5826, acc : 1.000, val_acc : 0.708\n",
            "Epoch 3, loss : 0.0000, val_loss : 2.3598, acc : 1.000, val_acc : 0.792\n",
            "Epoch 4, loss : 0.0000, val_loss : 2.0108, acc : 1.000, val_acc : 0.792\n",
            "Epoch 5, loss : 0.0000, val_loss : 1.9781, acc : 1.000, val_acc : 0.792\n",
            "Epoch 6, loss : 0.0000, val_loss : 1.2127, acc : 1.000, val_acc : 0.792\n",
            "Epoch 7, loss : 0.0019, val_loss : 1.7950, acc : 1.000, val_acc : 0.750\n",
            "Epoch 8, loss : 0.3235, val_loss : 3.8058, acc : 0.833, val_acc : 0.708\n",
            "Epoch 9, loss : 0.0576, val_loss : 3.4647, acc : 1.000, val_acc : 0.750\n",
            "test_acc : 0.933\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "problem 4"
      ],
      "metadata": {
        "id": "TdR7WciTzXzQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "dataset_path =\"train.csv\"\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "y = df[\"SalePrice\"]\n",
        "X = df.loc[:, [\"GrLivArea\", \"YearBuilt\"]]\n",
        "y = np.array(y)\n",
        "X = np.array(X)\n",
        "y = y.astype(int)[:, np.newaxis]\n",
        "y = np.log(y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "\n",
        "class GetMiniBatch:\n",
        "\n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "\n",
        "learning_rate = 0.01\n",
        "batch_size = 10\n",
        "num_epochs = 10\n",
        "\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 1\n",
        "\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "def example_net(x):\n",
        "\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3']\n",
        "    return layer_output\n",
        "\n",
        "logits = example_net(X)\n",
        "loss_op =  tf.losses.mean_squared_error(labels=Y, predictions=logits)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "y_pred = logits\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    loss_list = []\n",
        "    val_loss_list = []\n",
        "    for epoch in range(num_epochs):\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "        loss = sess.run(loss_op, feed_dict={X: X_train, Y: y_train})\n",
        "        loss_list.append(loss)\n",
        "        val_loss = sess.run(loss_op, feed_dict={X: X_val, Y: y_val})\n",
        "        val_loss_list.append(val_loss)\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}\".format(epoch, loss, val_loss))\n",
        "    print(\"test_mse : {:.3f}\".format(loss))\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('loss')\n",
        "    plt.plot(loss_list, label='loss')\n",
        "    plt.plot(val_loss_list, label='val_loss')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 656
        },
        "id": "uaGYOx_dYL2t",
        "outputId": "be34f6cd-c81e-4dba-e06d-2790dd340063"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss : 1188367.1250, val_loss : 272756.6562\n",
            "Epoch 1, loss : 78056.7188, val_loss : 65472.3828\n",
            "Epoch 2, loss : 52389.6094, val_loss : 54965.5078\n",
            "Epoch 3, loss : 15095.5098, val_loss : 12358.5781\n",
            "Epoch 4, loss : 11723.8438, val_loss : 10174.5615\n",
            "Epoch 5, loss : 10306.6602, val_loss : 9709.1094\n",
            "Epoch 6, loss : 10358.9238, val_loss : 9714.8281\n",
            "Epoch 7, loss : 7486.3008, val_loss : 7040.1172\n",
            "Epoch 8, loss : 8303.0947, val_loss : 9050.0654\n",
            "Epoch 9, loss : 14563.6143, val_loss : 15932.9443\n",
            "test_mse : 14563.614\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "problem 5"
      ],
      "metadata": {
        "id": "eB3ZZKEFzdQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train = X_train.reshape(-1, 784)\n",
        "X_test = X_test.reshape(-1, 784)\n",
        "\n",
        "X_train = X_train.astype(np.float64)\n",
        "X_test = X_test.astype(np.float64)\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "y_train = y_train.astype(int)[:, np.newaxis]\n",
        "y_test = y_test.astype(int)[:, np.newaxis]\n",
        "\n",
        "enc = OneHotEncoder(handle_unknown='ignore')\n",
        "y_train_one_hot = enc.fit_transform(y_train[:])\n",
        "y_test_one_hot = enc.fit_transform(y_test[:])\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)\n",
        "\n",
        "class GetMiniBatch:\n",
        "\n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1].toarray()\n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1].toarray()\n",
        "\n",
        "learning_rate = 0.01\n",
        "batch_size = 10\n",
        "num_epochs = 10\n",
        "\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 10\n",
        "\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "def example_net(x):\n",
        "\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3']\n",
        "    return layer_output\n",
        "\n",
        "logits = example_net(X)\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "max_Y = (tf.argmax(Y, 1))\n",
        "max_Y_pred = tf.argmax(logits, 1)\n",
        "correct_pred = tf.equal(tf.argmax(Y, 1), tf.argmax(logits, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(int)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "            total_acc += acc\n",
        "        total_loss /= n_samples\n",
        "        total_acc /= n_samples\n",
        "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val.toarray()})\n",
        "\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test_one_hot.toarray()})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e28kcDm8aKAA",
        "outputId": "b02624a7-6c37-423b-a2f2-e2065efca7b7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss : 1.0964, val_loss : 1.0173, acc : 0.600, val_acc : 0.724\n",
            "Epoch 1, loss : 0.6185, val_loss : 0.7293, acc : 0.900, val_acc : 0.828\n",
            "Epoch 2, loss : 0.3275, val_loss : 0.4250, acc : 0.800, val_acc : 0.891\n",
            "Epoch 3, loss : 0.2180, val_loss : 0.4013, acc : 1.000, val_acc : 0.900\n",
            "Epoch 4, loss : 0.0308, val_loss : 0.3544, acc : 1.000, val_acc : 0.909\n",
            "Epoch 5, loss : 0.1811, val_loss : 0.3622, acc : 0.900, val_acc : 0.912\n",
            "Epoch 6, loss : 0.0314, val_loss : 0.3806, acc : 1.000, val_acc : 0.913\n",
            "Epoch 7, loss : 0.1447, val_loss : 0.3382, acc : 1.000, val_acc : 0.923\n",
            "Epoch 8, loss : 0.0351, val_loss : 0.3384, acc : 1.000, val_acc : 0.927\n",
            "Epoch 9, loss : 0.1344, val_loss : 0.3590, acc : 0.900, val_acc : 0.921\n",
            "test_acc : 0.929\n"
          ]
        }
      ]
    }
  ]
}