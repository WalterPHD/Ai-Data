{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPAgRoF9pVslf3BwGOysX0z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WalterPHD/Ai-Data/blob/main/LogisticRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scratch code"
      ],
      "metadata": {
        "id": "Rt1iK2xM8LK4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQkRldtdwdyJ"
      },
      "outputs": [],
      "source": [
        "class ScratchLogisticRegression():\n",
        "  def __init__(self, num_iter, lr, bias, verbose, lam):\n",
        "    self.num_iter = num_iter\n",
        "    self.lr = lr\n",
        "    self.bias = bias\n",
        "    self.verbose = verbose\n",
        "    self.lam = lam\n",
        "    self.theta = np.array([])\n",
        "    self.loss = np.array([])\n",
        "    self.val_loss = np.array([])\n",
        "\n",
        "def fit(self, X_train, y_train, X_val = None, y_val = None):\n",
        "  \"\"\n",
        "\"train\"\n",
        "\"\"\n",
        "pass\n",
        "\n",
        "def _gradient_descent(self, X, y):\n",
        "  \"\"\n",
        "\"Θ update (steepest descent method)\"\n",
        "\"\"\n",
        "pass\n",
        "\n",
        "def _sigmoid(self, y):\n",
        "  \"\"\n",
        "\"sigmoidfunction\"\n",
        "\"\"\n",
        "pass\n",
        "\n",
        "def _logistic_hypothesis(self, X):\n",
        "  \"\"\n",
        "\"Hypothetical function\"\n",
        "\"\"\n",
        "pass\n",
        "\n",
        "def predict(self, X):\n",
        "  \"\"\n",
        "\"predict\"\n",
        "\"\"\n",
        "pass\n",
        "\n",
        "def predict_proba(self, X):\n",
        "  \"\"\n",
        "\"probabilistic prediction \"\n",
        "\"\"\n",
        "pass\n",
        "\n",
        "def _loss_func(self, pred, y):\n",
        "  \"\"\n",
        "\"cross entropy error function\"\n",
        "\"\"\n",
        "pass"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Functions"
      ],
      "metadata": {
        "id": "0Vh14ryI89Ej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _sigmoid(self, y):\n",
        "    return 1 / (1 + np.exp(-y))\n",
        "\n",
        "\n",
        "def _logistic_hypothesis(self, X):\n",
        "    pred = X @ self.theta\n",
        "    pred = self._sigmoid(pred)\n",
        "    return pred"
      ],
      "metadata": {
        "id": "0OjSEgOu85Cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "function with L2 regularization focus"
      ],
      "metadata": {
        "id": "kuZJdUdd9efp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _gradient_descent(self, X, y):\n",
        "    m = X.shape[0]\n",
        "    n = X.shape[1]\n",
        "    pred = self._logistic_hypothesis(X)\n",
        "    for j in range(n):\n",
        "        gradient = 0\n",
        "        for i in range(m):\n",
        "            gradient += (pred[i] - y[i]) * X[i, j]\n",
        "        self.theta[j] = self.theta[j] - self.lr * (\n",
        "            (gradient + self.lam * self.theta[j]) / m\n",
        "        )"
      ],
      "metadata": {
        "id": "NdbH6dcb9h3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "functionn for prediction"
      ],
      "metadata": {
        "id": "vOab2p2Q9x90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_proba(self, X):\n",
        "    if self.bias == True:\n",
        "        a = np.ones(X.shape[0]).reshape(X.shape[0], 1)\n",
        "        X = np.hstack([a, X])\n",
        "    pred = self._logistic_hypothesis(X)\n",
        "    return pred"
      ],
      "metadata": {
        "id": "7QDF-a6U937M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _loss_func(self, pred, y):\n",
        "    error = 0\n",
        "    for i in range(y.shape[0]):\n",
        "        error += -np.sum(y[i] * np.log(pred[i]) + (1 - y[i]) * np.log(1 - pred[i]))\n",
        "    loss = error / (y.shape[0])\n",
        "    loss = loss + np.sum(self.theta**2) * self.lam / (2 * y.shape[0])\n",
        "    return loss"
      ],
      "metadata": {
        "id": "0hB86Tbb-Y3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "visualization"
      ],
      "metadata": {
        "id": "NsXs2UV0-9kQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decision_region(X, y, slr):\n",
        "    mesh_f0, mesh_f1 = np.meshgrid(\n",
        "        np.arange(np.min(X[:, 0]), np.max(X[:, 0]), 0.01),\n",
        "        np.arange(np.min(X[:, 1]), np.max(X[:, 1]), 0.01),\n",
        "    )\n",
        "    mesh = np.c_[np.ravel(mesh_f0), np.ravel(mesh_f1)]\n",
        "    y_pred = slr.predict(mesh).reshape(mesh_f0.shape)\n",
        "    plt.title(\"decision region\")\n",
        "    plt.xlabel(\"feature0\")\n",
        "    plt.ylabel(\"feature1\")\n",
        "    plt.contourf(mesh_f0, mesh_f1, y_pred, cmap=ListedColormap([\"pink\", \"skyblue\"]))\n",
        "    plt.contour(mesh_f0, mesh_f1, y_pred, colors=\"red\")\n",
        "    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], label=\"0\")\n",
        "    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], label=\"1\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "iuT3fHCu-_eV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment Scratch Lositic Regression"
      ],
      "metadata": {
        "id": "P1GyYFNc_CIP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 1 Hypothetical Function"
      ],
      "metadata": {
        "id": "t8KJoZ7t_O3F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please implement the method of logistic regression assumption function in ScratchLogisticRegression class.\n",
        "\n",
        "The assumed function for logistic regression is the assumed function for linear regression passed through the Sigmoid function. The sigmoid function is represented by the following equation\n",
        "\n",
        "g​ ​(z)​ ​=​ ​1​ ​1​ ​+​ ​e-zg(z)=11+e−z"
      ],
      "metadata": {
        "id": "4JodFvLLBvL9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n"
      ],
      "metadata": {
        "id": "I3IkviEtGAAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 2 Steepest Decent"
      ],
      "metadata": {
        "id": "pOS73XdRB2zx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement the steepest descent method for learning. Add a method _gradient_descent that updates the parameters as shown in the following equation, and call it from the fit method."
      ],
      "metadata": {
        "id": "6H1O0fu2B5z9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _gradient_descent(self, X, y):\n",
        "        m, n = X.shape\n",
        "        pred = self._logistic_hypothesis(X)\n",
        "        for j in range(n):\n",
        "            gradient = 0\n",
        "            for i in range(m):\n",
        "                gradient += (pred[i] - y[i]) * X[i, j]\n",
        "            # Regularize all parameters including theta_0 since no bias term here\n",
        "            reg_term = self.lam * self.theta[j]\n",
        "            self.theta[j] = self.theta[j] - self.lr * ((gradient + reg_term) / m)\n"
      ],
      "metadata": {
        "id": "e0onD9aDISpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 3 Estimated"
      ],
      "metadata": {
        "id": "0ye4snmKB9r3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please implement the estimation mechanism. Add to the predict method and predict_proba method included in the template of ScratchLogisticRegression class.\n",
        "\n",
        "The output of the hypothetical function $h_\\theta(x)$ is the return value of predict_proba, and the return value of predict is a threshold value labeled 1 and 0."
      ],
      "metadata": {
        "id": "L8Y_qi3rCCG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _logistic_hypothesis(self, X):\n",
        "        return self._sigmoid(X @ self.theta)"
      ],
      "metadata": {
        "id": "TXZC4-TAKvuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_proba(self, X):\n",
        "        return self._logistic_hypothesis(X)\n"
      ],
      "metadata": {
        "id": "JzR9R3t7JbVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(self, X):\n",
        "        proba = self.predict_proba(X)\n",
        "        return (proba >= 0.5).astype(int)\n"
      ],
      "metadata": {
        "id": "5z8CrPU_JmQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 4 Objective Function\n"
      ],
      "metadata": {
        "id": "aNniKHI3CFtN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement the Objective function (loss function) of the logistic regression expressed in the following formula And make sure that this is recorded inself.loss, self.val_loss .\n",
        "\n"
      ],
      "metadata": {
        "id": "OAc8CmenCIbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(self, X, y, X_val=None, y_val=None):\n",
        "    \"\"\"\n",
        "    Train the logistic regression model using gradient descent.\n",
        "    Records training and validation loss per iteration.\n",
        "    \"\"\"\n",
        "\n",
        "    if self.bias:\n",
        "        X = np.c_[np.ones(X.shape[0]), X]\n",
        "        if X_val is not None:\n",
        "            X_val = np.c_[np.ones(X_val.shape[0]), X_val]\n",
        "\n",
        "    self.theta = np.zeros(X.shape[1])\n",
        "\n",
        "    for i in range(self.iter):\n",
        "        # Update parameters using gradient descent\n",
        "        self._gradient_descent(X, y)\n",
        "\n",
        "        # Compute and record training loss\n",
        "        self.loss[i] = self._compute_loss(X, y)\n",
        "\n",
        "        # Compute and record validation loss if validation data provided\n",
        "        if X_val is not None and y_val is not None:\n",
        "            self.val_loss[i] = self._compute_loss(X_val, y_val)\n",
        "\n",
        "        # Verbose output to track training progress\n",
        "        if self.verbose:\n",
        "            msg = f\"Iter {i+1}/{self.iter} - Train Loss: {self.loss[i]:.4f}\"\n",
        "            if X_val is not None and y_val is not None:\n",
        "                msg += f\" - Val Loss: {self.val_loss[i]:.4f}\"\n",
        "            print(msg)\n"
      ],
      "metadata": {
        "id": "-a32pFODKzj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "487764b9"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compiled code"
      ],
      "metadata": {
        "id": "GCoIIQVCK0FE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class ScratchLogisticRegression():\n",
        "    \"\"\"\n",
        "    Scratch implementation of logistic regression\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    num_iter : int\n",
        "      Number of iterations\n",
        "    lr : float\n",
        "      Learning rate\n",
        "    lam : float\n",
        "      Regularization strength (lambda)\n",
        "    verbose : bool\n",
        "      True to output the learning process\n",
        "    bias: bool\n",
        "      True to include bias term\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    self.theta : ndarray, shape (n_features,)\n",
        "      Parameters (weights)\n",
        "    self.loss : ndarray, shape (self.iter,)\n",
        "      Record losses on training data\n",
        "    self.val_loss : ndarray, shape (self.iter,)\n",
        "      Record loss on validation data\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_iter, lr, lam=0.0, verbose=False, bias=True):\n",
        "        self.iter = num_iter\n",
        "        self.lr = lr\n",
        "        self.lam = lam\n",
        "        self.verbose = verbose\n",
        "        self.bias = bias\n",
        "        self.loss = np.zeros(self.iter)\n",
        "        self.val_loss = np.zeros(self.iter)\n",
        "\n",
        "\n",
        "    def _sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def _logistic_hypothesis(self, X):\n",
        "        return self._sigmoid(X @ self.theta)\n",
        "\n",
        "    def _compute_loss(self, X, y):\n",
        "        m = X.shape[0]\n",
        "        pred = self._logistic_hypothesis(X)\n",
        "        eps = 1e-15\n",
        "        pred = np.clip(pred, eps, 1 - eps)\n",
        "        loss = - (y * np.log(pred) + (1 - y) * np.log(1 - pred)).mean()\n",
        "        # Exclude bias term from regularization\n",
        "        reg = (self.lam / (2 * m)) * np.sum(self.theta[1:] ** 2) if self.bias else (self.lam / (2 * m)) * np.sum(self.theta ** 2)\n",
        "        return loss + reg\n",
        "\n",
        "\n",
        "    def _gradient_descent(self, X, y):\n",
        "        m, n = X.shape\n",
        "        pred = self._logistic_hypothesis(X)\n",
        "        for j in range(n):\n",
        "            gradient = 0\n",
        "            for i in range(m):\n",
        "                gradient += (pred[i] - y[i]) * X[i, j]\n",
        "            # Regularize all parameters including theta_0 if no bias term, otherwise exclude theta_0\n",
        "            reg_term = self.lam * self.theta[j] if (not self.bias or j != 0) else 0\n",
        "            self.theta[j] = self.theta[j] - self.lr * ((gradient + reg_term) / m)\n",
        "\n",
        "\n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        \"\"\"\n",
        "        Train the logistic regression model using gradient descent.\n",
        "        Records training and validation loss per iteration.\n",
        "        \"\"\"\n",
        "        if self.bias:\n",
        "            X = np.c_[np.ones(X.shape[0]), X]\n",
        "            if X_val is not None:\n",
        "                X_val = np.c_[np.ones(X_val.shape[0]), X_val]\n",
        "\n",
        "        self.theta = np.zeros(X.shape[1])\n",
        "\n",
        "        for i in range(self.iter):\n",
        "            # Update parameters using gradient descent\n",
        "            self._gradient_descent(X, y)\n",
        "\n",
        "            # Compute and record training loss\n",
        "            self.loss[i] = self._compute_loss(X, y)\n",
        "\n",
        "            # Compute and record validation loss if validation data provided\n",
        "            if X_val is not None and y_val is not None:\n",
        "                self.val_loss[i] = self._compute_loss(X_val, y_val)\n",
        "\n",
        "            # Verbose output to track training progress\n",
        "            if self.verbose:\n",
        "                msg = f\"Iter {i+1}/{self.iter} - Train Loss: {self.loss[i]:.4f}\"\n",
        "                if X_val is not None and y_val is not None:\n",
        "                    msg += f\" - Val Loss: {self.val_loss[i]:.4f}\"\n",
        "                print(msg)\n",
        "\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        if self.bias:\n",
        "            X = np.c_[np.ones(X.shape[0]), X]\n",
        "        return self._logistic_hypothesis(X)\n",
        "\n",
        "    def predict(self, X):\n",
        "        proba = self.predict_proba(X)\n",
        "        return (proba >= 0.5).astype(int)"
      ],
      "metadata": {
        "id": "QYb9W3uLEEZS"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 5 Learning and Estimation"
      ],
      "metadata": {
        "id": "T4dJv0tSCOQc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Learn and estimate the scratch implementation for the binary classification of virgicolor and virginica in the iris data set provided in the Introduction to Scratch Machine Learning Sprint.\n",
        "\n"
      ],
      "metadata": {
        "id": "DLQuMP4XCqao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression as SklearnLogistic\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Filter to keep only versicolor (1) and virginica (2)\n",
        "binary_mask = (y == 1) | (y == 2)\n",
        "X_binary = X[binary_mask]\n",
        "y_binary = y[binary_mask]\n",
        "\n",
        "# Convert labels: versicolor -> 0, virginica -> 1\n",
        "y_binary = (y_binary == 2).astype(int)\n",
        "\n",
        "# Split dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_binary, y_binary, test_size=0.3, random_state=42, stratify=y_binary\n",
        ")\n",
        "\n",
        "# Initialize and train your ScratchLogisticRegression\n",
        "scratch_model = ScratchLogisticRegression(num_iter=1000, lr=0.1, lam=0.01, verbose=False)\n",
        "scratch_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred_scratch = scratch_model.predict(X_test)\n",
        "\n",
        "# Train scikit-learn logistic regression for comparison\n",
        "sklearn_model = SklearnLogistic(max_iter=1000)\n",
        "sklearn_model.fit(X_train, y_train)\n",
        "y_pred_sklearn = sklearn_model.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "print(\"Scratch Logistic Regression:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_scratch))\n",
        "print(\"Precision:\", precision_score(y_test, y_pred_scratch))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred_scratch))\n",
        "\n",
        "print(\"\\nScikit-learn Logistic Regression:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_sklearn))\n",
        "print(\"Precision:\", precision_score(y_test, y_pred_sklearn))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred_sklearn))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjh0cbpBOzbQ",
        "outputId": "adb12acf-4431-475f-8a46-1b52839acc9b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scratch Logistic Regression:\n",
            "Accuracy: 0.9333333333333333\n",
            "Precision: 0.8823529411764706\n",
            "Recall: 1.0\n",
            "\n",
            "Scikit-learn Logistic Regression:\n",
            "Accuracy: 0.9333333333333333\n",
            "Precision: 1.0\n",
            "Recall: 0.8666666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Both have equal accuracy, so overall performance is similar.**\n",
        "\n",
        "*Scratch LR:*\n",
        "\n",
        "More tolerant to false positives (lower precision 0.88)\n",
        "\n",
        "Catches all positives (recall 1.0)\n",
        "\n",
        "*Scikit-learn LR:*\n",
        "\n",
        "Perfect precision (no false positives)\n",
        "\n",
        "Misses some positives (recall 0.87)"
      ],
      "metadata": {
        "id": "4AUzZETFPYec"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 6 Plot of Learning Curve"
      ],
      "metadata": {
        "id": "IDjOdM0BCts5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look at the learning curve to see if the losses are falling properly."
      ],
      "metadata": {
        "id": "E2ZH5pRrCzUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train, X_val=X_test, y_val=y_test)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.title('Learning Curve')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.scatter(range(model.iter), model.loss, label='Train Loss', color='blue', s=10)\n",
        "plt.scatter(range(model.iter), model.val_loss, label='Validation Loss', color='orange', s=10)\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "Xyfwutlzd7nF",
        "outputId": "d089e6f0-dfe3-4069-9e7c-c3be0f4697f1"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 7 Visualization of Decision area"
      ],
      "metadata": {
        "id": "VTiW8U-4C3TB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize the decision region. The following is a reference diagram. It is drawn with sepal width on the horizontal axis and petal length on the vertical axis."
      ],
      "metadata": {
        "id": "iZ-gIQx6C_QX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "import matplotlib.patches as mpatches\n",
        "\n",
        "def decision_region(X, y, model, step=0.01, title='decision region', xlabel='xlabel', ylabel='ylabel', target_names=['versicolor', 'virginica']):\n",
        "    \"\"\"\n",
        "    Draw the determination area of the model that learned binary classification with two-dimensional features.\n",
        "    The background color is drawn from the estimated values of the trained model.\n",
        "    The points on the scatter plot are training or validation data.\n",
        "    Parameters\n",
        "    ----------------\n",
        "    X : ndarray, shape(n_samples, 2)\n",
        "        Feature value\n",
        "    y : ndarray, shape(n_samples,)\n",
        "        label\n",
        "    model : object\n",
        "        Insert the installed model of the learned model\n",
        "    step : float, (default : 0.1)\n",
        "        Set the interval to calculate the estimate\n",
        "    title : str\n",
        "        Give the text of the graph Title\n",
        "    xlabel, ylabel : str\n",
        "        Give the text of the axis label\n",
        "    target_names= : list of str\n",
        "        Give a list of legends\n",
        "    \"\"\"\n",
        "    # setting\n",
        "    scatter_color = ['red', 'blue']\n",
        "    contourf_color = ['pink', 'skyblue']\n",
        "    n_class = 2\n",
        "    # pred\n",
        "    mesh_f0, mesh_f1  = np.meshgrid(np.arange(np.min(X[:,0])-0.5, np.max(X[:,0])+0.5, step), np.arange(np.min(X[:,1])-0.5, np.max(X[:,1])+0.5, step))\n",
        "    mesh = np.c_[np.ravel(mesh_f0),np.ravel(mesh_f1)]\n",
        "    y_pred = model.predict(mesh).reshape(mesh_f0.shape)\n",
        "    # plot\n",
        "    plt.title(title)\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.contourf(mesh_f0, mesh_f1, y_pred, n_class-1, cmap=ListedColormap(contourf_color))\n",
        "    plt.contour(mesh_f0, mesh_f1, y_pred, n_class-1, colors='y', linewidths=3, alpha=0.5)\n",
        "    for i, target in enumerate(set(y)):\n",
        "        plt.scatter(X[y==target][:, 0], X[y==target][:, 1], s=80, color=scatter_color[i], label=target_names[i], marker='o')\n",
        "    patches = [mpatches.Patch(color=scatter_color[i], label=target_names[i]) for i in range(n_class)]\n",
        "    plt.legend(handles=patches)\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "wYSODUwGipaQ"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "30755618",
        "outputId": "c3a4ac09-a2fd-451b-9028-7f960e6adb50"
      },
      "source": [
        "slr = ScratchLogisticRegression(num_iter=1000, lr=0.01, lam=0.1, verbose=True)\n",
        "slr.fit(X_train_scaled, y_train, X_test_scaled, y_test)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 1/1000 - Train Loss: 0.6914 - Val Loss: 0.6914\n",
            "Iter 2/1000 - Train Loss: 0.6897 - Val Loss: 0.6898\n",
            "Iter 3/1000 - Train Loss: 0.6881 - Val Loss: 0.6881\n",
            "Iter 4/1000 - Train Loss: 0.6864 - Val Loss: 0.6864\n",
            "Iter 5/1000 - Train Loss: 0.6847 - Val Loss: 0.6847\n",
            "Iter 6/1000 - Train Loss: 0.6831 - Val Loss: 0.6831\n",
            "Iter 7/1000 - Train Loss: 0.6814 - Val Loss: 0.6815\n",
            "Iter 8/1000 - Train Loss: 0.6798 - Val Loss: 0.6798\n",
            "Iter 9/1000 - Train Loss: 0.6782 - Val Loss: 0.6782\n",
            "Iter 10/1000 - Train Loss: 0.6766 - Val Loss: 0.6766\n",
            "Iter 11/1000 - Train Loss: 0.6750 - Val Loss: 0.6750\n",
            "Iter 12/1000 - Train Loss: 0.6734 - Val Loss: 0.6734\n",
            "Iter 13/1000 - Train Loss: 0.6718 - Val Loss: 0.6719\n",
            "Iter 14/1000 - Train Loss: 0.6702 - Val Loss: 0.6703\n",
            "Iter 15/1000 - Train Loss: 0.6687 - Val Loss: 0.6688\n",
            "Iter 16/1000 - Train Loss: 0.6671 - Val Loss: 0.6672\n",
            "Iter 17/1000 - Train Loss: 0.6656 - Val Loss: 0.6657\n",
            "Iter 18/1000 - Train Loss: 0.6641 - Val Loss: 0.6642\n",
            "Iter 19/1000 - Train Loss: 0.6625 - Val Loss: 0.6627\n",
            "Iter 20/1000 - Train Loss: 0.6610 - Val Loss: 0.6612\n",
            "Iter 21/1000 - Train Loss: 0.6595 - Val Loss: 0.6597\n",
            "Iter 22/1000 - Train Loss: 0.6580 - Val Loss: 0.6582\n",
            "Iter 23/1000 - Train Loss: 0.6565 - Val Loss: 0.6567\n",
            "Iter 24/1000 - Train Loss: 0.6551 - Val Loss: 0.6553\n",
            "Iter 25/1000 - Train Loss: 0.6536 - Val Loss: 0.6538\n",
            "Iter 26/1000 - Train Loss: 0.6521 - Val Loss: 0.6524\n",
            "Iter 27/1000 - Train Loss: 0.6507 - Val Loss: 0.6510\n",
            "Iter 28/1000 - Train Loss: 0.6492 - Val Loss: 0.6496\n",
            "Iter 29/1000 - Train Loss: 0.6478 - Val Loss: 0.6481\n",
            "Iter 30/1000 - Train Loss: 0.6464 - Val Loss: 0.6467\n",
            "Iter 31/1000 - Train Loss: 0.6450 - Val Loss: 0.6454\n",
            "Iter 32/1000 - Train Loss: 0.6436 - Val Loss: 0.6440\n",
            "Iter 33/1000 - Train Loss: 0.6422 - Val Loss: 0.6426\n",
            "Iter 34/1000 - Train Loss: 0.6408 - Val Loss: 0.6412\n",
            "Iter 35/1000 - Train Loss: 0.6394 - Val Loss: 0.6399\n",
            "Iter 36/1000 - Train Loss: 0.6380 - Val Loss: 0.6385\n",
            "Iter 37/1000 - Train Loss: 0.6367 - Val Loss: 0.6372\n",
            "Iter 38/1000 - Train Loss: 0.6353 - Val Loss: 0.6359\n",
            "Iter 39/1000 - Train Loss: 0.6340 - Val Loss: 0.6346\n",
            "Iter 40/1000 - Train Loss: 0.6326 - Val Loss: 0.6332\n",
            "Iter 41/1000 - Train Loss: 0.6313 - Val Loss: 0.6319\n",
            "Iter 42/1000 - Train Loss: 0.6300 - Val Loss: 0.6306\n",
            "Iter 43/1000 - Train Loss: 0.6287 - Val Loss: 0.6294\n",
            "Iter 44/1000 - Train Loss: 0.6273 - Val Loss: 0.6281\n",
            "Iter 45/1000 - Train Loss: 0.6260 - Val Loss: 0.6268\n",
            "Iter 46/1000 - Train Loss: 0.6248 - Val Loss: 0.6256\n",
            "Iter 47/1000 - Train Loss: 0.6235 - Val Loss: 0.6243\n",
            "Iter 48/1000 - Train Loss: 0.6222 - Val Loss: 0.6231\n",
            "Iter 49/1000 - Train Loss: 0.6209 - Val Loss: 0.6218\n",
            "Iter 50/1000 - Train Loss: 0.6197 - Val Loss: 0.6206\n",
            "Iter 51/1000 - Train Loss: 0.6184 - Val Loss: 0.6194\n",
            "Iter 52/1000 - Train Loss: 0.6172 - Val Loss: 0.6182\n",
            "Iter 53/1000 - Train Loss: 0.6159 - Val Loss: 0.6170\n",
            "Iter 54/1000 - Train Loss: 0.6147 - Val Loss: 0.6158\n",
            "Iter 55/1000 - Train Loss: 0.6135 - Val Loss: 0.6146\n",
            "Iter 56/1000 - Train Loss: 0.6123 - Val Loss: 0.6134\n",
            "Iter 57/1000 - Train Loss: 0.6110 - Val Loss: 0.6122\n",
            "Iter 58/1000 - Train Loss: 0.6098 - Val Loss: 0.6111\n",
            "Iter 59/1000 - Train Loss: 0.6086 - Val Loss: 0.6099\n",
            "Iter 60/1000 - Train Loss: 0.6075 - Val Loss: 0.6088\n",
            "Iter 61/1000 - Train Loss: 0.6063 - Val Loss: 0.6076\n",
            "Iter 62/1000 - Train Loss: 0.6051 - Val Loss: 0.6065\n",
            "Iter 63/1000 - Train Loss: 0.6039 - Val Loss: 0.6054\n",
            "Iter 64/1000 - Train Loss: 0.6028 - Val Loss: 0.6042\n",
            "Iter 65/1000 - Train Loss: 0.6016 - Val Loss: 0.6031\n",
            "Iter 66/1000 - Train Loss: 0.6005 - Val Loss: 0.6020\n",
            "Iter 67/1000 - Train Loss: 0.5993 - Val Loss: 0.6009\n",
            "Iter 68/1000 - Train Loss: 0.5982 - Val Loss: 0.5998\n",
            "Iter 69/1000 - Train Loss: 0.5970 - Val Loss: 0.5987\n",
            "Iter 70/1000 - Train Loss: 0.5959 - Val Loss: 0.5977\n",
            "Iter 71/1000 - Train Loss: 0.5948 - Val Loss: 0.5966\n",
            "Iter 72/1000 - Train Loss: 0.5937 - Val Loss: 0.5955\n",
            "Iter 73/1000 - Train Loss: 0.5926 - Val Loss: 0.5945\n",
            "Iter 74/1000 - Train Loss: 0.5915 - Val Loss: 0.5934\n",
            "Iter 75/1000 - Train Loss: 0.5904 - Val Loss: 0.5924\n",
            "Iter 76/1000 - Train Loss: 0.5893 - Val Loss: 0.5913\n",
            "Iter 77/1000 - Train Loss: 0.5882 - Val Loss: 0.5903\n",
            "Iter 78/1000 - Train Loss: 0.5872 - Val Loss: 0.5893\n",
            "Iter 79/1000 - Train Loss: 0.5861 - Val Loss: 0.5882\n",
            "Iter 80/1000 - Train Loss: 0.5850 - Val Loss: 0.5872\n",
            "Iter 81/1000 - Train Loss: 0.5840 - Val Loss: 0.5862\n",
            "Iter 82/1000 - Train Loss: 0.5829 - Val Loss: 0.5852\n",
            "Iter 83/1000 - Train Loss: 0.5819 - Val Loss: 0.5842\n",
            "Iter 84/1000 - Train Loss: 0.5808 - Val Loss: 0.5832\n",
            "Iter 85/1000 - Train Loss: 0.5798 - Val Loss: 0.5822\n",
            "Iter 86/1000 - Train Loss: 0.5788 - Val Loss: 0.5813\n",
            "Iter 87/1000 - Train Loss: 0.5778 - Val Loss: 0.5803\n",
            "Iter 88/1000 - Train Loss: 0.5768 - Val Loss: 0.5793\n",
            "Iter 89/1000 - Train Loss: 0.5757 - Val Loss: 0.5784\n",
            "Iter 90/1000 - Train Loss: 0.5747 - Val Loss: 0.5774\n",
            "Iter 91/1000 - Train Loss: 0.5737 - Val Loss: 0.5765\n",
            "Iter 92/1000 - Train Loss: 0.5727 - Val Loss: 0.5755\n",
            "Iter 93/1000 - Train Loss: 0.5718 - Val Loss: 0.5746\n",
            "Iter 94/1000 - Train Loss: 0.5708 - Val Loss: 0.5737\n",
            "Iter 95/1000 - Train Loss: 0.5698 - Val Loss: 0.5727\n",
            "Iter 96/1000 - Train Loss: 0.5688 - Val Loss: 0.5718\n",
            "Iter 97/1000 - Train Loss: 0.5679 - Val Loss: 0.5709\n",
            "Iter 98/1000 - Train Loss: 0.5669 - Val Loss: 0.5700\n",
            "Iter 99/1000 - Train Loss: 0.5659 - Val Loss: 0.5691\n",
            "Iter 100/1000 - Train Loss: 0.5650 - Val Loss: 0.5682\n",
            "Iter 101/1000 - Train Loss: 0.5640 - Val Loss: 0.5673\n",
            "Iter 102/1000 - Train Loss: 0.5631 - Val Loss: 0.5664\n",
            "Iter 103/1000 - Train Loss: 0.5622 - Val Loss: 0.5655\n",
            "Iter 104/1000 - Train Loss: 0.5612 - Val Loss: 0.5647\n",
            "Iter 105/1000 - Train Loss: 0.5603 - Val Loss: 0.5638\n",
            "Iter 106/1000 - Train Loss: 0.5594 - Val Loss: 0.5629\n",
            "Iter 107/1000 - Train Loss: 0.5585 - Val Loss: 0.5621\n",
            "Iter 108/1000 - Train Loss: 0.5575 - Val Loss: 0.5612\n",
            "Iter 109/1000 - Train Loss: 0.5566 - Val Loss: 0.5603\n",
            "Iter 110/1000 - Train Loss: 0.5557 - Val Loss: 0.5595\n",
            "Iter 111/1000 - Train Loss: 0.5548 - Val Loss: 0.5587\n",
            "Iter 112/1000 - Train Loss: 0.5539 - Val Loss: 0.5578\n",
            "Iter 113/1000 - Train Loss: 0.5531 - Val Loss: 0.5570\n",
            "Iter 114/1000 - Train Loss: 0.5522 - Val Loss: 0.5562\n",
            "Iter 115/1000 - Train Loss: 0.5513 - Val Loss: 0.5553\n",
            "Iter 116/1000 - Train Loss: 0.5504 - Val Loss: 0.5545\n",
            "Iter 117/1000 - Train Loss: 0.5495 - Val Loss: 0.5537\n",
            "Iter 118/1000 - Train Loss: 0.5487 - Val Loss: 0.5529\n",
            "Iter 119/1000 - Train Loss: 0.5478 - Val Loss: 0.5521\n",
            "Iter 120/1000 - Train Loss: 0.5470 - Val Loss: 0.5513\n",
            "Iter 121/1000 - Train Loss: 0.5461 - Val Loss: 0.5505\n",
            "Iter 122/1000 - Train Loss: 0.5452 - Val Loss: 0.5497\n",
            "Iter 123/1000 - Train Loss: 0.5444 - Val Loss: 0.5489\n",
            "Iter 124/1000 - Train Loss: 0.5436 - Val Loss: 0.5481\n",
            "Iter 125/1000 - Train Loss: 0.5427 - Val Loss: 0.5474\n",
            "Iter 126/1000 - Train Loss: 0.5419 - Val Loss: 0.5466\n",
            "Iter 127/1000 - Train Loss: 0.5411 - Val Loss: 0.5458\n",
            "Iter 128/1000 - Train Loss: 0.5402 - Val Loss: 0.5451\n",
            "Iter 129/1000 - Train Loss: 0.5394 - Val Loss: 0.5443\n",
            "Iter 130/1000 - Train Loss: 0.5386 - Val Loss: 0.5436\n",
            "Iter 131/1000 - Train Loss: 0.5378 - Val Loss: 0.5428\n",
            "Iter 132/1000 - Train Loss: 0.5370 - Val Loss: 0.5421\n",
            "Iter 133/1000 - Train Loss: 0.5362 - Val Loss: 0.5413\n",
            "Iter 134/1000 - Train Loss: 0.5354 - Val Loss: 0.5406\n",
            "Iter 135/1000 - Train Loss: 0.5346 - Val Loss: 0.5398\n",
            "Iter 136/1000 - Train Loss: 0.5338 - Val Loss: 0.5391\n",
            "Iter 137/1000 - Train Loss: 0.5330 - Val Loss: 0.5384\n",
            "Iter 138/1000 - Train Loss: 0.5322 - Val Loss: 0.5377\n",
            "Iter 139/1000 - Train Loss: 0.5314 - Val Loss: 0.5369\n",
            "Iter 140/1000 - Train Loss: 0.5307 - Val Loss: 0.5362\n",
            "Iter 141/1000 - Train Loss: 0.5299 - Val Loss: 0.5355\n",
            "Iter 142/1000 - Train Loss: 0.5291 - Val Loss: 0.5348\n",
            "Iter 143/1000 - Train Loss: 0.5284 - Val Loss: 0.5341\n",
            "Iter 144/1000 - Train Loss: 0.5276 - Val Loss: 0.5334\n",
            "Iter 145/1000 - Train Loss: 0.5268 - Val Loss: 0.5327\n",
            "Iter 146/1000 - Train Loss: 0.5261 - Val Loss: 0.5320\n",
            "Iter 147/1000 - Train Loss: 0.5253 - Val Loss: 0.5313\n",
            "Iter 148/1000 - Train Loss: 0.5246 - Val Loss: 0.5307\n",
            "Iter 149/1000 - Train Loss: 0.5238 - Val Loss: 0.5300\n",
            "Iter 150/1000 - Train Loss: 0.5231 - Val Loss: 0.5293\n",
            "Iter 151/1000 - Train Loss: 0.5224 - Val Loss: 0.5286\n",
            "Iter 152/1000 - Train Loss: 0.5216 - Val Loss: 0.5280\n",
            "Iter 153/1000 - Train Loss: 0.5209 - Val Loss: 0.5273\n",
            "Iter 154/1000 - Train Loss: 0.5202 - Val Loss: 0.5266\n",
            "Iter 155/1000 - Train Loss: 0.5194 - Val Loss: 0.5260\n",
            "Iter 156/1000 - Train Loss: 0.5187 - Val Loss: 0.5253\n",
            "Iter 157/1000 - Train Loss: 0.5180 - Val Loss: 0.5247\n",
            "Iter 158/1000 - Train Loss: 0.5173 - Val Loss: 0.5240\n",
            "Iter 159/1000 - Train Loss: 0.5166 - Val Loss: 0.5234\n",
            "Iter 160/1000 - Train Loss: 0.5159 - Val Loss: 0.5227\n",
            "Iter 161/1000 - Train Loss: 0.5152 - Val Loss: 0.5221\n",
            "Iter 162/1000 - Train Loss: 0.5145 - Val Loss: 0.5215\n",
            "Iter 163/1000 - Train Loss: 0.5138 - Val Loss: 0.5208\n",
            "Iter 164/1000 - Train Loss: 0.5131 - Val Loss: 0.5202\n",
            "Iter 165/1000 - Train Loss: 0.5124 - Val Loss: 0.5196\n",
            "Iter 166/1000 - Train Loss: 0.5117 - Val Loss: 0.5190\n",
            "Iter 167/1000 - Train Loss: 0.5110 - Val Loss: 0.5183\n",
            "Iter 168/1000 - Train Loss: 0.5103 - Val Loss: 0.5177\n",
            "Iter 169/1000 - Train Loss: 0.5096 - Val Loss: 0.5171\n",
            "Iter 170/1000 - Train Loss: 0.5090 - Val Loss: 0.5165\n",
            "Iter 171/1000 - Train Loss: 0.5083 - Val Loss: 0.5159\n",
            "Iter 172/1000 - Train Loss: 0.5076 - Val Loss: 0.5153\n",
            "Iter 173/1000 - Train Loss: 0.5070 - Val Loss: 0.5147\n",
            "Iter 174/1000 - Train Loss: 0.5063 - Val Loss: 0.5141\n",
            "Iter 175/1000 - Train Loss: 0.5056 - Val Loss: 0.5135\n",
            "Iter 176/1000 - Train Loss: 0.5050 - Val Loss: 0.5129\n",
            "Iter 177/1000 - Train Loss: 0.5043 - Val Loss: 0.5123\n",
            "Iter 178/1000 - Train Loss: 0.5037 - Val Loss: 0.5117\n",
            "Iter 179/1000 - Train Loss: 0.5030 - Val Loss: 0.5112\n",
            "Iter 180/1000 - Train Loss: 0.5024 - Val Loss: 0.5106\n",
            "Iter 181/1000 - Train Loss: 0.5017 - Val Loss: 0.5100\n",
            "Iter 182/1000 - Train Loss: 0.5011 - Val Loss: 0.5094\n",
            "Iter 183/1000 - Train Loss: 0.5004 - Val Loss: 0.5089\n",
            "Iter 184/1000 - Train Loss: 0.4998 - Val Loss: 0.5083\n",
            "Iter 185/1000 - Train Loss: 0.4992 - Val Loss: 0.5077\n",
            "Iter 186/1000 - Train Loss: 0.4985 - Val Loss: 0.5072\n",
            "Iter 187/1000 - Train Loss: 0.4979 - Val Loss: 0.5066\n",
            "Iter 188/1000 - Train Loss: 0.4973 - Val Loss: 0.5061\n",
            "Iter 189/1000 - Train Loss: 0.4967 - Val Loss: 0.5055\n",
            "Iter 190/1000 - Train Loss: 0.4961 - Val Loss: 0.5050\n",
            "Iter 191/1000 - Train Loss: 0.4954 - Val Loss: 0.5044\n",
            "Iter 192/1000 - Train Loss: 0.4948 - Val Loss: 0.5039\n",
            "Iter 193/1000 - Train Loss: 0.4942 - Val Loss: 0.5033\n",
            "Iter 194/1000 - Train Loss: 0.4936 - Val Loss: 0.5028\n",
            "Iter 195/1000 - Train Loss: 0.4930 - Val Loss: 0.5022\n",
            "Iter 196/1000 - Train Loss: 0.4924 - Val Loss: 0.5017\n",
            "Iter 197/1000 - Train Loss: 0.4918 - Val Loss: 0.5012\n",
            "Iter 198/1000 - Train Loss: 0.4912 - Val Loss: 0.5007\n",
            "Iter 199/1000 - Train Loss: 0.4906 - Val Loss: 0.5001\n",
            "Iter 200/1000 - Train Loss: 0.4900 - Val Loss: 0.4996\n",
            "Iter 201/1000 - Train Loss: 0.4894 - Val Loss: 0.4991\n",
            "Iter 202/1000 - Train Loss: 0.4888 - Val Loss: 0.4986\n",
            "Iter 203/1000 - Train Loss: 0.4882 - Val Loss: 0.4980\n",
            "Iter 204/1000 - Train Loss: 0.4876 - Val Loss: 0.4975\n",
            "Iter 205/1000 - Train Loss: 0.4871 - Val Loss: 0.4970\n",
            "Iter 206/1000 - Train Loss: 0.4865 - Val Loss: 0.4965\n",
            "Iter 207/1000 - Train Loss: 0.4859 - Val Loss: 0.4960\n",
            "Iter 208/1000 - Train Loss: 0.4853 - Val Loss: 0.4955\n",
            "Iter 209/1000 - Train Loss: 0.4848 - Val Loss: 0.4950\n",
            "Iter 210/1000 - Train Loss: 0.4842 - Val Loss: 0.4945\n",
            "Iter 211/1000 - Train Loss: 0.4836 - Val Loss: 0.4940\n",
            "Iter 212/1000 - Train Loss: 0.4831 - Val Loss: 0.4935\n",
            "Iter 213/1000 - Train Loss: 0.4825 - Val Loss: 0.4930\n",
            "Iter 214/1000 - Train Loss: 0.4819 - Val Loss: 0.4925\n",
            "Iter 215/1000 - Train Loss: 0.4814 - Val Loss: 0.4920\n",
            "Iter 216/1000 - Train Loss: 0.4808 - Val Loss: 0.4916\n",
            "Iter 217/1000 - Train Loss: 0.4803 - Val Loss: 0.4911\n",
            "Iter 218/1000 - Train Loss: 0.4797 - Val Loss: 0.4906\n",
            "Iter 219/1000 - Train Loss: 0.4792 - Val Loss: 0.4901\n",
            "Iter 220/1000 - Train Loss: 0.4786 - Val Loss: 0.4896\n",
            "Iter 221/1000 - Train Loss: 0.4781 - Val Loss: 0.4892\n",
            "Iter 222/1000 - Train Loss: 0.4775 - Val Loss: 0.4887\n",
            "Iter 223/1000 - Train Loss: 0.4770 - Val Loss: 0.4882\n",
            "Iter 224/1000 - Train Loss: 0.4764 - Val Loss: 0.4878\n",
            "Iter 225/1000 - Train Loss: 0.4759 - Val Loss: 0.4873\n",
            "Iter 226/1000 - Train Loss: 0.4754 - Val Loss: 0.4868\n",
            "Iter 227/1000 - Train Loss: 0.4748 - Val Loss: 0.4864\n",
            "Iter 228/1000 - Train Loss: 0.4743 - Val Loss: 0.4859\n",
            "Iter 229/1000 - Train Loss: 0.4738 - Val Loss: 0.4854\n",
            "Iter 230/1000 - Train Loss: 0.4733 - Val Loss: 0.4850\n",
            "Iter 231/1000 - Train Loss: 0.4727 - Val Loss: 0.4845\n",
            "Iter 232/1000 - Train Loss: 0.4722 - Val Loss: 0.4841\n",
            "Iter 233/1000 - Train Loss: 0.4717 - Val Loss: 0.4836\n",
            "Iter 234/1000 - Train Loss: 0.4712 - Val Loss: 0.4832\n",
            "Iter 235/1000 - Train Loss: 0.4707 - Val Loss: 0.4828\n",
            "Iter 236/1000 - Train Loss: 0.4701 - Val Loss: 0.4823\n",
            "Iter 237/1000 - Train Loss: 0.4696 - Val Loss: 0.4819\n",
            "Iter 238/1000 - Train Loss: 0.4691 - Val Loss: 0.4814\n",
            "Iter 239/1000 - Train Loss: 0.4686 - Val Loss: 0.4810\n",
            "Iter 240/1000 - Train Loss: 0.4681 - Val Loss: 0.4806\n",
            "Iter 241/1000 - Train Loss: 0.4676 - Val Loss: 0.4801\n",
            "Iter 242/1000 - Train Loss: 0.4671 - Val Loss: 0.4797\n",
            "Iter 243/1000 - Train Loss: 0.4666 - Val Loss: 0.4793\n",
            "Iter 244/1000 - Train Loss: 0.4661 - Val Loss: 0.4788\n",
            "Iter 245/1000 - Train Loss: 0.4656 - Val Loss: 0.4784\n",
            "Iter 246/1000 - Train Loss: 0.4651 - Val Loss: 0.4780\n",
            "Iter 247/1000 - Train Loss: 0.4646 - Val Loss: 0.4776\n",
            "Iter 248/1000 - Train Loss: 0.4641 - Val Loss: 0.4772\n",
            "Iter 249/1000 - Train Loss: 0.4636 - Val Loss: 0.4767\n",
            "Iter 250/1000 - Train Loss: 0.4631 - Val Loss: 0.4763\n",
            "Iter 251/1000 - Train Loss: 0.4626 - Val Loss: 0.4759\n",
            "Iter 252/1000 - Train Loss: 0.4622 - Val Loss: 0.4755\n",
            "Iter 253/1000 - Train Loss: 0.4617 - Val Loss: 0.4751\n",
            "Iter 254/1000 - Train Loss: 0.4612 - Val Loss: 0.4747\n",
            "Iter 255/1000 - Train Loss: 0.4607 - Val Loss: 0.4743\n",
            "Iter 256/1000 - Train Loss: 0.4602 - Val Loss: 0.4739\n",
            "Iter 257/1000 - Train Loss: 0.4598 - Val Loss: 0.4735\n",
            "Iter 258/1000 - Train Loss: 0.4593 - Val Loss: 0.4731\n",
            "Iter 259/1000 - Train Loss: 0.4588 - Val Loss: 0.4727\n",
            "Iter 260/1000 - Train Loss: 0.4583 - Val Loss: 0.4723\n",
            "Iter 261/1000 - Train Loss: 0.4579 - Val Loss: 0.4719\n",
            "Iter 262/1000 - Train Loss: 0.4574 - Val Loss: 0.4715\n",
            "Iter 263/1000 - Train Loss: 0.4569 - Val Loss: 0.4711\n",
            "Iter 264/1000 - Train Loss: 0.4565 - Val Loss: 0.4707\n",
            "Iter 265/1000 - Train Loss: 0.4560 - Val Loss: 0.4703\n",
            "Iter 266/1000 - Train Loss: 0.4555 - Val Loss: 0.4699\n",
            "Iter 267/1000 - Train Loss: 0.4551 - Val Loss: 0.4695\n",
            "Iter 268/1000 - Train Loss: 0.4546 - Val Loss: 0.4691\n",
            "Iter 269/1000 - Train Loss: 0.4542 - Val Loss: 0.4688\n",
            "Iter 270/1000 - Train Loss: 0.4537 - Val Loss: 0.4684\n",
            "Iter 271/1000 - Train Loss: 0.4533 - Val Loss: 0.4680\n",
            "Iter 272/1000 - Train Loss: 0.4528 - Val Loss: 0.4676\n",
            "Iter 273/1000 - Train Loss: 0.4524 - Val Loss: 0.4672\n",
            "Iter 274/1000 - Train Loss: 0.4519 - Val Loss: 0.4669\n",
            "Iter 275/1000 - Train Loss: 0.4515 - Val Loss: 0.4665\n",
            "Iter 276/1000 - Train Loss: 0.4510 - Val Loss: 0.4661\n",
            "Iter 277/1000 - Train Loss: 0.4506 - Val Loss: 0.4658\n",
            "Iter 278/1000 - Train Loss: 0.4501 - Val Loss: 0.4654\n",
            "Iter 279/1000 - Train Loss: 0.4497 - Val Loss: 0.4650\n",
            "Iter 280/1000 - Train Loss: 0.4493 - Val Loss: 0.4647\n",
            "Iter 281/1000 - Train Loss: 0.4488 - Val Loss: 0.4643\n",
            "Iter 282/1000 - Train Loss: 0.4484 - Val Loss: 0.4639\n",
            "Iter 283/1000 - Train Loss: 0.4479 - Val Loss: 0.4636\n",
            "Iter 284/1000 - Train Loss: 0.4475 - Val Loss: 0.4632\n",
            "Iter 285/1000 - Train Loss: 0.4471 - Val Loss: 0.4628\n",
            "Iter 286/1000 - Train Loss: 0.4467 - Val Loss: 0.4625\n",
            "Iter 287/1000 - Train Loss: 0.4462 - Val Loss: 0.4621\n",
            "Iter 288/1000 - Train Loss: 0.4458 - Val Loss: 0.4618\n",
            "Iter 289/1000 - Train Loss: 0.4454 - Val Loss: 0.4614\n",
            "Iter 290/1000 - Train Loss: 0.4449 - Val Loss: 0.4611\n",
            "Iter 291/1000 - Train Loss: 0.4445 - Val Loss: 0.4607\n",
            "Iter 292/1000 - Train Loss: 0.4441 - Val Loss: 0.4604\n",
            "Iter 293/1000 - Train Loss: 0.4437 - Val Loss: 0.4600\n",
            "Iter 294/1000 - Train Loss: 0.4433 - Val Loss: 0.4597\n",
            "Iter 295/1000 - Train Loss: 0.4428 - Val Loss: 0.4593\n",
            "Iter 296/1000 - Train Loss: 0.4424 - Val Loss: 0.4590\n",
            "Iter 297/1000 - Train Loss: 0.4420 - Val Loss: 0.4587\n",
            "Iter 298/1000 - Train Loss: 0.4416 - Val Loss: 0.4583\n",
            "Iter 299/1000 - Train Loss: 0.4412 - Val Loss: 0.4580\n",
            "Iter 300/1000 - Train Loss: 0.4408 - Val Loss: 0.4576\n",
            "Iter 301/1000 - Train Loss: 0.4404 - Val Loss: 0.4573\n",
            "Iter 302/1000 - Train Loss: 0.4400 - Val Loss: 0.4570\n",
            "Iter 303/1000 - Train Loss: 0.4396 - Val Loss: 0.4566\n",
            "Iter 304/1000 - Train Loss: 0.4391 - Val Loss: 0.4563\n",
            "Iter 305/1000 - Train Loss: 0.4387 - Val Loss: 0.4560\n",
            "Iter 306/1000 - Train Loss: 0.4383 - Val Loss: 0.4557\n",
            "Iter 307/1000 - Train Loss: 0.4379 - Val Loss: 0.4553\n",
            "Iter 308/1000 - Train Loss: 0.4375 - Val Loss: 0.4550\n",
            "Iter 309/1000 - Train Loss: 0.4371 - Val Loss: 0.4547\n",
            "Iter 310/1000 - Train Loss: 0.4367 - Val Loss: 0.4544\n",
            "Iter 311/1000 - Train Loss: 0.4363 - Val Loss: 0.4540\n",
            "Iter 312/1000 - Train Loss: 0.4359 - Val Loss: 0.4537\n",
            "Iter 313/1000 - Train Loss: 0.4356 - Val Loss: 0.4534\n",
            "Iter 314/1000 - Train Loss: 0.4352 - Val Loss: 0.4531\n",
            "Iter 315/1000 - Train Loss: 0.4348 - Val Loss: 0.4528\n",
            "Iter 316/1000 - Train Loss: 0.4344 - Val Loss: 0.4524\n",
            "Iter 317/1000 - Train Loss: 0.4340 - Val Loss: 0.4521\n",
            "Iter 318/1000 - Train Loss: 0.4336 - Val Loss: 0.4518\n",
            "Iter 319/1000 - Train Loss: 0.4332 - Val Loss: 0.4515\n",
            "Iter 320/1000 - Train Loss: 0.4328 - Val Loss: 0.4512\n",
            "Iter 321/1000 - Train Loss: 0.4324 - Val Loss: 0.4509\n",
            "Iter 322/1000 - Train Loss: 0.4321 - Val Loss: 0.4506\n",
            "Iter 323/1000 - Train Loss: 0.4317 - Val Loss: 0.4503\n",
            "Iter 324/1000 - Train Loss: 0.4313 - Val Loss: 0.4500\n",
            "Iter 325/1000 - Train Loss: 0.4309 - Val Loss: 0.4496\n",
            "Iter 326/1000 - Train Loss: 0.4305 - Val Loss: 0.4493\n",
            "Iter 327/1000 - Train Loss: 0.4302 - Val Loss: 0.4490\n",
            "Iter 328/1000 - Train Loss: 0.4298 - Val Loss: 0.4487\n",
            "Iter 329/1000 - Train Loss: 0.4294 - Val Loss: 0.4484\n",
            "Iter 330/1000 - Train Loss: 0.4290 - Val Loss: 0.4481\n",
            "Iter 331/1000 - Train Loss: 0.4287 - Val Loss: 0.4478\n",
            "Iter 332/1000 - Train Loss: 0.4283 - Val Loss: 0.4475\n",
            "Iter 333/1000 - Train Loss: 0.4279 - Val Loss: 0.4472\n",
            "Iter 334/1000 - Train Loss: 0.4276 - Val Loss: 0.4470\n",
            "Iter 335/1000 - Train Loss: 0.4272 - Val Loss: 0.4467\n",
            "Iter 336/1000 - Train Loss: 0.4268 - Val Loss: 0.4464\n",
            "Iter 337/1000 - Train Loss: 0.4265 - Val Loss: 0.4461\n",
            "Iter 338/1000 - Train Loss: 0.4261 - Val Loss: 0.4458\n",
            "Iter 339/1000 - Train Loss: 0.4257 - Val Loss: 0.4455\n",
            "Iter 340/1000 - Train Loss: 0.4254 - Val Loss: 0.4452\n",
            "Iter 341/1000 - Train Loss: 0.4250 - Val Loss: 0.4449\n",
            "Iter 342/1000 - Train Loss: 0.4246 - Val Loss: 0.4446\n",
            "Iter 343/1000 - Train Loss: 0.4243 - Val Loss: 0.4444\n",
            "Iter 344/1000 - Train Loss: 0.4239 - Val Loss: 0.4441\n",
            "Iter 345/1000 - Train Loss: 0.4236 - Val Loss: 0.4438\n",
            "Iter 346/1000 - Train Loss: 0.4232 - Val Loss: 0.4435\n",
            "Iter 347/1000 - Train Loss: 0.4229 - Val Loss: 0.4432\n",
            "Iter 348/1000 - Train Loss: 0.4225 - Val Loss: 0.4429\n",
            "Iter 349/1000 - Train Loss: 0.4222 - Val Loss: 0.4427\n",
            "Iter 350/1000 - Train Loss: 0.4218 - Val Loss: 0.4424\n",
            "Iter 351/1000 - Train Loss: 0.4215 - Val Loss: 0.4421\n",
            "Iter 352/1000 - Train Loss: 0.4211 - Val Loss: 0.4418\n",
            "Iter 353/1000 - Train Loss: 0.4208 - Val Loss: 0.4416\n",
            "Iter 354/1000 - Train Loss: 0.4204 - Val Loss: 0.4413\n",
            "Iter 355/1000 - Train Loss: 0.4201 - Val Loss: 0.4410\n",
            "Iter 356/1000 - Train Loss: 0.4197 - Val Loss: 0.4407\n",
            "Iter 357/1000 - Train Loss: 0.4194 - Val Loss: 0.4405\n",
            "Iter 358/1000 - Train Loss: 0.4190 - Val Loss: 0.4402\n",
            "Iter 359/1000 - Train Loss: 0.4187 - Val Loss: 0.4399\n",
            "Iter 360/1000 - Train Loss: 0.4183 - Val Loss: 0.4397\n",
            "Iter 361/1000 - Train Loss: 0.4180 - Val Loss: 0.4394\n",
            "Iter 362/1000 - Train Loss: 0.4177 - Val Loss: 0.4391\n",
            "Iter 363/1000 - Train Loss: 0.4173 - Val Loss: 0.4389\n",
            "Iter 364/1000 - Train Loss: 0.4170 - Val Loss: 0.4386\n",
            "Iter 365/1000 - Train Loss: 0.4166 - Val Loss: 0.4383\n",
            "Iter 366/1000 - Train Loss: 0.4163 - Val Loss: 0.4381\n",
            "Iter 367/1000 - Train Loss: 0.4160 - Val Loss: 0.4378\n",
            "Iter 368/1000 - Train Loss: 0.4156 - Val Loss: 0.4376\n",
            "Iter 369/1000 - Train Loss: 0.4153 - Val Loss: 0.4373\n",
            "Iter 370/1000 - Train Loss: 0.4150 - Val Loss: 0.4370\n",
            "Iter 371/1000 - Train Loss: 0.4146 - Val Loss: 0.4368\n",
            "Iter 372/1000 - Train Loss: 0.4143 - Val Loss: 0.4365\n",
            "Iter 373/1000 - Train Loss: 0.4140 - Val Loss: 0.4363\n",
            "Iter 374/1000 - Train Loss: 0.4137 - Val Loss: 0.4360\n",
            "Iter 375/1000 - Train Loss: 0.4133 - Val Loss: 0.4358\n",
            "Iter 376/1000 - Train Loss: 0.4130 - Val Loss: 0.4355\n",
            "Iter 377/1000 - Train Loss: 0.4127 - Val Loss: 0.4353\n",
            "Iter 378/1000 - Train Loss: 0.4124 - Val Loss: 0.4350\n",
            "Iter 379/1000 - Train Loss: 0.4120 - Val Loss: 0.4348\n",
            "Iter 380/1000 - Train Loss: 0.4117 - Val Loss: 0.4345\n",
            "Iter 381/1000 - Train Loss: 0.4114 - Val Loss: 0.4343\n",
            "Iter 382/1000 - Train Loss: 0.4111 - Val Loss: 0.4340\n",
            "Iter 383/1000 - Train Loss: 0.4107 - Val Loss: 0.4338\n",
            "Iter 384/1000 - Train Loss: 0.4104 - Val Loss: 0.4335\n",
            "Iter 385/1000 - Train Loss: 0.4101 - Val Loss: 0.4333\n",
            "Iter 386/1000 - Train Loss: 0.4098 - Val Loss: 0.4330\n",
            "Iter 387/1000 - Train Loss: 0.4095 - Val Loss: 0.4328\n",
            "Iter 388/1000 - Train Loss: 0.4092 - Val Loss: 0.4325\n",
            "Iter 389/1000 - Train Loss: 0.4088 - Val Loss: 0.4323\n",
            "Iter 390/1000 - Train Loss: 0.4085 - Val Loss: 0.4321\n",
            "Iter 391/1000 - Train Loss: 0.4082 - Val Loss: 0.4318\n",
            "Iter 392/1000 - Train Loss: 0.4079 - Val Loss: 0.4316\n",
            "Iter 393/1000 - Train Loss: 0.4076 - Val Loss: 0.4313\n",
            "Iter 394/1000 - Train Loss: 0.4073 - Val Loss: 0.4311\n",
            "Iter 395/1000 - Train Loss: 0.4070 - Val Loss: 0.4309\n",
            "Iter 396/1000 - Train Loss: 0.4067 - Val Loss: 0.4306\n",
            "Iter 397/1000 - Train Loss: 0.4064 - Val Loss: 0.4304\n",
            "Iter 398/1000 - Train Loss: 0.4060 - Val Loss: 0.4302\n",
            "Iter 399/1000 - Train Loss: 0.4057 - Val Loss: 0.4299\n",
            "Iter 400/1000 - Train Loss: 0.4054 - Val Loss: 0.4297\n",
            "Iter 401/1000 - Train Loss: 0.4051 - Val Loss: 0.4295\n",
            "Iter 402/1000 - Train Loss: 0.4048 - Val Loss: 0.4292\n",
            "Iter 403/1000 - Train Loss: 0.4045 - Val Loss: 0.4290\n",
            "Iter 404/1000 - Train Loss: 0.4042 - Val Loss: 0.4288\n",
            "Iter 405/1000 - Train Loss: 0.4039 - Val Loss: 0.4286\n",
            "Iter 406/1000 - Train Loss: 0.4036 - Val Loss: 0.4283\n",
            "Iter 407/1000 - Train Loss: 0.4033 - Val Loss: 0.4281\n",
            "Iter 408/1000 - Train Loss: 0.4030 - Val Loss: 0.4279\n",
            "Iter 409/1000 - Train Loss: 0.4027 - Val Loss: 0.4277\n",
            "Iter 410/1000 - Train Loss: 0.4024 - Val Loss: 0.4274\n",
            "Iter 411/1000 - Train Loss: 0.4021 - Val Loss: 0.4272\n",
            "Iter 412/1000 - Train Loss: 0.4018 - Val Loss: 0.4270\n",
            "Iter 413/1000 - Train Loss: 0.4015 - Val Loss: 0.4268\n",
            "Iter 414/1000 - Train Loss: 0.4012 - Val Loss: 0.4265\n",
            "Iter 415/1000 - Train Loss: 0.4009 - Val Loss: 0.4263\n",
            "Iter 416/1000 - Train Loss: 0.4006 - Val Loss: 0.4261\n",
            "Iter 417/1000 - Train Loss: 0.4004 - Val Loss: 0.4259\n",
            "Iter 418/1000 - Train Loss: 0.4001 - Val Loss: 0.4257\n",
            "Iter 419/1000 - Train Loss: 0.3998 - Val Loss: 0.4254\n",
            "Iter 420/1000 - Train Loss: 0.3995 - Val Loss: 0.4252\n",
            "Iter 421/1000 - Train Loss: 0.3992 - Val Loss: 0.4250\n",
            "Iter 422/1000 - Train Loss: 0.3989 - Val Loss: 0.4248\n",
            "Iter 423/1000 - Train Loss: 0.3986 - Val Loss: 0.4246\n",
            "Iter 424/1000 - Train Loss: 0.3983 - Val Loss: 0.4244\n",
            "Iter 425/1000 - Train Loss: 0.3980 - Val Loss: 0.4241\n",
            "Iter 426/1000 - Train Loss: 0.3978 - Val Loss: 0.4239\n",
            "Iter 427/1000 - Train Loss: 0.3975 - Val Loss: 0.4237\n",
            "Iter 428/1000 - Train Loss: 0.3972 - Val Loss: 0.4235\n",
            "Iter 429/1000 - Train Loss: 0.3969 - Val Loss: 0.4233\n",
            "Iter 430/1000 - Train Loss: 0.3966 - Val Loss: 0.4231\n",
            "Iter 431/1000 - Train Loss: 0.3963 - Val Loss: 0.4229\n",
            "Iter 432/1000 - Train Loss: 0.3961 - Val Loss: 0.4227\n",
            "Iter 433/1000 - Train Loss: 0.3958 - Val Loss: 0.4225\n",
            "Iter 434/1000 - Train Loss: 0.3955 - Val Loss: 0.4223\n",
            "Iter 435/1000 - Train Loss: 0.3952 - Val Loss: 0.4221\n",
            "Iter 436/1000 - Train Loss: 0.3949 - Val Loss: 0.4218\n",
            "Iter 437/1000 - Train Loss: 0.3947 - Val Loss: 0.4216\n",
            "Iter 438/1000 - Train Loss: 0.3944 - Val Loss: 0.4214\n",
            "Iter 439/1000 - Train Loss: 0.3941 - Val Loss: 0.4212\n",
            "Iter 440/1000 - Train Loss: 0.3938 - Val Loss: 0.4210\n",
            "Iter 441/1000 - Train Loss: 0.3935 - Val Loss: 0.4208\n",
            "Iter 442/1000 - Train Loss: 0.3933 - Val Loss: 0.4206\n",
            "Iter 443/1000 - Train Loss: 0.3930 - Val Loss: 0.4204\n",
            "Iter 444/1000 - Train Loss: 0.3927 - Val Loss: 0.4202\n",
            "Iter 445/1000 - Train Loss: 0.3925 - Val Loss: 0.4200\n",
            "Iter 446/1000 - Train Loss: 0.3922 - Val Loss: 0.4198\n",
            "Iter 447/1000 - Train Loss: 0.3919 - Val Loss: 0.4196\n",
            "Iter 448/1000 - Train Loss: 0.3916 - Val Loss: 0.4194\n",
            "Iter 449/1000 - Train Loss: 0.3914 - Val Loss: 0.4192\n",
            "Iter 450/1000 - Train Loss: 0.3911 - Val Loss: 0.4190\n",
            "Iter 451/1000 - Train Loss: 0.3908 - Val Loss: 0.4188\n",
            "Iter 452/1000 - Train Loss: 0.3906 - Val Loss: 0.4186\n",
            "Iter 453/1000 - Train Loss: 0.3903 - Val Loss: 0.4184\n",
            "Iter 454/1000 - Train Loss: 0.3900 - Val Loss: 0.4183\n",
            "Iter 455/1000 - Train Loss: 0.3898 - Val Loss: 0.4181\n",
            "Iter 456/1000 - Train Loss: 0.3895 - Val Loss: 0.4179\n",
            "Iter 457/1000 - Train Loss: 0.3892 - Val Loss: 0.4177\n",
            "Iter 458/1000 - Train Loss: 0.3890 - Val Loss: 0.4175\n",
            "Iter 459/1000 - Train Loss: 0.3887 - Val Loss: 0.4173\n",
            "Iter 460/1000 - Train Loss: 0.3884 - Val Loss: 0.4171\n",
            "Iter 461/1000 - Train Loss: 0.3882 - Val Loss: 0.4169\n",
            "Iter 462/1000 - Train Loss: 0.3879 - Val Loss: 0.4167\n",
            "Iter 463/1000 - Train Loss: 0.3877 - Val Loss: 0.4165\n",
            "Iter 464/1000 - Train Loss: 0.3874 - Val Loss: 0.4163\n",
            "Iter 465/1000 - Train Loss: 0.3871 - Val Loss: 0.4162\n",
            "Iter 466/1000 - Train Loss: 0.3869 - Val Loss: 0.4160\n",
            "Iter 467/1000 - Train Loss: 0.3866 - Val Loss: 0.4158\n",
            "Iter 468/1000 - Train Loss: 0.3864 - Val Loss: 0.4156\n",
            "Iter 469/1000 - Train Loss: 0.3861 - Val Loss: 0.4154\n",
            "Iter 470/1000 - Train Loss: 0.3858 - Val Loss: 0.4152\n",
            "Iter 471/1000 - Train Loss: 0.3856 - Val Loss: 0.4150\n",
            "Iter 472/1000 - Train Loss: 0.3853 - Val Loss: 0.4149\n",
            "Iter 473/1000 - Train Loss: 0.3851 - Val Loss: 0.4147\n",
            "Iter 474/1000 - Train Loss: 0.3848 - Val Loss: 0.4145\n",
            "Iter 475/1000 - Train Loss: 0.3846 - Val Loss: 0.4143\n",
            "Iter 476/1000 - Train Loss: 0.3843 - Val Loss: 0.4141\n",
            "Iter 477/1000 - Train Loss: 0.3841 - Val Loss: 0.4139\n",
            "Iter 478/1000 - Train Loss: 0.3838 - Val Loss: 0.4138\n",
            "Iter 479/1000 - Train Loss: 0.3836 - Val Loss: 0.4136\n",
            "Iter 480/1000 - Train Loss: 0.3833 - Val Loss: 0.4134\n",
            "Iter 481/1000 - Train Loss: 0.3831 - Val Loss: 0.4132\n",
            "Iter 482/1000 - Train Loss: 0.3828 - Val Loss: 0.4131\n",
            "Iter 483/1000 - Train Loss: 0.3826 - Val Loss: 0.4129\n",
            "Iter 484/1000 - Train Loss: 0.3823 - Val Loss: 0.4127\n",
            "Iter 485/1000 - Train Loss: 0.3821 - Val Loss: 0.4125\n",
            "Iter 486/1000 - Train Loss: 0.3818 - Val Loss: 0.4123\n",
            "Iter 487/1000 - Train Loss: 0.3816 - Val Loss: 0.4122\n",
            "Iter 488/1000 - Train Loss: 0.3813 - Val Loss: 0.4120\n",
            "Iter 489/1000 - Train Loss: 0.3811 - Val Loss: 0.4118\n",
            "Iter 490/1000 - Train Loss: 0.3808 - Val Loss: 0.4117\n",
            "Iter 491/1000 - Train Loss: 0.3806 - Val Loss: 0.4115\n",
            "Iter 492/1000 - Train Loss: 0.3803 - Val Loss: 0.4113\n",
            "Iter 493/1000 - Train Loss: 0.3801 - Val Loss: 0.4111\n",
            "Iter 494/1000 - Train Loss: 0.3799 - Val Loss: 0.4110\n",
            "Iter 495/1000 - Train Loss: 0.3796 - Val Loss: 0.4108\n",
            "Iter 496/1000 - Train Loss: 0.3794 - Val Loss: 0.4106\n",
            "Iter 497/1000 - Train Loss: 0.3791 - Val Loss: 0.4105\n",
            "Iter 498/1000 - Train Loss: 0.3789 - Val Loss: 0.4103\n",
            "Iter 499/1000 - Train Loss: 0.3786 - Val Loss: 0.4101\n",
            "Iter 500/1000 - Train Loss: 0.3784 - Val Loss: 0.4099\n",
            "Iter 501/1000 - Train Loss: 0.3782 - Val Loss: 0.4098\n",
            "Iter 502/1000 - Train Loss: 0.3779 - Val Loss: 0.4096\n",
            "Iter 503/1000 - Train Loss: 0.3777 - Val Loss: 0.4094\n",
            "Iter 504/1000 - Train Loss: 0.3775 - Val Loss: 0.4093\n",
            "Iter 505/1000 - Train Loss: 0.3772 - Val Loss: 0.4091\n",
            "Iter 506/1000 - Train Loss: 0.3770 - Val Loss: 0.4089\n",
            "Iter 507/1000 - Train Loss: 0.3767 - Val Loss: 0.4088\n",
            "Iter 508/1000 - Train Loss: 0.3765 - Val Loss: 0.4086\n",
            "Iter 509/1000 - Train Loss: 0.3763 - Val Loss: 0.4085\n",
            "Iter 510/1000 - Train Loss: 0.3760 - Val Loss: 0.4083\n",
            "Iter 511/1000 - Train Loss: 0.3758 - Val Loss: 0.4081\n",
            "Iter 512/1000 - Train Loss: 0.3756 - Val Loss: 0.4080\n",
            "Iter 513/1000 - Train Loss: 0.3753 - Val Loss: 0.4078\n",
            "Iter 514/1000 - Train Loss: 0.3751 - Val Loss: 0.4076\n",
            "Iter 515/1000 - Train Loss: 0.3749 - Val Loss: 0.4075\n",
            "Iter 516/1000 - Train Loss: 0.3746 - Val Loss: 0.4073\n",
            "Iter 517/1000 - Train Loss: 0.3744 - Val Loss: 0.4072\n",
            "Iter 518/1000 - Train Loss: 0.3742 - Val Loss: 0.4070\n",
            "Iter 519/1000 - Train Loss: 0.3739 - Val Loss: 0.4068\n",
            "Iter 520/1000 - Train Loss: 0.3737 - Val Loss: 0.4067\n",
            "Iter 521/1000 - Train Loss: 0.3735 - Val Loss: 0.4065\n",
            "Iter 522/1000 - Train Loss: 0.3733 - Val Loss: 0.4064\n",
            "Iter 523/1000 - Train Loss: 0.3730 - Val Loss: 0.4062\n",
            "Iter 524/1000 - Train Loss: 0.3728 - Val Loss: 0.4061\n",
            "Iter 525/1000 - Train Loss: 0.3726 - Val Loss: 0.4059\n",
            "Iter 526/1000 - Train Loss: 0.3724 - Val Loss: 0.4057\n",
            "Iter 527/1000 - Train Loss: 0.3721 - Val Loss: 0.4056\n",
            "Iter 528/1000 - Train Loss: 0.3719 - Val Loss: 0.4054\n",
            "Iter 529/1000 - Train Loss: 0.3717 - Val Loss: 0.4053\n",
            "Iter 530/1000 - Train Loss: 0.3714 - Val Loss: 0.4051\n",
            "Iter 531/1000 - Train Loss: 0.3712 - Val Loss: 0.4050\n",
            "Iter 532/1000 - Train Loss: 0.3710 - Val Loss: 0.4048\n",
            "Iter 533/1000 - Train Loss: 0.3708 - Val Loss: 0.4047\n",
            "Iter 534/1000 - Train Loss: 0.3706 - Val Loss: 0.4045\n",
            "Iter 535/1000 - Train Loss: 0.3703 - Val Loss: 0.4044\n",
            "Iter 536/1000 - Train Loss: 0.3701 - Val Loss: 0.4042\n",
            "Iter 537/1000 - Train Loss: 0.3699 - Val Loss: 0.4041\n",
            "Iter 538/1000 - Train Loss: 0.3697 - Val Loss: 0.4039\n",
            "Iter 539/1000 - Train Loss: 0.3694 - Val Loss: 0.4038\n",
            "Iter 540/1000 - Train Loss: 0.3692 - Val Loss: 0.4036\n",
            "Iter 541/1000 - Train Loss: 0.3690 - Val Loss: 0.4035\n",
            "Iter 542/1000 - Train Loss: 0.3688 - Val Loss: 0.4033\n",
            "Iter 543/1000 - Train Loss: 0.3686 - Val Loss: 0.4032\n",
            "Iter 544/1000 - Train Loss: 0.3684 - Val Loss: 0.4030\n",
            "Iter 545/1000 - Train Loss: 0.3681 - Val Loss: 0.4029\n",
            "Iter 546/1000 - Train Loss: 0.3679 - Val Loss: 0.4027\n",
            "Iter 547/1000 - Train Loss: 0.3677 - Val Loss: 0.4026\n",
            "Iter 548/1000 - Train Loss: 0.3675 - Val Loss: 0.4024\n",
            "Iter 549/1000 - Train Loss: 0.3673 - Val Loss: 0.4023\n",
            "Iter 550/1000 - Train Loss: 0.3671 - Val Loss: 0.4021\n",
            "Iter 551/1000 - Train Loss: 0.3668 - Val Loss: 0.4020\n",
            "Iter 552/1000 - Train Loss: 0.3666 - Val Loss: 0.4018\n",
            "Iter 553/1000 - Train Loss: 0.3664 - Val Loss: 0.4017\n",
            "Iter 554/1000 - Train Loss: 0.3662 - Val Loss: 0.4016\n",
            "Iter 555/1000 - Train Loss: 0.3660 - Val Loss: 0.4014\n",
            "Iter 556/1000 - Train Loss: 0.3658 - Val Loss: 0.4013\n",
            "Iter 557/1000 - Train Loss: 0.3656 - Val Loss: 0.4011\n",
            "Iter 558/1000 - Train Loss: 0.3653 - Val Loss: 0.4010\n",
            "Iter 559/1000 - Train Loss: 0.3651 - Val Loss: 0.4008\n",
            "Iter 560/1000 - Train Loss: 0.3649 - Val Loss: 0.4007\n",
            "Iter 561/1000 - Train Loss: 0.3647 - Val Loss: 0.4006\n",
            "Iter 562/1000 - Train Loss: 0.3645 - Val Loss: 0.4004\n",
            "Iter 563/1000 - Train Loss: 0.3643 - Val Loss: 0.4003\n",
            "Iter 564/1000 - Train Loss: 0.3641 - Val Loss: 0.4001\n",
            "Iter 565/1000 - Train Loss: 0.3639 - Val Loss: 0.4000\n",
            "Iter 566/1000 - Train Loss: 0.3637 - Val Loss: 0.3999\n",
            "Iter 567/1000 - Train Loss: 0.3635 - Val Loss: 0.3997\n",
            "Iter 568/1000 - Train Loss: 0.3632 - Val Loss: 0.3996\n",
            "Iter 569/1000 - Train Loss: 0.3630 - Val Loss: 0.3994\n",
            "Iter 570/1000 - Train Loss: 0.3628 - Val Loss: 0.3993\n",
            "Iter 571/1000 - Train Loss: 0.3626 - Val Loss: 0.3992\n",
            "Iter 572/1000 - Train Loss: 0.3624 - Val Loss: 0.3990\n",
            "Iter 573/1000 - Train Loss: 0.3622 - Val Loss: 0.3989\n",
            "Iter 574/1000 - Train Loss: 0.3620 - Val Loss: 0.3988\n",
            "Iter 575/1000 - Train Loss: 0.3618 - Val Loss: 0.3986\n",
            "Iter 576/1000 - Train Loss: 0.3616 - Val Loss: 0.3985\n",
            "Iter 577/1000 - Train Loss: 0.3614 - Val Loss: 0.3984\n",
            "Iter 578/1000 - Train Loss: 0.3612 - Val Loss: 0.3982\n",
            "Iter 579/1000 - Train Loss: 0.3610 - Val Loss: 0.3981\n",
            "Iter 580/1000 - Train Loss: 0.3608 - Val Loss: 0.3980\n",
            "Iter 581/1000 - Train Loss: 0.3606 - Val Loss: 0.3978\n",
            "Iter 582/1000 - Train Loss: 0.3604 - Val Loss: 0.3977\n",
            "Iter 583/1000 - Train Loss: 0.3602 - Val Loss: 0.3976\n",
            "Iter 584/1000 - Train Loss: 0.3600 - Val Loss: 0.3974\n",
            "Iter 585/1000 - Train Loss: 0.3598 - Val Loss: 0.3973\n",
            "Iter 586/1000 - Train Loss: 0.3596 - Val Loss: 0.3972\n",
            "Iter 587/1000 - Train Loss: 0.3594 - Val Loss: 0.3970\n",
            "Iter 588/1000 - Train Loss: 0.3592 - Val Loss: 0.3969\n",
            "Iter 589/1000 - Train Loss: 0.3590 - Val Loss: 0.3968\n",
            "Iter 590/1000 - Train Loss: 0.3588 - Val Loss: 0.3966\n",
            "Iter 591/1000 - Train Loss: 0.3586 - Val Loss: 0.3965\n",
            "Iter 592/1000 - Train Loss: 0.3584 - Val Loss: 0.3964\n",
            "Iter 593/1000 - Train Loss: 0.3582 - Val Loss: 0.3963\n",
            "Iter 594/1000 - Train Loss: 0.3580 - Val Loss: 0.3961\n",
            "Iter 595/1000 - Train Loss: 0.3578 - Val Loss: 0.3960\n",
            "Iter 596/1000 - Train Loss: 0.3576 - Val Loss: 0.3959\n",
            "Iter 597/1000 - Train Loss: 0.3574 - Val Loss: 0.3957\n",
            "Iter 598/1000 - Train Loss: 0.3572 - Val Loss: 0.3956\n",
            "Iter 599/1000 - Train Loss: 0.3570 - Val Loss: 0.3955\n",
            "Iter 600/1000 - Train Loss: 0.3568 - Val Loss: 0.3954\n",
            "Iter 601/1000 - Train Loss: 0.3566 - Val Loss: 0.3952\n",
            "Iter 602/1000 - Train Loss: 0.3564 - Val Loss: 0.3951\n",
            "Iter 603/1000 - Train Loss: 0.3562 - Val Loss: 0.3950\n",
            "Iter 604/1000 - Train Loss: 0.3560 - Val Loss: 0.3949\n",
            "Iter 605/1000 - Train Loss: 0.3558 - Val Loss: 0.3947\n",
            "Iter 606/1000 - Train Loss: 0.3556 - Val Loss: 0.3946\n",
            "Iter 607/1000 - Train Loss: 0.3555 - Val Loss: 0.3945\n",
            "Iter 608/1000 - Train Loss: 0.3553 - Val Loss: 0.3944\n",
            "Iter 609/1000 - Train Loss: 0.3551 - Val Loss: 0.3942\n",
            "Iter 610/1000 - Train Loss: 0.3549 - Val Loss: 0.3941\n",
            "Iter 611/1000 - Train Loss: 0.3547 - Val Loss: 0.3940\n",
            "Iter 612/1000 - Train Loss: 0.3545 - Val Loss: 0.3939\n",
            "Iter 613/1000 - Train Loss: 0.3543 - Val Loss: 0.3938\n",
            "Iter 614/1000 - Train Loss: 0.3541 - Val Loss: 0.3936\n",
            "Iter 615/1000 - Train Loss: 0.3539 - Val Loss: 0.3935\n",
            "Iter 616/1000 - Train Loss: 0.3537 - Val Loss: 0.3934\n",
            "Iter 617/1000 - Train Loss: 0.3536 - Val Loss: 0.3933\n",
            "Iter 618/1000 - Train Loss: 0.3534 - Val Loss: 0.3931\n",
            "Iter 619/1000 - Train Loss: 0.3532 - Val Loss: 0.3930\n",
            "Iter 620/1000 - Train Loss: 0.3530 - Val Loss: 0.3929\n",
            "Iter 621/1000 - Train Loss: 0.3528 - Val Loss: 0.3928\n",
            "Iter 622/1000 - Train Loss: 0.3526 - Val Loss: 0.3927\n",
            "Iter 623/1000 - Train Loss: 0.3524 - Val Loss: 0.3926\n",
            "Iter 624/1000 - Train Loss: 0.3522 - Val Loss: 0.3924\n",
            "Iter 625/1000 - Train Loss: 0.3521 - Val Loss: 0.3923\n",
            "Iter 626/1000 - Train Loss: 0.3519 - Val Loss: 0.3922\n",
            "Iter 627/1000 - Train Loss: 0.3517 - Val Loss: 0.3921\n",
            "Iter 628/1000 - Train Loss: 0.3515 - Val Loss: 0.3920\n",
            "Iter 629/1000 - Train Loss: 0.3513 - Val Loss: 0.3918\n",
            "Iter 630/1000 - Train Loss: 0.3511 - Val Loss: 0.3917\n",
            "Iter 631/1000 - Train Loss: 0.3509 - Val Loss: 0.3916\n",
            "Iter 632/1000 - Train Loss: 0.3508 - Val Loss: 0.3915\n",
            "Iter 633/1000 - Train Loss: 0.3506 - Val Loss: 0.3914\n",
            "Iter 634/1000 - Train Loss: 0.3504 - Val Loss: 0.3913\n",
            "Iter 635/1000 - Train Loss: 0.3502 - Val Loss: 0.3911\n",
            "Iter 636/1000 - Train Loss: 0.3500 - Val Loss: 0.3910\n",
            "Iter 637/1000 - Train Loss: 0.3498 - Val Loss: 0.3909\n",
            "Iter 638/1000 - Train Loss: 0.3497 - Val Loss: 0.3908\n",
            "Iter 639/1000 - Train Loss: 0.3495 - Val Loss: 0.3907\n",
            "Iter 640/1000 - Train Loss: 0.3493 - Val Loss: 0.3906\n",
            "Iter 641/1000 - Train Loss: 0.3491 - Val Loss: 0.3905\n",
            "Iter 642/1000 - Train Loss: 0.3489 - Val Loss: 0.3904\n",
            "Iter 643/1000 - Train Loss: 0.3488 - Val Loss: 0.3902\n",
            "Iter 644/1000 - Train Loss: 0.3486 - Val Loss: 0.3901\n",
            "Iter 645/1000 - Train Loss: 0.3484 - Val Loss: 0.3900\n",
            "Iter 646/1000 - Train Loss: 0.3482 - Val Loss: 0.3899\n",
            "Iter 647/1000 - Train Loss: 0.3480 - Val Loss: 0.3898\n",
            "Iter 648/1000 - Train Loss: 0.3479 - Val Loss: 0.3897\n",
            "Iter 649/1000 - Train Loss: 0.3477 - Val Loss: 0.3896\n",
            "Iter 650/1000 - Train Loss: 0.3475 - Val Loss: 0.3895\n",
            "Iter 651/1000 - Train Loss: 0.3473 - Val Loss: 0.3893\n",
            "Iter 652/1000 - Train Loss: 0.3472 - Val Loss: 0.3892\n",
            "Iter 653/1000 - Train Loss: 0.3470 - Val Loss: 0.3891\n",
            "Iter 654/1000 - Train Loss: 0.3468 - Val Loss: 0.3890\n",
            "Iter 655/1000 - Train Loss: 0.3466 - Val Loss: 0.3889\n",
            "Iter 656/1000 - Train Loss: 0.3464 - Val Loss: 0.3888\n",
            "Iter 657/1000 - Train Loss: 0.3463 - Val Loss: 0.3887\n",
            "Iter 658/1000 - Train Loss: 0.3461 - Val Loss: 0.3886\n",
            "Iter 659/1000 - Train Loss: 0.3459 - Val Loss: 0.3885\n",
            "Iter 660/1000 - Train Loss: 0.3457 - Val Loss: 0.3884\n",
            "Iter 661/1000 - Train Loss: 0.3456 - Val Loss: 0.3883\n",
            "Iter 662/1000 - Train Loss: 0.3454 - Val Loss: 0.3882\n",
            "Iter 663/1000 - Train Loss: 0.3452 - Val Loss: 0.3880\n",
            "Iter 664/1000 - Train Loss: 0.3451 - Val Loss: 0.3879\n",
            "Iter 665/1000 - Train Loss: 0.3449 - Val Loss: 0.3878\n",
            "Iter 666/1000 - Train Loss: 0.3447 - Val Loss: 0.3877\n",
            "Iter 667/1000 - Train Loss: 0.3445 - Val Loss: 0.3876\n",
            "Iter 668/1000 - Train Loss: 0.3444 - Val Loss: 0.3875\n",
            "Iter 669/1000 - Train Loss: 0.3442 - Val Loss: 0.3874\n",
            "Iter 670/1000 - Train Loss: 0.3440 - Val Loss: 0.3873\n",
            "Iter 671/1000 - Train Loss: 0.3438 - Val Loss: 0.3872\n",
            "Iter 672/1000 - Train Loss: 0.3437 - Val Loss: 0.3871\n",
            "Iter 673/1000 - Train Loss: 0.3435 - Val Loss: 0.3870\n",
            "Iter 674/1000 - Train Loss: 0.3433 - Val Loss: 0.3869\n",
            "Iter 675/1000 - Train Loss: 0.3432 - Val Loss: 0.3868\n",
            "Iter 676/1000 - Train Loss: 0.3430 - Val Loss: 0.3867\n",
            "Iter 677/1000 - Train Loss: 0.3428 - Val Loss: 0.3866\n",
            "Iter 678/1000 - Train Loss: 0.3427 - Val Loss: 0.3865\n",
            "Iter 679/1000 - Train Loss: 0.3425 - Val Loss: 0.3864\n",
            "Iter 680/1000 - Train Loss: 0.3423 - Val Loss: 0.3863\n",
            "Iter 681/1000 - Train Loss: 0.3422 - Val Loss: 0.3862\n",
            "Iter 682/1000 - Train Loss: 0.3420 - Val Loss: 0.3861\n",
            "Iter 683/1000 - Train Loss: 0.3418 - Val Loss: 0.3860\n",
            "Iter 684/1000 - Train Loss: 0.3416 - Val Loss: 0.3859\n",
            "Iter 685/1000 - Train Loss: 0.3415 - Val Loss: 0.3858\n",
            "Iter 686/1000 - Train Loss: 0.3413 - Val Loss: 0.3857\n",
            "Iter 687/1000 - Train Loss: 0.3411 - Val Loss: 0.3856\n",
            "Iter 688/1000 - Train Loss: 0.3410 - Val Loss: 0.3855\n",
            "Iter 689/1000 - Train Loss: 0.3408 - Val Loss: 0.3854\n",
            "Iter 690/1000 - Train Loss: 0.3406 - Val Loss: 0.3853\n",
            "Iter 691/1000 - Train Loss: 0.3405 - Val Loss: 0.3852\n",
            "Iter 692/1000 - Train Loss: 0.3403 - Val Loss: 0.3851\n",
            "Iter 693/1000 - Train Loss: 0.3402 - Val Loss: 0.3850\n",
            "Iter 694/1000 - Train Loss: 0.3400 - Val Loss: 0.3849\n",
            "Iter 695/1000 - Train Loss: 0.3398 - Val Loss: 0.3848\n",
            "Iter 696/1000 - Train Loss: 0.3397 - Val Loss: 0.3847\n",
            "Iter 697/1000 - Train Loss: 0.3395 - Val Loss: 0.3846\n",
            "Iter 698/1000 - Train Loss: 0.3393 - Val Loss: 0.3845\n",
            "Iter 699/1000 - Train Loss: 0.3392 - Val Loss: 0.3844\n",
            "Iter 700/1000 - Train Loss: 0.3390 - Val Loss: 0.3843\n",
            "Iter 701/1000 - Train Loss: 0.3388 - Val Loss: 0.3842\n",
            "Iter 702/1000 - Train Loss: 0.3387 - Val Loss: 0.3841\n",
            "Iter 703/1000 - Train Loss: 0.3385 - Val Loss: 0.3840\n",
            "Iter 704/1000 - Train Loss: 0.3384 - Val Loss: 0.3839\n",
            "Iter 705/1000 - Train Loss: 0.3382 - Val Loss: 0.3838\n",
            "Iter 706/1000 - Train Loss: 0.3380 - Val Loss: 0.3837\n",
            "Iter 707/1000 - Train Loss: 0.3379 - Val Loss: 0.3836\n",
            "Iter 708/1000 - Train Loss: 0.3377 - Val Loss: 0.3835\n",
            "Iter 709/1000 - Train Loss: 0.3376 - Val Loss: 0.3834\n",
            "Iter 710/1000 - Train Loss: 0.3374 - Val Loss: 0.3833\n",
            "Iter 711/1000 - Train Loss: 0.3372 - Val Loss: 0.3832\n",
            "Iter 712/1000 - Train Loss: 0.3371 - Val Loss: 0.3831\n",
            "Iter 713/1000 - Train Loss: 0.3369 - Val Loss: 0.3830\n",
            "Iter 714/1000 - Train Loss: 0.3368 - Val Loss: 0.3829\n",
            "Iter 715/1000 - Train Loss: 0.3366 - Val Loss: 0.3829\n",
            "Iter 716/1000 - Train Loss: 0.3364 - Val Loss: 0.3828\n",
            "Iter 717/1000 - Train Loss: 0.3363 - Val Loss: 0.3827\n",
            "Iter 718/1000 - Train Loss: 0.3361 - Val Loss: 0.3826\n",
            "Iter 719/1000 - Train Loss: 0.3360 - Val Loss: 0.3825\n",
            "Iter 720/1000 - Train Loss: 0.3358 - Val Loss: 0.3824\n",
            "Iter 721/1000 - Train Loss: 0.3356 - Val Loss: 0.3823\n",
            "Iter 722/1000 - Train Loss: 0.3355 - Val Loss: 0.3822\n",
            "Iter 723/1000 - Train Loss: 0.3353 - Val Loss: 0.3821\n",
            "Iter 724/1000 - Train Loss: 0.3352 - Val Loss: 0.3820\n",
            "Iter 725/1000 - Train Loss: 0.3350 - Val Loss: 0.3819\n",
            "Iter 726/1000 - Train Loss: 0.3349 - Val Loss: 0.3818\n",
            "Iter 727/1000 - Train Loss: 0.3347 - Val Loss: 0.3817\n",
            "Iter 728/1000 - Train Loss: 0.3345 - Val Loss: 0.3817\n",
            "Iter 729/1000 - Train Loss: 0.3344 - Val Loss: 0.3816\n",
            "Iter 730/1000 - Train Loss: 0.3342 - Val Loss: 0.3815\n",
            "Iter 731/1000 - Train Loss: 0.3341 - Val Loss: 0.3814\n",
            "Iter 732/1000 - Train Loss: 0.3339 - Val Loss: 0.3813\n",
            "Iter 733/1000 - Train Loss: 0.3338 - Val Loss: 0.3812\n",
            "Iter 734/1000 - Train Loss: 0.3336 - Val Loss: 0.3811\n",
            "Iter 735/1000 - Train Loss: 0.3335 - Val Loss: 0.3810\n",
            "Iter 736/1000 - Train Loss: 0.3333 - Val Loss: 0.3809\n",
            "Iter 737/1000 - Train Loss: 0.3332 - Val Loss: 0.3808\n",
            "Iter 738/1000 - Train Loss: 0.3330 - Val Loss: 0.3808\n",
            "Iter 739/1000 - Train Loss: 0.3329 - Val Loss: 0.3807\n",
            "Iter 740/1000 - Train Loss: 0.3327 - Val Loss: 0.3806\n",
            "Iter 741/1000 - Train Loss: 0.3326 - Val Loss: 0.3805\n",
            "Iter 742/1000 - Train Loss: 0.3324 - Val Loss: 0.3804\n",
            "Iter 743/1000 - Train Loss: 0.3322 - Val Loss: 0.3803\n",
            "Iter 744/1000 - Train Loss: 0.3321 - Val Loss: 0.3802\n",
            "Iter 745/1000 - Train Loss: 0.3319 - Val Loss: 0.3801\n",
            "Iter 746/1000 - Train Loss: 0.3318 - Val Loss: 0.3801\n",
            "Iter 747/1000 - Train Loss: 0.3316 - Val Loss: 0.3800\n",
            "Iter 748/1000 - Train Loss: 0.3315 - Val Loss: 0.3799\n",
            "Iter 749/1000 - Train Loss: 0.3313 - Val Loss: 0.3798\n",
            "Iter 750/1000 - Train Loss: 0.3312 - Val Loss: 0.3797\n",
            "Iter 751/1000 - Train Loss: 0.3310 - Val Loss: 0.3796\n",
            "Iter 752/1000 - Train Loss: 0.3309 - Val Loss: 0.3795\n",
            "Iter 753/1000 - Train Loss: 0.3307 - Val Loss: 0.3795\n",
            "Iter 754/1000 - Train Loss: 0.3306 - Val Loss: 0.3794\n",
            "Iter 755/1000 - Train Loss: 0.3304 - Val Loss: 0.3793\n",
            "Iter 756/1000 - Train Loss: 0.3303 - Val Loss: 0.3792\n",
            "Iter 757/1000 - Train Loss: 0.3301 - Val Loss: 0.3791\n",
            "Iter 758/1000 - Train Loss: 0.3300 - Val Loss: 0.3790\n",
            "Iter 759/1000 - Train Loss: 0.3298 - Val Loss: 0.3790\n",
            "Iter 760/1000 - Train Loss: 0.3297 - Val Loss: 0.3789\n",
            "Iter 761/1000 - Train Loss: 0.3296 - Val Loss: 0.3788\n",
            "Iter 762/1000 - Train Loss: 0.3294 - Val Loss: 0.3787\n",
            "Iter 763/1000 - Train Loss: 0.3293 - Val Loss: 0.3786\n",
            "Iter 764/1000 - Train Loss: 0.3291 - Val Loss: 0.3785\n",
            "Iter 765/1000 - Train Loss: 0.3290 - Val Loss: 0.3785\n",
            "Iter 766/1000 - Train Loss: 0.3288 - Val Loss: 0.3784\n",
            "Iter 767/1000 - Train Loss: 0.3287 - Val Loss: 0.3783\n",
            "Iter 768/1000 - Train Loss: 0.3285 - Val Loss: 0.3782\n",
            "Iter 769/1000 - Train Loss: 0.3284 - Val Loss: 0.3781\n",
            "Iter 770/1000 - Train Loss: 0.3282 - Val Loss: 0.3780\n",
            "Iter 771/1000 - Train Loss: 0.3281 - Val Loss: 0.3780\n",
            "Iter 772/1000 - Train Loss: 0.3279 - Val Loss: 0.3779\n",
            "Iter 773/1000 - Train Loss: 0.3278 - Val Loss: 0.3778\n",
            "Iter 774/1000 - Train Loss: 0.3277 - Val Loss: 0.3777\n",
            "Iter 775/1000 - Train Loss: 0.3275 - Val Loss: 0.3776\n",
            "Iter 776/1000 - Train Loss: 0.3274 - Val Loss: 0.3776\n",
            "Iter 777/1000 - Train Loss: 0.3272 - Val Loss: 0.3775\n",
            "Iter 778/1000 - Train Loss: 0.3271 - Val Loss: 0.3774\n",
            "Iter 779/1000 - Train Loss: 0.3269 - Val Loss: 0.3773\n",
            "Iter 780/1000 - Train Loss: 0.3268 - Val Loss: 0.3772\n",
            "Iter 781/1000 - Train Loss: 0.3267 - Val Loss: 0.3772\n",
            "Iter 782/1000 - Train Loss: 0.3265 - Val Loss: 0.3771\n",
            "Iter 783/1000 - Train Loss: 0.3264 - Val Loss: 0.3770\n",
            "Iter 784/1000 - Train Loss: 0.3262 - Val Loss: 0.3769\n",
            "Iter 785/1000 - Train Loss: 0.3261 - Val Loss: 0.3768\n",
            "Iter 786/1000 - Train Loss: 0.3259 - Val Loss: 0.3768\n",
            "Iter 787/1000 - Train Loss: 0.3258 - Val Loss: 0.3767\n",
            "Iter 788/1000 - Train Loss: 0.3257 - Val Loss: 0.3766\n",
            "Iter 789/1000 - Train Loss: 0.3255 - Val Loss: 0.3765\n",
            "Iter 790/1000 - Train Loss: 0.3254 - Val Loss: 0.3764\n",
            "Iter 791/1000 - Train Loss: 0.3252 - Val Loss: 0.3764\n",
            "Iter 792/1000 - Train Loss: 0.3251 - Val Loss: 0.3763\n",
            "Iter 793/1000 - Train Loss: 0.3250 - Val Loss: 0.3762\n",
            "Iter 794/1000 - Train Loss: 0.3248 - Val Loss: 0.3761\n",
            "Iter 795/1000 - Train Loss: 0.3247 - Val Loss: 0.3761\n",
            "Iter 796/1000 - Train Loss: 0.3245 - Val Loss: 0.3760\n",
            "Iter 797/1000 - Train Loss: 0.3244 - Val Loss: 0.3759\n",
            "Iter 798/1000 - Train Loss: 0.3243 - Val Loss: 0.3758\n",
            "Iter 799/1000 - Train Loss: 0.3241 - Val Loss: 0.3757\n",
            "Iter 800/1000 - Train Loss: 0.3240 - Val Loss: 0.3757\n",
            "Iter 801/1000 - Train Loss: 0.3238 - Val Loss: 0.3756\n",
            "Iter 802/1000 - Train Loss: 0.3237 - Val Loss: 0.3755\n",
            "Iter 803/1000 - Train Loss: 0.3236 - Val Loss: 0.3754\n",
            "Iter 804/1000 - Train Loss: 0.3234 - Val Loss: 0.3754\n",
            "Iter 805/1000 - Train Loss: 0.3233 - Val Loss: 0.3753\n",
            "Iter 806/1000 - Train Loss: 0.3231 - Val Loss: 0.3752\n",
            "Iter 807/1000 - Train Loss: 0.3230 - Val Loss: 0.3751\n",
            "Iter 808/1000 - Train Loss: 0.3229 - Val Loss: 0.3751\n",
            "Iter 809/1000 - Train Loss: 0.3227 - Val Loss: 0.3750\n",
            "Iter 810/1000 - Train Loss: 0.3226 - Val Loss: 0.3749\n",
            "Iter 811/1000 - Train Loss: 0.3225 - Val Loss: 0.3748\n",
            "Iter 812/1000 - Train Loss: 0.3223 - Val Loss: 0.3748\n",
            "Iter 813/1000 - Train Loss: 0.3222 - Val Loss: 0.3747\n",
            "Iter 814/1000 - Train Loss: 0.3221 - Val Loss: 0.3746\n",
            "Iter 815/1000 - Train Loss: 0.3219 - Val Loss: 0.3746\n",
            "Iter 816/1000 - Train Loss: 0.3218 - Val Loss: 0.3745\n",
            "Iter 817/1000 - Train Loss: 0.3216 - Val Loss: 0.3744\n",
            "Iter 818/1000 - Train Loss: 0.3215 - Val Loss: 0.3743\n",
            "Iter 819/1000 - Train Loss: 0.3214 - Val Loss: 0.3743\n",
            "Iter 820/1000 - Train Loss: 0.3212 - Val Loss: 0.3742\n",
            "Iter 821/1000 - Train Loss: 0.3211 - Val Loss: 0.3741\n",
            "Iter 822/1000 - Train Loss: 0.3210 - Val Loss: 0.3740\n",
            "Iter 823/1000 - Train Loss: 0.3208 - Val Loss: 0.3740\n",
            "Iter 824/1000 - Train Loss: 0.3207 - Val Loss: 0.3739\n",
            "Iter 825/1000 - Train Loss: 0.3206 - Val Loss: 0.3738\n",
            "Iter 826/1000 - Train Loss: 0.3204 - Val Loss: 0.3738\n",
            "Iter 827/1000 - Train Loss: 0.3203 - Val Loss: 0.3737\n",
            "Iter 828/1000 - Train Loss: 0.3202 - Val Loss: 0.3736\n",
            "Iter 829/1000 - Train Loss: 0.3200 - Val Loss: 0.3735\n",
            "Iter 830/1000 - Train Loss: 0.3199 - Val Loss: 0.3735\n",
            "Iter 831/1000 - Train Loss: 0.3198 - Val Loss: 0.3734\n",
            "Iter 832/1000 - Train Loss: 0.3196 - Val Loss: 0.3733\n",
            "Iter 833/1000 - Train Loss: 0.3195 - Val Loss: 0.3733\n",
            "Iter 834/1000 - Train Loss: 0.3194 - Val Loss: 0.3732\n",
            "Iter 835/1000 - Train Loss: 0.3192 - Val Loss: 0.3731\n",
            "Iter 836/1000 - Train Loss: 0.3191 - Val Loss: 0.3730\n",
            "Iter 837/1000 - Train Loss: 0.3190 - Val Loss: 0.3730\n",
            "Iter 838/1000 - Train Loss: 0.3188 - Val Loss: 0.3729\n",
            "Iter 839/1000 - Train Loss: 0.3187 - Val Loss: 0.3728\n",
            "Iter 840/1000 - Train Loss: 0.3186 - Val Loss: 0.3728\n",
            "Iter 841/1000 - Train Loss: 0.3185 - Val Loss: 0.3727\n",
            "Iter 842/1000 - Train Loss: 0.3183 - Val Loss: 0.3726\n",
            "Iter 843/1000 - Train Loss: 0.3182 - Val Loss: 0.3726\n",
            "Iter 844/1000 - Train Loss: 0.3181 - Val Loss: 0.3725\n",
            "Iter 845/1000 - Train Loss: 0.3179 - Val Loss: 0.3724\n",
            "Iter 846/1000 - Train Loss: 0.3178 - Val Loss: 0.3724\n",
            "Iter 847/1000 - Train Loss: 0.3177 - Val Loss: 0.3723\n",
            "Iter 848/1000 - Train Loss: 0.3175 - Val Loss: 0.3722\n",
            "Iter 849/1000 - Train Loss: 0.3174 - Val Loss: 0.3721\n",
            "Iter 850/1000 - Train Loss: 0.3173 - Val Loss: 0.3721\n",
            "Iter 851/1000 - Train Loss: 0.3172 - Val Loss: 0.3720\n",
            "Iter 852/1000 - Train Loss: 0.3170 - Val Loss: 0.3719\n",
            "Iter 853/1000 - Train Loss: 0.3169 - Val Loss: 0.3719\n",
            "Iter 854/1000 - Train Loss: 0.3168 - Val Loss: 0.3718\n",
            "Iter 855/1000 - Train Loss: 0.3166 - Val Loss: 0.3717\n",
            "Iter 856/1000 - Train Loss: 0.3165 - Val Loss: 0.3717\n",
            "Iter 857/1000 - Train Loss: 0.3164 - Val Loss: 0.3716\n",
            "Iter 858/1000 - Train Loss: 0.3163 - Val Loss: 0.3715\n",
            "Iter 859/1000 - Train Loss: 0.3161 - Val Loss: 0.3715\n",
            "Iter 860/1000 - Train Loss: 0.3160 - Val Loss: 0.3714\n",
            "Iter 861/1000 - Train Loss: 0.3159 - Val Loss: 0.3713\n",
            "Iter 862/1000 - Train Loss: 0.3157 - Val Loss: 0.3713\n",
            "Iter 863/1000 - Train Loss: 0.3156 - Val Loss: 0.3712\n",
            "Iter 864/1000 - Train Loss: 0.3155 - Val Loss: 0.3711\n",
            "Iter 865/1000 - Train Loss: 0.3154 - Val Loss: 0.3711\n",
            "Iter 866/1000 - Train Loss: 0.3152 - Val Loss: 0.3710\n",
            "Iter 867/1000 - Train Loss: 0.3151 - Val Loss: 0.3710\n",
            "Iter 868/1000 - Train Loss: 0.3150 - Val Loss: 0.3709\n",
            "Iter 869/1000 - Train Loss: 0.3149 - Val Loss: 0.3708\n",
            "Iter 870/1000 - Train Loss: 0.3147 - Val Loss: 0.3708\n",
            "Iter 871/1000 - Train Loss: 0.3146 - Val Loss: 0.3707\n",
            "Iter 872/1000 - Train Loss: 0.3145 - Val Loss: 0.3706\n",
            "Iter 873/1000 - Train Loss: 0.3144 - Val Loss: 0.3706\n",
            "Iter 874/1000 - Train Loss: 0.3142 - Val Loss: 0.3705\n",
            "Iter 875/1000 - Train Loss: 0.3141 - Val Loss: 0.3704\n",
            "Iter 876/1000 - Train Loss: 0.3140 - Val Loss: 0.3704\n",
            "Iter 877/1000 - Train Loss: 0.3139 - Val Loss: 0.3703\n",
            "Iter 878/1000 - Train Loss: 0.3137 - Val Loss: 0.3702\n",
            "Iter 879/1000 - Train Loss: 0.3136 - Val Loss: 0.3702\n",
            "Iter 880/1000 - Train Loss: 0.3135 - Val Loss: 0.3701\n",
            "Iter 881/1000 - Train Loss: 0.3134 - Val Loss: 0.3701\n",
            "Iter 882/1000 - Train Loss: 0.3132 - Val Loss: 0.3700\n",
            "Iter 883/1000 - Train Loss: 0.3131 - Val Loss: 0.3699\n",
            "Iter 884/1000 - Train Loss: 0.3130 - Val Loss: 0.3699\n",
            "Iter 885/1000 - Train Loss: 0.3129 - Val Loss: 0.3698\n",
            "Iter 886/1000 - Train Loss: 0.3128 - Val Loss: 0.3697\n",
            "Iter 887/1000 - Train Loss: 0.3126 - Val Loss: 0.3697\n",
            "Iter 888/1000 - Train Loss: 0.3125 - Val Loss: 0.3696\n",
            "Iter 889/1000 - Train Loss: 0.3124 - Val Loss: 0.3696\n",
            "Iter 890/1000 - Train Loss: 0.3123 - Val Loss: 0.3695\n",
            "Iter 891/1000 - Train Loss: 0.3121 - Val Loss: 0.3694\n",
            "Iter 892/1000 - Train Loss: 0.3120 - Val Loss: 0.3694\n",
            "Iter 893/1000 - Train Loss: 0.3119 - Val Loss: 0.3693\n",
            "Iter 894/1000 - Train Loss: 0.3118 - Val Loss: 0.3692\n",
            "Iter 895/1000 - Train Loss: 0.3117 - Val Loss: 0.3692\n",
            "Iter 896/1000 - Train Loss: 0.3115 - Val Loss: 0.3691\n",
            "Iter 897/1000 - Train Loss: 0.3114 - Val Loss: 0.3691\n",
            "Iter 898/1000 - Train Loss: 0.3113 - Val Loss: 0.3690\n",
            "Iter 899/1000 - Train Loss: 0.3112 - Val Loss: 0.3689\n",
            "Iter 900/1000 - Train Loss: 0.3111 - Val Loss: 0.3689\n",
            "Iter 901/1000 - Train Loss: 0.3109 - Val Loss: 0.3688\n",
            "Iter 902/1000 - Train Loss: 0.3108 - Val Loss: 0.3688\n",
            "Iter 903/1000 - Train Loss: 0.3107 - Val Loss: 0.3687\n",
            "Iter 904/1000 - Train Loss: 0.3106 - Val Loss: 0.3686\n",
            "Iter 905/1000 - Train Loss: 0.3105 - Val Loss: 0.3686\n",
            "Iter 906/1000 - Train Loss: 0.3103 - Val Loss: 0.3685\n",
            "Iter 907/1000 - Train Loss: 0.3102 - Val Loss: 0.3685\n",
            "Iter 908/1000 - Train Loss: 0.3101 - Val Loss: 0.3684\n",
            "Iter 909/1000 - Train Loss: 0.3100 - Val Loss: 0.3683\n",
            "Iter 910/1000 - Train Loss: 0.3099 - Val Loss: 0.3683\n",
            "Iter 911/1000 - Train Loss: 0.3097 - Val Loss: 0.3682\n",
            "Iter 912/1000 - Train Loss: 0.3096 - Val Loss: 0.3682\n",
            "Iter 913/1000 - Train Loss: 0.3095 - Val Loss: 0.3681\n",
            "Iter 914/1000 - Train Loss: 0.3094 - Val Loss: 0.3680\n",
            "Iter 915/1000 - Train Loss: 0.3093 - Val Loss: 0.3680\n",
            "Iter 916/1000 - Train Loss: 0.3092 - Val Loss: 0.3679\n",
            "Iter 917/1000 - Train Loss: 0.3090 - Val Loss: 0.3679\n",
            "Iter 918/1000 - Train Loss: 0.3089 - Val Loss: 0.3678\n",
            "Iter 919/1000 - Train Loss: 0.3088 - Val Loss: 0.3678\n",
            "Iter 920/1000 - Train Loss: 0.3087 - Val Loss: 0.3677\n",
            "Iter 921/1000 - Train Loss: 0.3086 - Val Loss: 0.3676\n",
            "Iter 922/1000 - Train Loss: 0.3085 - Val Loss: 0.3676\n",
            "Iter 923/1000 - Train Loss: 0.3083 - Val Loss: 0.3675\n",
            "Iter 924/1000 - Train Loss: 0.3082 - Val Loss: 0.3675\n",
            "Iter 925/1000 - Train Loss: 0.3081 - Val Loss: 0.3674\n",
            "Iter 926/1000 - Train Loss: 0.3080 - Val Loss: 0.3674\n",
            "Iter 927/1000 - Train Loss: 0.3079 - Val Loss: 0.3673\n",
            "Iter 928/1000 - Train Loss: 0.3078 - Val Loss: 0.3672\n",
            "Iter 929/1000 - Train Loss: 0.3076 - Val Loss: 0.3672\n",
            "Iter 930/1000 - Train Loss: 0.3075 - Val Loss: 0.3671\n",
            "Iter 931/1000 - Train Loss: 0.3074 - Val Loss: 0.3671\n",
            "Iter 932/1000 - Train Loss: 0.3073 - Val Loss: 0.3670\n",
            "Iter 933/1000 - Train Loss: 0.3072 - Val Loss: 0.3670\n",
            "Iter 934/1000 - Train Loss: 0.3071 - Val Loss: 0.3669\n",
            "Iter 935/1000 - Train Loss: 0.3069 - Val Loss: 0.3668\n",
            "Iter 936/1000 - Train Loss: 0.3068 - Val Loss: 0.3668\n",
            "Iter 937/1000 - Train Loss: 0.3067 - Val Loss: 0.3667\n",
            "Iter 938/1000 - Train Loss: 0.3066 - Val Loss: 0.3667\n",
            "Iter 939/1000 - Train Loss: 0.3065 - Val Loss: 0.3666\n",
            "Iter 940/1000 - Train Loss: 0.3064 - Val Loss: 0.3666\n",
            "Iter 941/1000 - Train Loss: 0.3063 - Val Loss: 0.3665\n",
            "Iter 942/1000 - Train Loss: 0.3061 - Val Loss: 0.3665\n",
            "Iter 943/1000 - Train Loss: 0.3060 - Val Loss: 0.3664\n",
            "Iter 944/1000 - Train Loss: 0.3059 - Val Loss: 0.3664\n",
            "Iter 945/1000 - Train Loss: 0.3058 - Val Loss: 0.3663\n",
            "Iter 946/1000 - Train Loss: 0.3057 - Val Loss: 0.3662\n",
            "Iter 947/1000 - Train Loss: 0.3056 - Val Loss: 0.3662\n",
            "Iter 948/1000 - Train Loss: 0.3055 - Val Loss: 0.3661\n",
            "Iter 949/1000 - Train Loss: 0.3054 - Val Loss: 0.3661\n",
            "Iter 950/1000 - Train Loss: 0.3052 - Val Loss: 0.3660\n",
            "Iter 951/1000 - Train Loss: 0.3051 - Val Loss: 0.3660\n",
            "Iter 952/1000 - Train Loss: 0.3050 - Val Loss: 0.3659\n",
            "Iter 953/1000 - Train Loss: 0.3049 - Val Loss: 0.3659\n",
            "Iter 954/1000 - Train Loss: 0.3048 - Val Loss: 0.3658\n",
            "Iter 955/1000 - Train Loss: 0.3047 - Val Loss: 0.3658\n",
            "Iter 956/1000 - Train Loss: 0.3046 - Val Loss: 0.3657\n",
            "Iter 957/1000 - Train Loss: 0.3045 - Val Loss: 0.3656\n",
            "Iter 958/1000 - Train Loss: 0.3044 - Val Loss: 0.3656\n",
            "Iter 959/1000 - Train Loss: 0.3042 - Val Loss: 0.3655\n",
            "Iter 960/1000 - Train Loss: 0.3041 - Val Loss: 0.3655\n",
            "Iter 961/1000 - Train Loss: 0.3040 - Val Loss: 0.3654\n",
            "Iter 962/1000 - Train Loss: 0.3039 - Val Loss: 0.3654\n",
            "Iter 963/1000 - Train Loss: 0.3038 - Val Loss: 0.3653\n",
            "Iter 964/1000 - Train Loss: 0.3037 - Val Loss: 0.3653\n",
            "Iter 965/1000 - Train Loss: 0.3036 - Val Loss: 0.3652\n",
            "Iter 966/1000 - Train Loss: 0.3035 - Val Loss: 0.3652\n",
            "Iter 967/1000 - Train Loss: 0.3034 - Val Loss: 0.3651\n",
            "Iter 968/1000 - Train Loss: 0.3032 - Val Loss: 0.3651\n",
            "Iter 969/1000 - Train Loss: 0.3031 - Val Loss: 0.3650\n",
            "Iter 970/1000 - Train Loss: 0.3030 - Val Loss: 0.3650\n",
            "Iter 971/1000 - Train Loss: 0.3029 - Val Loss: 0.3649\n",
            "Iter 972/1000 - Train Loss: 0.3028 - Val Loss: 0.3649\n",
            "Iter 973/1000 - Train Loss: 0.3027 - Val Loss: 0.3648\n",
            "Iter 974/1000 - Train Loss: 0.3026 - Val Loss: 0.3648\n",
            "Iter 975/1000 - Train Loss: 0.3025 - Val Loss: 0.3647\n",
            "Iter 976/1000 - Train Loss: 0.3024 - Val Loss: 0.3647\n",
            "Iter 977/1000 - Train Loss: 0.3023 - Val Loss: 0.3646\n",
            "Iter 978/1000 - Train Loss: 0.3022 - Val Loss: 0.3646\n",
            "Iter 979/1000 - Train Loss: 0.3020 - Val Loss: 0.3645\n",
            "Iter 980/1000 - Train Loss: 0.3019 - Val Loss: 0.3645\n",
            "Iter 981/1000 - Train Loss: 0.3018 - Val Loss: 0.3644\n",
            "Iter 982/1000 - Train Loss: 0.3017 - Val Loss: 0.3644\n",
            "Iter 983/1000 - Train Loss: 0.3016 - Val Loss: 0.3643\n",
            "Iter 984/1000 - Train Loss: 0.3015 - Val Loss: 0.3643\n",
            "Iter 985/1000 - Train Loss: 0.3014 - Val Loss: 0.3642\n",
            "Iter 986/1000 - Train Loss: 0.3013 - Val Loss: 0.3642\n",
            "Iter 987/1000 - Train Loss: 0.3012 - Val Loss: 0.3641\n",
            "Iter 988/1000 - Train Loss: 0.3011 - Val Loss: 0.3641\n",
            "Iter 989/1000 - Train Loss: 0.3010 - Val Loss: 0.3640\n",
            "Iter 990/1000 - Train Loss: 0.3009 - Val Loss: 0.3640\n",
            "Iter 991/1000 - Train Loss: 0.3008 - Val Loss: 0.3639\n",
            "Iter 992/1000 - Train Loss: 0.3007 - Val Loss: 0.3639\n",
            "Iter 993/1000 - Train Loss: 0.3005 - Val Loss: 0.3638\n",
            "Iter 994/1000 - Train Loss: 0.3004 - Val Loss: 0.3638\n",
            "Iter 995/1000 - Train Loss: 0.3003 - Val Loss: 0.3637\n",
            "Iter 996/1000 - Train Loss: 0.3002 - Val Loss: 0.3637\n",
            "Iter 997/1000 - Train Loss: 0.3001 - Val Loss: 0.3636\n",
            "Iter 998/1000 - Train Loss: 0.3000 - Val Loss: 0.3636\n",
            "Iter 999/1000 - Train Loss: 0.2999 - Val Loss: 0.3635\n",
            "Iter 1000/1000 - Train Loss: 0.2998 - Val Loss: 0.3635\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decision_region(X_train_scaled, y_train, slr) #Scratch (C=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "QM-sKwY9iucH",
        "outputId": "6497442f-2b53-4bd9-9c96-23698269b2a1"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 8 Saving weights"
      ],
      "metadata": {
        "id": "pLRFMeCQDAEF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s save and load the learned weights for easy verification. Use the pickle module and NumPy’s np.savez."
      ],
      "metadata": {
        "id": "LEHH4jJzDFz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Save learned weights (theta)\n",
        "def save_weights(model, filename='weights.pkl'):\n",
        "    with open(filename, 'wb') as f:\n",
        "        pickle.dump(model.theta, f)\n",
        "\n",
        "# Load learned weights (theta)\n",
        "def load_weights(model, filename='weights.pkl'):\n",
        "    with open(filename, 'rb') as f:\n",
        "        model.theta = pickle.load(f)\n"
      ],
      "metadata": {
        "id": "pPZMedh6DHwO"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_weights(model, 'my_weights.pkl')\n",
        "\n",
        "# Later or elsewhere\n",
        "load_weights(model, 'my_weights.pkl')\n"
      ],
      "metadata": {
        "id": "oKgZ_t96j3z5"
      },
      "execution_count": 47,
      "outputs": []
    }
  ]
}